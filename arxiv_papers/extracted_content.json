[
  {
    "metadata": {
      "id": "2510.03231v1",
      "title": "Reward Models are Metrics in a Trench Coat",
      "authors": [
        "Sebastian Gehrmann"
      ],
      "summary": "The emergence of reinforcement learning in post-training of large language\nmodels has sparked significant interest in reward models. Reward models assess\nthe quality of sampled model outputs to generate training signals. This task is\nalso performed by evaluation metrics that monitor the performance of an AI\nmodel. We find that the two research areas are mostly separate, leading to\nredundant terminology and repeated pitfalls. Common challenges include\nsusceptibility to spurious correlations, impact on downstream reward hacking,\nmethods to improve data quality, and approaches to meta-evaluation. Our\nposition paper argues that a closer collaboration between the fields can help\novercome these issues. To that end, we show how metrics outperform reward\nmodels on specific tasks and provide an extensive survey of the two areas.\nGrounded in this survey, we point to multiple research topics in which closer\nalignment can improve reward models and metrics in areas such as preference\nelicitation methods, avoidance of spurious correlations and reward hacking, and\ncalibration-aware meta-evaluation.",
      "published": "2025-10-03T17:59:44Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03231v1"
    },
    "text": "Reward Models are Metrics in a Trench Coat - Preprint\n\n## R EWARD M ODELS ARE M ETRICS IN A T RENCH C OAT\n\n\n**Sebastian Gehrmann**\nBloomberg\nsgehrmann8@bloomberg.net\n\n\nA BSTRACT\n\n\nThe emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the\nquality of sampled model outputs to generate training signals. This task is also\nperformed by evaluation metrics that monitor the performance of an AI model.\nWe find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve\ndata quality, and approaches to meta-evaluation. Our position paper argues that\na closer collaboration between the fields can help overcome these issues. To that\nend, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to\nmultiple research topics in which closer alignment can improve reward models\nand metrics in areas such as preference elicitation methods, avoidance of spurious\ncorrelations and reward hacking, and calibration-aware meta-evaluation.\n\n\n1 I NTRODUCTION\n\n\nReinforcement learning (RL) plays a major role in post-training, aligning, and adapting large language models (LLMs) to a broad range of tasks (OpenAI, 2025; Comanici et al., 2025; xAI, 2025;\nKimi et al., 2025; Guo et al., 2025). Scaling laws apply to reinforcement learning from human\nfeedback (RLHF, Christiano et al., 2017) similarly as to the rest of the training stack (Bai et al.,\n2022a). As such, scalable alternatives to human feedback have become popular, either in the form\nof verifiable rewards (Lambert et al., 2024) or in the form of models that assess the quality of model\noutputs (Li et al., 2018). Developing robust and reliable _reward models_ is crucial, as the downstream\nRL models can experience reward hacking (Amodei et al., 2016), optimizing for spurious correlations in the reward model rather than learning the intended behavior. To overcome these issues,\nreward models have experienced significant research interest.\n\n\nIn parallel to research on reward models, _model-based evaluation_ of generated text has similarly\nseen a surge in interest, enabling a switch from “traditional” lexical metrics like BLEU (Papineni\net al., 2002) and ROUGE (Lin, 2004) to learned models (Ma et al., 2018; 2019) and prompt-based\napproaches referred to as LLM-as-a-judge (Zheng et al., 2023). The two fields present two sides of\nthe same coin: while reward models assess output quality to directly improve models, evaluation\nmetrics assess output quality to identify potential areas of improvement. Both fields seek to develop\nclassifiers that consume generated content as input and assign a goodness score as output. Both\nfields strongly benefit from rigor, consideration of the sociotechnical context in which a system is\ndeployed, and improved correlation between model-based judgments and expert human raters. The\nkey difference between the two is that while metrics tend to be more specialized, reward models\ntend to assess broad capabilities spanning many tasks. Due to their similarities, one might expect\nthe fields learn from and inform each other, and that breakthroughs transfer quickly between them.\n\n\nOur position paper argues that while this should be the case, it is not. Instead, the academic literature\nin these fields only infrequently informs each other and the fields are actively developing and using\ndifferent terminology for the same methods. While metrics are commonly used to generate training\ndata for reward models (Malik et al., 2025), and are thus instrumental to reward model performance,\nlittle attention is being paid to which metrics generate that data. We demonstrate this phenomenon\nby analyzing the citation graphs of papers in each sub-field, showing that inter-field citations account\nfor fewer than 10% of total cited papers. We further support this claim by presenting results from\n\n\n1\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\ntwo small experiments: one in which we apply a metric to a reward modeling benchmark and one\nwhere we apply reward modeling techniques to a factuality evaluation benchmark. The results show\nthat reward modeling approaches lag behind dedicated metrics for these specialized tasks, providing\nopportunities for improvements and motivating cross-testing on their respective benchmarks.\n\n\nMotivated by these findings, we conduct an extensive survey of the two fields and their intersection.\nWe lay out scenarios in which we can use all the tools at our disposal and showcase how it could lead\nto better reward models and evaluation metrics. Specifically, we argue that a closer collaboration\ncould lead to major progress in overcoming reward hacking, in preference elicitation, and in metaevaluation. We also discuss areas in which the fields differ and should not interact, and how this\nrelates to Goodhart’s law, which states that a measure ceases to be a good measure when it becomes\na target. Grounded in these discussions, we make specific recommendations to researchers working\non reward models and evaluation metrics on how the separation can be overcome.\n\n\n2 H OW DID MODERN R EWARD M ODELS COME ABOUT ?\n\n\nAs part of the rising popularity of deep learning, RL started to be explored for tasks like structured\nprediction (Daum´e et al., 2009; Ross et al., 2011), image recognition (Mnih et al., 2014; Ba et al.,\n2015) and for agents like the Neural Turing Machine (Zaremba & Sutskever, 2015). Successfully\ntraining a model via RL hinges on being able to generate reward signals. This includes being able\nto derive the value of intermediate states. As Sutton & Barto (2018) argue, “the most important\ncomponent of almost all reinforcement learning algorithms we consider is a method for efficiently\nestimating values.” Commenting on this issue, Yann LeCun famously criticized RL for having much\nsparser rewards than self-supervised learning during his talk “Predictive Learning” (LeCun, 2016).\n\n\nThis issue applies to generated language: generation has a combinatorially large state space with\nits sequential token choices from a large vocabulary, and no single objective number can represents\nthe value of an output (Gehrmann et al., 2023). For that reason, generation models are typically\ntrained via teacher-forcing, a supervised approach that shows the model a ground-truth token at\neach prediction step. This happens only during training, not at test-time. Moreover, while models\nare trained with a cross-entropy objective, they are evaluated via different metrics. Ranzato et al.\n(2016) coined the term _exposure bias_ for this mismatch between training and test time.\n\n\nIf there was a way to directly optimize for the metric(s) we care about, the exposure bias could be\novercome. Evaluation metrics are designed to act as a proxy for human judgments and are thus wellsuited to serve as a reward function. While some inference-time methods optimize metrics (Wiseman & Rush, 2016; Freitag et al., 2021b), reinforcement learning is a natural fit to optimize for\nthese metrics during training. REINFORCE (Williams, 1992) and minimum risk training (Duda &\nHart, 1974) generate metric-based reward signals using sampled token sequences, and actor-critic\napproaches estimate partial rewards for predicted tokens (Bahdanau et al., 2017). Various instantiations of these approaches were used for machine translation (Ranzato et al., 2016; Shen et al.,\n2016), image captioning (Rennie et al., 2017), video captioning (Pasunuru & Bansal, 2017), and\nsummarization (Paulus et al., 2018).\n\n\nAt that time, the reward models were measuring lexical overlap between a generated sequence and a\nground truth (e.g., Papineni et al., 2002; Lin, 2004; Vedantam et al., 2015). These metrics have wellunderstood drawbacks (e.g., Reiter, 2018; Freitag et al., 2020), especially for RL (Choshen et al.,\n2020). Among others, they lead to _reward hacking_ where models generate non-fluent language that\nmaximizes reward scores (Amodei et al., 2016). Researchers worked to overcome these issues, for\nexample by regularizing the training process by combining cross-entropy losses with RL or by handcrafting additional reward functions (Pasunuru & Bansal, 2018; Kry´sci´nski et al., 2018; Wu et al.,\n2018a). The advent of metrics measuring semantic rather than lexical similarity led to significantly\nreduced reward-hacking since the new models avoided over-optimizing for the generation of relevant\nwords without fluent context (Li et al., 2018; Yasui et al., 2019; Scialom et al., 2019). [1] These models\nled to a clear path whereby new metrics could be validated and then used as reward models. For\n\n\n1 This is also related to Generative Adversarial Networks (Goodfellow et al., 2014) for generation (e.g., Yu\net al., 2017; Wu et al., 2018b) where a discriminator differentiates generated text from the ground truth, thus\nsimilarly generating a model-based signal for human-likeness.\n\n\n2\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n\n\n\n\n\n|P<br>Evaluation Metric|Col2|Col3|Col4|aper Count Over T|Col6|Col7|ime b|y Fiel|d|Col11|Col12|Col13|Col14|Col15|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|~~Reward~~<br>LLM as|~~ Model~~<br>a Judge||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n\n\nFigure 1: The figure shows the number of publications per year in the three subfields according to\na keyword search on Google Scholar. Publications on evaluation metrics have slowed, even though\nresearch on reward modeling and LLM-as-a-judge is quickly rising in popularity. If the fields were\nactively learning from one another, one could assume that mentions of ”evaluation metrics” should\nbe growing alongside these newly emerging fields, but they are not.\n\n\nexample, the metric BLEURT was introduced (Sellam et al., 2020), evaluated as part of the WMT\nMetrics shared task (Mathur et al., 2020), and then assessed as reward model (Shu et al., 2021).\n\n\nIn parallel to the work above, the concept of _reinforcement learning from human feedback (RLHF)_\nwas introduced for game playing and robotics (Christiano et al., 2017). In an essay titled “Scalable\nagent alignment via reward modeling: a research direction”, Leike et al. (2018) propose capturing\nhuman preferences via dedicated _reward models_ . This research culminated in the work on RLHF\nfor summarization (Stiennon et al., 2020) which popularized Proximal Policy Optimization (PPO,\nSchulman et al., 2017) as RL approach for text generation. While Stiennon et al. (2020) analyzed\ncorrelations between ROUGE and their human preference data, they did not use widely accepted\nalignment metrics, existing human preference corpora, or the semantic similarity evaluation metrics\ndiscussed above. In the followup work on InstructGPT (Ouyang et al., 2022), there are no references\nto the generation RL literature and no evaluation of the reward model. Subsequent work introduced\nthe notion of AI Feedback as reward models (Bai et al., 2022b; Lee et al., 2024a) and argued that language model probabilities can be directly used to model rewards in a Bradley-Terry model (Rafailov\net al., 2023; Bradley & Terry, 1952). Neither draws the connection to the role that perplexity and\nmodel probabilities played in existing evaluations (e.g., Lewis et al., 2020; Min et al., 2023).\n\n\nThis separation culminated in benchmarks for reward models (e.g., Frick et al., 2025; Lambert et al.,\n2025; Liu et al., 2025c; Zhou et al., 2025) and for metrics (e.g., Honovich et al., 2022; Clark et al.,\n2023; Freitag et al., 2024) that exist in parallel without meaningful interaction. This raises the\nquestion of whether this disconnect is part of a broader trend. And if the two fields were integrated\ntighter, would we be in a better state today? And what should one learn from the other?\n\n\n3 Q UANTIFYING THE R ESEARCH F IELD S EPARATION\n\n\nFigure 1 establishes the need for this investigation by showing the number of papers found on Google\nScholar per year that contain the exact strings “Evaluation Metric”, “Reward Model”, and “LLM-asa-judge”. We include “LLM-as-a-judge” as an emerging field that has similarly experienced rapid\ngrowth and which also uses language models to estimate the quality of generated output. Notably,\ndespite the exponential growth of the two emerging topics, the number of papers mentioning evaluation metrics decreased in 2024, with the trend continuing into 2025.\n\n\nIf the terminology was merely changing, one would expect the new literature to still build on the\nolder one. For that reason, we empirically study the cause of this phenomenon by conducting a\ncitation analysis. We select up to 300 papers per field per year (2021–2025) via the Semantic Scholar\nGraph API, with sensitivity checks at 100/200 yielding similar trends (Kinney et al., 2023). [2] For\n\n\n2 [Documented at https://api.semanticscholar.org/api-docs/graph. The search results](https://api.semanticscholar.org/api-docs/graph)\nin the maximum 300 papers for the first two fields and 8, 25, and 43 papers respectively for LLM-as-a-judge\nover the past three years. More details on this analysis in Appendix A.\n\n\n3\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The fraction of citations from one field to\nanother, based on keywords in cited papers.\n\n\n\n(b) The fraction of citations to papers in venues\nassociated with a research area.\n\n\n\nFigure 2: In our analysis of citation dynamics across the three fields, we find that evaluation papers\ntend to cite other evaluation papers across research fields, while reward model papers mostly cite\neach other and are highly focused on machine learning venues. LLM-as-a-judge work mostly cites\nML and NLP venues, but has less clear citation dynamics.\n\n\neach paper, we additionally retrieve its citations, yielding approximately 10,000 citations per year\nfor each field to analyze. As a proxy to identify whether a paper in field A cites a paper in field B, we\nselect _signaling terms_ for each field: (1) “metric(s)”, (2) “reward”, “reinforcement”, “policy”, and\n(3) “judge”. If any of those terms appears in the title or abstract of a cited paper, we count this as an\ninter-field or intra-field citation. The results in Figure 2a show that evaluation metrics and reward\nmodels are distinct fields, with only few inter-field citations but many intra-field citations. This is\nespecially pronounced for reward models where almost 40% intra-field citations. The numbers for\nevaluation metrics trend lower at around 15–20% which we attribute to the heterogeneity of the\nfield; for example, papers on metrics for summarization cite summarization papers rather than only\nevaluation papers. LLM-as-a-judge is an outlier, with too few papers to draw definitive conclusions.\n\n\nWe find more evidence for the field separation when we analyze the venues of the cited papers.\nFor this, we categorize venues into fields (e.g., ICLR as ML venue) and calculate the percentage\nof citations to papers published in the various fields. The results in Figure 2b reveal that reward\nmodel research predominantly cites research in machine learning venues and not NLP and Computer\nVision. In contrast, evaluation metric work is evenly distributed and LLM-as-a-judge work focuses\non ML and NLP. [3] Since all observed trends are stable across years, we conclude that the three\nresearch fields are largely separate with limited interaction.\n\n\n4 W HAT CAN ONE LEARN FROM THE OTHER ?\n\n\nA rebuttal to our proposition that the two fields should learn from each other is that maybe there is\nlittle to learn. We thus highlight two scenarios in which a closer cross-field interaction could have\nchanged conclusions or yielded additional insights.\n\n\n4.1 M ETRICS CAN PERFORM WELL ON R EWARD M ODEL B ENCHMARKS\n\n\nThe recently introduced RewardBench-M (Gureja et al., 2025) uses a subset of the MAPLE\ndataset (Zhu et al., 2024) to assess reward models on translation evaluation. The task requires\nidentifying which of two translation outputs was rated higher by human evaluators. The data is split\ninto an easy and difficult subset based on the difference of human scores of the two provided translations. While all their tested models perform nearly perfectly on the easy subset, Gureja et al. (2025)\nremark that “ _models that perform well on easy tasks can struggle to maintain the same level of per-_\n\n\n3 We omit fields with _≤_ 5% of citations in all years, including Speech, IR, and HCI. Ambiguous venues like\npreprint servers or broad venues like AAAI are excluded from this analysis. Detailed list in Appendix C.\n\n\n4\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nde _→_ en en _→_ de zh _→_ en en _→_ zh\n\n\nGPT-4o **71.0** 61.0 **77.0** 80.0\nAya Expanse 32B 62.0 **69.0** 76.0 79.0\n\n\nC OMET K IWI -DA (2022) 59.0 **68.0** 59.0 **86.0**\n\n\nTable 1: Results on the hard machine translation evaluation subset of RewardBench-M. For nonEnglish evaluations, a 3 year old model with 550M parameters outperforms much larger LLMs.\n\n\n_formance on harder translations, indicating the need for more sophisticated mechanisms to handle_\n\n_[...] challenging scenarios_ ”. However, no machine translation evaluation metric was assessed as a\nbaseline. Thus, to test this hypothesis, we evaluate the three-year-old metric CometKiwi (Rei et al.,\n2022) which is based on InfoXLM (Chi et al., 2021) and has only 550M parameters.\n\n\nThe results of CometKiwi alongside the two best-performing models on the challenging translation\ntest set of RewardBench-M (Dang et al., 2024; Hurst et al., 2024) are shown in Table 1. Despite\nits age and being significantly smaller, CometKiwi performs similarly on German and outperforms\nthe other models on Chinese, with the overall best evaluation performance for the non-English generated text, demonstrating that the “sophisticated mechanisms” needed in current reward models\nalready exist. Building on this observation, the MetaMetrics approach (Anugraha et al., 2024) has\nbeen evaluated on the latest MT metrics shared task (Freitag et al., 2024) and on the RewardBench\nleaderboard (Lambert et al., 2025), scoring highly in both, although not with the same model.\n\n\n4.2 R EWARD M ODELS CAN UNDERPERFORM ON M ETRICS B ENCHMARKS\n\n\nAnother area in which reward model and metrics benchmarks are aligned in their goals is the assessment of how well models can assess factuality and attribution. There exist benchmarks for\nmetrics (Honovich et al., 2022), model performance (Jacovi et al., 2025), and reward models (Malik\net al., 2025) that assess factuality. Recent work demonstrates the effectiveness of LLM judge (and\nreward) models (e.g., Calderon et al., 2025; Hashemi et al., 2024), some even finding that dedicated\nfinetuned evaluation models underperform LLM judges (Huang et al., 2025).\n\n\nAmong these benchmarks, the metrics benchmark SEAHORSE (Clark et al., 2023) is the largest with\nover 100,000 human judgments of summarization quality aspects across multiple languages. For this\nexperiment, we prompt various LLMs with the same instructions provided to human annotators in\nSEAHORSE to give a binary judgment whether a summary is attributable to an article. [4] Due to a\nlack of data availability, we exclude the WikiLingua (Ladhak et al., 2020) subset of SEAHORSE\nand focus only on XLSum (Hasan et al., 2021) and MLSum (Scialom et al., 2020), retaining 7,793\nof the 18,330 test examples. We report Pearson correlation ( _ρ_ ) and accuracy.\n\n\nThe results (Table 2) show that LLMs underperform the dedicated model trained on in-domain data.\nThis remains true even if we assess judge models with a high reasoning budget like Gemini 2.5 Pro\nand GPT-5. In fact, the two reasoning models have an 89% agreement rate, higher than the inter-rater\nagreement of 73% reported in the paper, indicating that the models look for similar input and outputfeatures to make their prediction. The results are fairly consistent across languages. Interestingly,\nall models score lowest on English among the evaluated languages.\n\n\nOverall, our findings disagree with Huang et al. (2025) who show that LLM judges can outperform dedicated metrics in similar setups, while agreeing with Bavaresco et al. (2025) who show low\nLLM judge correlations for summarization evaluation. Multiple explanations exist for the results\npresented here, including annotation artifacts that cause a lower performance of the LLM judge\nsetup. However, we can conclude that for evaluating attribution for summarization, it remains unclear whether LLM judges have caught up to dedicated models, a question that requires further\nrigorous study. This conclusion mirrors the argument by Chehbouni et al. (2025) that the “rapid\nand widespread adoption [of LLM judges] may have occurred prematurely”. Moreover, as shown\nin Table 2, the LLM judge setup outperforms a strong Natural Language Inference model (NLI)\nbaseline (Conneau et al., 2018). As such, this setup could still be useful in cases where dedicated\ntraining data for a reward model or evaluation metric is unavailable.\n\n\n4 We optimized performance on the validation set to minimize the effect of the prompt format.\n\n\n5\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n_ρ_ Acc. %\n\n\nROUGE-L 0.13\nmT5 XNLI 0.43\nmT5 SEAHORSE 0.59\n\n\nGPT-4o 0.47 73.3\n\nGemini 2.0 Flash 0.42 72.1\n\nGemini 2.5 Flash 0.48 73.8\n\nClaude Sonnet 4 0.45 70.1\n\n\n_+ Reasoning_\n\n\nGemini 2.5 Pro 0.50 73.9\n\nGPT-5 0.47 70.2\n\n\nTable 2: Pearson _ρ_ coefficient and binary prediction accuracy on SEAHORSE for identifying\nwhether a summary is **attributable** to a source article. The baselines are finetuned mT5 XXL models\nby Clark et al. (2023). The LLM-as-a-judge approach is outperformed by a dedicated trained metric.\n\n\n5 M ETRICS AND REWARD MODELS ARE ( NOT ) THE SAME\n\n\nMetrics and reward models both judge quality aspects of generated content with the goal of being\naligned with human preferences. Yet, they are not the same: they can differ in their design, application, training, and testing. To explore these aspects, we provide a survey of the two fields and\ndiscuss themes where a closer interaction could lead to mutually helpful insights.\n\n\n5.1 D ESIGNING R EWARD M ODELS AND E VALUATION M ETRICS\n\n\n**Sociotechnical Context matters** Evaluation metrics tend to be narrowly focused on specific quality aspects. These quality aspects should follow clear and standardized definitions so that the metrics\nare transferable and produce scores that are understandable across organizations. A lack of transparency in how metrics are designed and the subsequent lack of reproducibility has been subject of\nmuch past criticism (Rankel et al., 2013; Post, 2018; Gehrmann et al., 2023).\n\n\nIn contrast, if a reward model is the provider of training signals during reinforcement learning, it\nmust therefore be able to score a myriad of tasks and output types. As such, modeling human\npreferences encompasses many aspects of preference beyond output quality, including whether a\nmodel correctly refuses undesired requests or avoids producing toxic language (Bai et al., 2022b).\nThese judgments depend on the specific application the model is used for and the policies governing this application. Reward models that measure these application-specific aspects are inherently\nless transferrable and tied to the specific organizations that develop them (Gehrmann et al., 2025).\nFollowing this reasoning, these reward models are inherently not comparable to another, which calls\ninto question the utility of non-specific reward-modeling benchmarks.\n\n\n**Aspect-aligned reward models** Fine-grained assessments of (partial) generations are areas with\nextensive recent work in RL that more closely align with work on evaluation metrics (Gunjal et al.,\n2025; Lightman et al., 2024). It is of particular interest, since a diverse set of reward signals can\nmitigate issues that arise from single-objective optimization (Freitag et al., 2021b; Zhang et al.,\n2024; Fisch et al., 2024). A popular approach for this is to use reward models that score rubrics\ninstead of providing generic preferences (Gunjal et al., 2025). Rubrics are fine-grained evaluation\ncriteria (Arora et al., 2025; Hashemi et al., 2024), similar to those traditionally assessed by dedicated metrics. Rubric-based prompted scoring, alongside learned reward models, is mentioned as an\ninstrumental ingredient for post-training of models like Gemini 2.5 (Comanici et al., 2025).\n\n\nSuccessfully assessing rubrics requires clear definitions of the evaluation categories (Howcroft et al.,\n2020). Yet, even for popular concepts like “hallucination rate”, definitions can vary widely (Maynez\net al., 2020; Rashkin et al., 2023; Ji et al., 2023). Increasing the consistency of these definitions will\nbe crucial as reward models become more specific, and thus are designed more similar to evaluation\n\n\n6\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nmetrics where the topic of fine-grained assessments is well-studied (e.g., Eyal et al., 2019; Wang\net al., 2020; Fabbri et al., 2021; Scialom et al., 2021; Lee et al., 2024b; Wei et al., 2025).\n\n\n5.2 T RAINING R EWARD M ODELS AND E VALUATION M ETRICS\n\n\n**Data Collection** The data collection methodology for any model must be aligned with its design\ngoals. For reward models, this means reflecting the preferences of the intended audience of the\ndownstream model, which can be extremely broad. Since many aspects of generated text cannot\nbe objectively assessed, this necessitates collecting feedback from diverse sources (Casper et al.,\n2023; Metz et al., 2025). The culture and lived experience of raters can lead to drastically different\nsubjective preference judgments (e.g., Aroyo et al., 2023; Rastogi et al., 2024; 2025).\n\n\nAnother critical question to consider is whether the selected raters have sufficient expertise, as\nchanges in annotation quality can lead to drastically different insights (Freitag et al., 2021a; Wei\net al., 2024). In many cases, existing metrics and reward models already outperform non-expert\nraters, and only the highest quality annotations can further improve the models (Cui et al., 2023;\nLiu et al., 2024; Wen et al., 2025b). Moreover, Wen et al. (2025a) find that RL may produce errors\nthat are increasingly difficult for humans to detect. However, hiring raters with expertise to judge\nlong-form generation is notoriously challenging (Zhang et al., 2023).\n\n\n**Optimization targets** Design aspects such as access to a ground truth and the output format (pairwise preferences, categorical labels, or continuous scores) influence how models are developed.\nThese choices depend on the downstream use case, and can have significant impact on model efficacy. For example, reference-less evaluation has improved significantly in recent years but still\nunderperforms reference-based metrics (Ma et al., 2019; Freitag et al., 2024). Similar results were\nfound for LLM-as-a-judge setups (Krumdick et al., 2025). These choices are reflected in benchmarking practices; many reward models produce pairwise comparisons, and their benchmarks consequently focus on this binary setup (Frick et al., 2025). In contrast, metrics typically generate\ncontinuous outputs, allowing for more flexible evaluation. By focusing primarily on pairwise judgments, reward model development may be ignoring the potential benefits of continuous scoring.\n\n\nAnother shared goal is the development of lightweight models that can run efficiently alongside\nlarger models during inference or training. Advances in distillation, quantization, parallelization,\nand pruning are therefore highly relevant to both fields. Consequently, approaches to model compression that seek to train student models to outperform their teachers can equally benefit the development of both reward models and evaluation metrics (Kim et al., 2024; Sun et al., 2023).\n\n\n5.3 T ESTING R EWARD M ODELS AND E VALUATION M ETRICS\n\n\n**Identifying and Debugging Reward Hacking** A lack of correlation between reported reward\nmodel and downstream RL model performance has been attributed to limitations of the reward\nmodel (Ivison et al., 2024; Kim et al., 2025; Wen et al., 2025c). When the reward model does not\nrobustly generalize, or focuses on spurious correlations, it can lead to _reward hacking_ . Amodei et al.\n(2016) describe reward hacking as the process of “gaming” flaws in the reward model to maximize\nthe rewards without learning the intended behavior. This phenomenon was empirically observed for\ntext (Pasunuru & Bansal, 2017; Kry´sci´nski et al., 2018; Wu et al., 2018a) and non-text RL (Amodei\n& Clark, 2016; Krakovna et al., 2020; Nagarajan et al., 2021).\n\n\nIt is not specific to reward models, as most classification models suffer from spurious correlations (Ribeiro et al., 2016; McCoy et al., 2019) and spurious correlations were found in reward\nmodels (Liu et al., 2025b) and metrics (Sun et al., 2019). Among the effects, reward models may\nprefer more confident-sounding answers (Leng et al., 2025), exhibit a verbosity bias (Saito et al.,\n2023), focus more on style than content (Feuer et al., 2025), and results may be confounded by the\norder in which outputs are shown (Wang et al., 2024). Relatedly, the problem of _sycophancy_ has been\ncharacterized as models learning to match user beliefs over generating truthful responses (Sharma\net al., 2024). Murugadoss et al. (2025) and Hu et al. (2024) further show that the detail of LLMas-a-judge prompts have little influence on its performance, implying that models rely too much\non their implicitly learned quality criteria definitions. These issues motivate work on diagnostic\ndatasets (Gabriel et al., 2021), distractor generation (Qiu et al., 2020; Dhole et al., 2023), and model\ninterpretability (Jacovi et al., 2023), to become aware of and overcome spurious correlations.\n\n\n7\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n**Meta-Evaluation Frameworks** The field of meta-evaluation is concerned with the question of\nhow we evaluate evaluators. Callison-Burch et al. (2007) popularized this practice in NLP through a\nshared task series that performs a yearly assessment of MT metrics. Meta-evaluation measures two\naspects: _segment-level_ and _system-level_ performance. A high system-level performance means that\nsystem rankings in a leaderboard are trustworthy, while segment-level assessments look at whether\nindividual pairs of system outputs are ranked correctly. These two measures are not always correlated (Wei & Jia, 2021), motivating an approach that matches how a model is used.\n\n\nAlgorithms like DPO (Rafailov et al., 2023) use the reward score difference between a chosen and\nrejected model output as training signal. This directly matches the segment-level meta-evaluation.\nHowever, a known issue is that evaluation metrics are often not well-calibrated (Kocmi et al., 2024),\nwhich may cause issues if they are applied as reward models. Moreover, reward model benchmarks\nlike RewardBench 2 (Lambert et al., 2025; Gureja et al., 2025) do not consider score calibration,\ninstead reporting overall accuracy on the task of identifying the highest rated system output, which\nmore closely matches a system-level assessment. As a result, calibration issues may be overlooked\nif one focuses only on reward model benchmark performance. This oversight of segment-level assessments could further contribute to the lack of correlation between reward model and downstream\nmodel performance. Thus, future work on reward model benchmarking could benefit from reporting\nsegment-level rather than system-level performance, including assessments of score calibration.\n\n\n**Meta-Evaluation Targets** A complicating factor for the meta-evaluation of reward models is the\nbreadth of tasks for which they need to assess output quality. Their meta-evaluations thus need to\nstrike a balance of breadth, validity, and relevance. Ivison et al. (2024) suggest that existing reward model benchmarks are too narrow, especially considering their performance variance across\ntasks (Bavaresco et al., 2025). Benchmarks like RewardBench 2 already average multiple categories, but the question of how to aggregate sub-category scores into a single ranking becomes\nimportant. To that end, Frick et al. (2025) find that pessimistic reward model evaluations instead of\naverage performance are more indicative of downstream model performance, motivating alternative\nleaderboard designs that focus on finding shortcomings, rather than averaging performance numbers. These issues are further exacerbated when the systems that are being evaluated by evaluation\nmetrics and reward models improve. As these models become harder to distinguish, biases in the\nevaluation setup become more noticeable (Wei & Jia, 2021) and tie handling procedures need to be\nintroduced (Thompson et al., 2024; Sun et al., 2025).\n\n\n5.4 R ECOMMENDATIONS\n\n\nWhile developing an unhackable reward model is likely impossible (Skalse et al., 2022), metrics,\nand more directly reward models, share a symbiotic relationship with the downstream models where\nimprovements in one translate to improvements in the other (Gehrmann et al., 2021). This means,\nwe should strive to produce the most accurate estimate of human preferences. To that end, both fields\nbenefit from having high-quality training and meta-evaluation data. This data needs to be grounded\nin clear definitions in the sociotechnical context that the to-be-assessed models are deployed in.\nWhile it is unavoidable to introduce spurious correlations, in both fields it is crucial to identify and\nmeasure them and to mitigate their impact on downstream uses.\n\n\nModeling choices and optimization targets similarly align between the two fields, whether that is\napplying LLM-as-a-judge, or training classification models on human-curated data. Due to this\noverlap, newly introduced methods for modeling human preferences should be evaluated on metrics\nand reward model benchmarks alike to paint a more accurate and complete picture of how these\nmethods perform. More generally, meta-evaluation and the development of leaderboards is an area\nin which the fields have significantly diverged. They should come together to address the poor\ncorrelation between reward model benchmark scores and downstream model performance. Shared\nbest practices on tie handling, segment-level correlation measures, conducting model calibration\nassessments, and collecting test datasets will benefit both fields.\n\n\nHowever, as reward modeling matures as a field, it will be important to avoid falling into traps\nlike developing default models and benchmarks that, despite being outdated, continue to be broadly\nused (Bommasani & Cardie, 2020). Instead, evaluation research could adopt the practice from\nreward modeling of moving to new and better performing model as they become available.\n\n\n8\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nWhile reward models and evaluation metrics should be developed using the same best practices, one\ncannot be used to replace the other. As Goodhart’s law states, _when a measure becomes a target, it_\n_ceases to be a good measure_ (Goodhart, 1984). Applying to both fields, models may perform well as\na proxy for the distribution over human preferences but will diverge in the tail (i.e., rarely seen model\ninputs) and may over-generalize and focus on spurious patterns (Manheim & Garrabrant, 2018; Gao\net al., 2023). Similar arguments apply to the utility of shared tasks and leaderboards which similarly\nare a frequent target of criticism (e.g., Scott & Moore, 2006; Ethayarajh & Jurafsky, 2020; Thomas\n& Uminsky, 2020; Raji et al., 2021; Bowman & Dahl, 2021). Our practical recommendation is thus\nfor the fields to share insights into methodologies, but not to collapse into one field.\n\n\nA full collapse would risk creating a monoculture where only a few benchmarks dictate model optimization, rather than having many different targets (Koch & Peterson, 2024). As Singh et al.\n(2025) state, an “over-reliance on a single leaderboard creates a risk that providers may overfit to\nthe aspects of leaderboard performance, without genuinely advancing the technology in meaningful\nways”. Issues with broadly adopted evaluation setups lead to overspecialization and lack of generalization beyond what a specific leaderboard measures (Liu et al., 2025a; Zouhar et al., 2024).\nBeing too rigid in how meta-evaluations are conducted could also exclude new methods from being\ninvestigated fairly (Perrella et al., 2024).\n\n\nFurthermore, while methods for reward modeling may be informed by insights from evaluation metric development, the specific reward models may not perform well on the same benchmarks. As\ndiscussed above, reward models are often specific to a sociotechnical context, and would thus not\nperform well on public reward modeling benchmarks. This may cause a rift between industry and\nacademic research where the best reward modeling approaches are not publicly disclosed because\nthey are too entangled within this context. Yet, especially for models that measure human preferences for whether a model output is considered offensive or undesirable, it is critical to develop\npublic standards and be transparent about the underlying policies a model is trying to enact.\n\n\n6 C ONCLUSIONS\n\n\nIn this work, we have argued that evaluation metrics and reward models share many similarities.\nTheir developers need to make the same choices about their inputs and outputs, their collection of\ntraining and validation data, and the resulting models suffer from the same drawbacks. While the\napplication areas and the specific choices made during development may differ, at their core, both\nseek to model human preferences of model output quality. This supports our thesis that the fields\nshould look at and learn from each other’s advances, rather than continuing to exist in parallel.\n\n\nWe grounded this discussion in a citation analysis that demonstrated that the research fields are\ndeveloping mostly in isolation from each other. This separation of fields can lead to missed opportunities, the rediscovery of established findings, and potentially flawed conclusions. We quantified the\nseparation through two experiments that show that reward models may be lacking when assessed\non domains for which evaluation metrics are already available. We provided an extensive survey\nand discussed several areas in which future work on metrics and reward models, meta-evaluations,\nand benchmark creation could incorporate insights from both fields. While we recommend against\nthe development of a monoculture with too few relevant benchmarks, we encourage researchers to\nconsider work from both fields and work on unifying both methodologies and terminologies.\n\n\nBeyond the scope of this work, we acknowledge efforts in reinforcement learning to solve tasks with\nverifiable rewards, for example math problems (Ke et al., 2025), for which reward models play a less\ncentral role. In this domain, model-based reward modeling approaches largely perform string matching between the verified and generated answer, and thus do not require as complex approaches as\nthose discussed here. Training models for these verifiable domains can induce reasoning capabilities\nand has led to broader generalization (Guo et al., 2025; Comanici et al., 2025). While this finding\ndoes not make reward models for non-verifiable domains obsolete, it presents a possible alternative\nor parallel path in which reward models do not play such a central role. Moreover, we note that improvements in reward models may not always translate into downstream model improvements. The\n_superficial alignment hypothesis_ by Zhou et al. (2023) poses that the reinforcement learning stage\nprimarily changes ’how’ a model responds, rather than contributing new world knowledge. Thus,\neven a perfect reward model cannot overcome fundamental knowledge gaps from pre-training.\n\n\n9\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nR EFERENCES\n\n\n[Dario Amodei and Jack Clark. Faulty reward functions in the wild. https://openai.com/](https://openai.com/index/faulty-reward-functions/)\n[index/faulty-reward-functions/, 2016.](https://openai.com/index/faulty-reward-functions/)\n\n\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Concrete problems in ai safety. _arXiv preprint arXiv:1606.06565_, 2016.\n\n\nDavid Anugraha, Garry Kuwanto, Lucky Susanto, Derry Tanti Wijaya, and Genta Winata.\nMetaMetrics-MT: Tuning meta-metrics for machine translation via human preference calibration. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), _Proceedings_\n_of the Ninth Conference on Machine Translation_, pp. 459–469, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.wmt-1.32. URL\n[https://aclanthology.org/2024.wmt-1.32/.](https://aclanthology.org/2024.wmt-1.32/)\n\n\nRahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Qui˜nonero Candela,\nFoivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes\nHeidecke, and Karan Singhal. Healthbench: Evaluating large language models towards improved\nhuman health. _CoRR_ [, abs/2505.08775, 2025. doi: 10.48550/ARXIV.2505.08775. URL https:](https://doi.org/10.48550/arXiv.2505.08775)\n[//doi.org/10.48550/arXiv.2505.08775.](https://doi.org/10.48550/arXiv.2505.08775)\n\n\nLora Aroyo, Mark Diaz, Christopher Homan, Vinodkumar Prabhakaran, Alex Taylor, and Ding\nWang. The reasonable effectiveness of diverse evaluation data. _arXiv preprint arXiv:2301.09406_,\n2023.\n\n\nJimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual attention. In Yoshua Bengio and Yann LeCun (eds.), _3rd International Conference on Learning_\n_Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-_\n_ings_ [, 2015. URL http://arxiv.org/abs/1412.7755.](http://arxiv.org/abs/1412.7755)\n\n\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau,\nAaron C. Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In _5th_\n_International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,_\n_2017, Conference Track Proceedings_ [. OpenReview.net, 2017. URL https://openreview.](https://openreview.net/forum?id=SJDaqqveg)\n[net/forum?id=SJDaqqveg.](https://openreview.net/forum?id=SJDaqqveg)\n\n\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson,\nDario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and\nJared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human\nfeedback. _ArXiv_ [, abs/2204.05862, 2022a. URL https://api.semanticscholar.org/](https://api.semanticscholar.org/CorpusID:248118878)\n[CorpusID:248118878.](https://api.semanticscholar.org/CorpusID:248118878)\n\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli TranJohnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse,\nKamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem´ı Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna\nKravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario\nAmodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI:\nharmlessness from AI feedback. _CoRR_, abs/2212.08073, 2022b. doi: 10.48550/ARXIV.2212.\n[08073. URL https://doi.org/10.48550/arXiv.2212.08073.](https://doi.org/10.48550/arXiv.2212.08073)\n\n\nAnna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern´andez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andre Martins,\nPhilipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, and Alberto Testoni. LLMs instead of human\njudges? a large scale empirical study across 20 NLP evaluation tasks. In Wanxiang Che, Joyce\n\n\n10\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nNabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), _Proceedings of the 63rd An-_\n_nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pp.\n238–255, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8[89176-252-7. doi: 10.18653/v1/2025.acl-short.20. URL https://aclanthology.org/](https://aclanthology.org/2025.acl-short.20/)\n[2025.acl-short.20/.](https://aclanthology.org/2025.acl-short.20/)\n\n\nRishi Bommasani and Claire Cardie. Intrinsic evaluation of summarization datasets. In Bonnie\nWebber, Trevor Cohn, Yulan He, and Yang Liu (eds.), _Proceedings of the 2020 Conference on_\n_Empirical Methods in Natural Language Processing (EMNLP)_, pp. 8075–8096, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.649.\n[URL https://aclanthology.org/2020.emnlp-main.649/.](https://aclanthology.org/2020.emnlp-main.649/)\n\n\nSamuel R. Bowman and George Dahl. What will it take to fix benchmarking in natural language understanding? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\nHakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou\n(eds.), _Proceedings of the 2021 Conference of the North American Chapter of the Association_\n_for Computational Linguistics: Human Language Technologies_, pp. 4843–4855, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.385. URL\n[https://aclanthology.org/2021.naacl-main.385/.](https://aclanthology.org/2021.naacl-main.385/)\n\n\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method\nof paired comparisons. _Biometrika_, 39(3/4):324–345, 1952.\n\n\nNitay Calderon, Roi Reichart, and Rotem Dror. The alternative annotator test for LLM-as-a-judge:\nHow to statistically justify replacing human annotators with LLMs. In Wanxiang Che, Joyce\nNabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), _Proceedings of the 63rd_\n_Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp.\n16051–16081, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979[8-89176-251-0. doi: 10.18653/v1/2025.acl-long.782. URL https://aclanthology.org/](https://aclanthology.org/2025.acl-long.782/)\n[2025.acl-long.782/.](https://aclanthology.org/2025.acl-long.782/)\n\n\nChris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. (meta) evaluation of machine translation. In Chris Callison-Burch, Philipp Koehn, Cameron Shaw\nFordyce, and Christof Monz (eds.), _Proceedings of the Second Workshop on Statistical Machine_\n_Translation_, pp. 136–158, Prague, Czech Republic, June 2007. Association for Computational\n[Linguistics. URL https://aclanthology.org/W07-0718/.](https://aclanthology.org/W07-0718/)\n\n\nStephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J´er´emy Scheurer, Javier\nRando, Rachel Freedman, Tomek Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel\nMarks, Charbel-Raphael Segerie, Micah Carroll, Andi Peng, Phillip J.K. Christoffersen, Mehul\nDamani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud,\nJacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca\nDragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. _Transactions on Machine_\n_Learning Research_ [, 2023. ISSN 2835-8856. URL https://openreview.net/forum?](https://openreview.net/forum?id=bx24KpJ4Eb)\n[id=bx24KpJ4Eb. Survey Certification, Featured Certification.](https://openreview.net/forum?id=bx24KpJ4Eb)\n\n\nKhaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, and Golnoosh Farnadi. Neither\nvalid nor reliable? investigating the use of llms as judges. _arXiv preprint arXiv:2508.18076_, 2025.\n\n\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. InfoXLM: An information-theoretic framework for crosslingual language model pre-training. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer,\nDilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\nZhou (eds.), _Proceedings of the 2021 Conference of the North American Chapter of the Asso-_\n_ciation for Computational Linguistics: Human Language Technologies_, pp. 3576–3588, Online,\nJune 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.280.\n[URL https://aclanthology.org/2021.naacl-main.280/.](https://aclanthology.org/2021.naacl-main.280/)\n\n\nLeshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. On the weaknesses of reinforcement\nlearning for neural machine translation. In _8th International Conference on Learning Represen-_\n_tations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_ . OpenReview.net, 2020. URL\n[https://openreview.net/forum?id=H1eCw3EKvH.](https://openreview.net/forum?id=H1eCw3EKvH)\n\n\n11\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences. In Isabelle Guyon, Ulrike von Luxburg,\nSamy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett\n(eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neu-_\n_ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp.\n[4299–4307, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)\n[d5e2c0adad503c91f91df240d0cd4e49-Abstract.html.](https://proceedings.neurips.cc/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)\n\n\nElizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann, Joshua Maynez, Roee Aharoni, Vitaly Nikolaev, Thibault Sellam, Aditya Siddhant, Dipanjan Das, and Ankur Parikh. SEAHORSE: A multilingual, multifaceted dataset for summarization evaluation. In Houda Bouamor, Juan Pino, and\nKalika Bali (eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Lan-_\n_guage Processing_, pp. 9397–9413, Singapore, December 2023. Association for Computational\n[Linguistics. doi: 10.18653/v1/2023.emnlp-main.584. URL https://aclanthology.org/](https://aclanthology.org/2023.emnlp-main.584/)\n[2023.emnlp-main.584/.](https://aclanthology.org/2023.emnlp-main.584/)\n\n\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. _arXiv preprint arXiv:2507.06261_, 2025.\n\n\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In\n_Conference on Empirical Methods in Natural Language Processing_ [, 2018. URL https://](https://api.semanticscholar.org/CorpusID:52271711)\n[api.semanticscholar.org/CorpusID:52271711.](https://api.semanticscholar.org/CorpusID:52271711)\n\n\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong\nXie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai\nfeedback. _arXiv preprint arXiv:2310.01377_, 2023.\n\n\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian\nStrub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru,\nBharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi,\nAmir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagn´e, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan\nGomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Ust¨un, and Sara Hooker. Aya ex- [¨]\n[panse: Combining research breakthroughs for a new multilingual frontier, 2024. URL https:](https://arxiv.org/abs/2412.04261)\n[//arxiv.org/abs/2412.04261.](https://arxiv.org/abs/2412.04261)\n\n\nHal Daum´e, John Langford, and Daniel Marcu. Search-based structured prediction. _Machine Learn-_\n_ing_ [, 75(3):297–325, 2009. doi: 10.1007/S10994-009-5106-X. URL https://doi.org/10.](https://doi.org/10.1007/s10994-009-5106-x)\n[1007/s10994-009-5106-x.](https://doi.org/10.1007/s10994-009-5106-x)\n\n\nKaustubh Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood,\nAbinaya Mahadiran, Simon Mille, Ashish Shrivastava, Samson Tan, Tongshang Wu, Jascha SohlDickstein, Jinho Choi, Eduard Hovy, Ondˇrej Duˇsek, Sebastian Ruder, Sajant Anand, Nagender\nAneja, Rabin Banjade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla Cabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary, Christian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit, Thomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana\nEkeinhor, Marco Di Giovanni, Tanya Goyal, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada, Antoine Honor´e, Ishan Jindal, Przemysław Joniak, Denis Kleyko, Venelin\nKovatchev, Kalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae Ryan Lee, Corey James\nLevinson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi Marivate,\nGerard de Melo, Simon Meoni, Maxine Meyer, Afnan Mir, Nafise Sadat Moosavi, Niklas Meunnighoff, Timothy Sum Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova, Priti\nOli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicholas Roberts, Juan Diego\nRodriguez, Claude Roux, Vasconcellos Samus, Ananya Sai, Robin Schmidt, Thomas Scialom,\n\n\n12\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nTshephisho Sefara, Saqib Shamsi, Xudong Shen, Yiwen Shi, Haoyue Shi, Anna Shvets, Nick\nSiegel, Damien Sileo, Jamie Simon, Chandan Singh, Roman Sitelew, Priyank Soni, Taylor\nSorensen, William Soto, Aman Srivastava, Aditya Srivatsa, Tony Sun, Mukund Varma, A Tabassum, Fiona Tan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Zijie Wang, Gloria Wang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata, Xinyu Wu, Witold\nWydmanski, Tianbao Xie, Usama Yaseen, Michael Yee, Jing Zhang, and Yue Zhang. NLaugmenter: A framework for task-sensitive natural language augmentation. _Northern Euro-_\n_pean Journal of Language Technology_, 9, 2023. doi: 10.3384/nejlt.2000-1533.2023.4725. URL\n[https://aclanthology.org/2023.nejlt-1.5/.](https://aclanthology.org/2023.nejlt-1.5/)\n\n\nRichard O. Duda and Peter E. Hart. Pattern classification and scene analysis. In _A_\n_Wiley-Interscience publication_, 1974. [URL https://api.semanticscholar.org/](https://api.semanticscholar.org/CorpusID:12946615)\n[CorpusID:12946615.](https://api.semanticscholar.org/CorpusID:12946615)\n\n\nKawin Ethayarajh and Dan Jurafsky. Utility is in the eye of the user: A critique of NLP leaderboards. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), _Proceedings of the 2020_\n_Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 4846–4853,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n[emnlp-main.393. URL https://aclanthology.org/2020.emnlp-main.393/.](https://aclanthology.org/2020.emnlp-main.393/)\n\n\nMatan Eyal, Tal Baumel, and Michael Elhadad. Question answering as an automatic evaluation metric for news article summarization. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.),\n_Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-_\n_putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp.\n3938–3948, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:\n[10.18653/v1/N19-1395. URL https://aclanthology.org/N19-1395/.](https://aclanthology.org/N19-1395/)\n\n\nAlexander R. Fabbri, Chien Sheng Wu, Wenhao Liu, and Caiming Xiong. Qafacteval: Improved\nqa-based factual consistency evaluation for summarization. In _North American Chapter of the_\n_Association for Computational Linguistics_ [, 2021. URL https://api.semanticscholar.](https://api.semanticscholar.org/CorpusID:245218667)\n[org/CorpusID:245218667.](https://api.semanticscholar.org/CorpusID:245218667)\n\n\nBenjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley,\nMax Cembalest, and John P Dickerson. Style outweighs substance: Failure modes of LLM judges\nin alignment benchmarking. In _The Thirteenth International Conference on Learning Represen-_\n_tations_ [, 2025. URL https://openreview.net/forum?id=MzHNftnAM1.](https://openreview.net/forum?id=MzHNftnAM1)\n\n\nAdam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete\nShaw, and Jonathan Berant. Robust preference optimization through reward model distillation.\n_arXiv preprint arXiv:2405.19316_, 2024.\n\n\nMarkus Freitag, David Grangier, and Isaac Caswell. BLEU might be guilty but references are not\ninnocent. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), _Proceedings of the_\n_2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 61–71,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\n[emnlp-main.5. URL https://aclanthology.org/2020.emnlp-main.5/.](https://aclanthology.org/2020.emnlp-main.5/)\n\n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang\nMacherey. Experts, errors, and context: A large-scale study of human evaluation for machine\ntranslation. _Transactions of the Association for Computational Linguistics_, 9:1460–1474, 2021a.\ndoi: 10.1162/tacl ~~a 0~~ [0437. URL https://aclanthology.org/2021.tacl-1.87/.](https://aclanthology.org/2021.tacl-1.87/)\n\n\nMarkus Freitag, David Grangier, Qijun Tan, and Bowen Liang. High quality rather than high\nmodel probability: Minimum bayes risk decoding with neural metrics. _Transactions of_\n_the Association for Computational Linguistics_, 10:811–825, 2021b. [URL https://api.](https://api.semanticscholar.org/CorpusID:248392447)\n[semanticscholar.org/CorpusID:248392447.](https://api.semanticscholar.org/CorpusID:248392447)\n\n\nMarkus Freitag, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei,\nBrian Thompson, Frederic Blain, Tom Kocmi, Jiayi Wang, David Ifeoluwa Adelani, Marianna\nBuchicchio, Chrysoula Zerva, and Alon Lavie. Are LLMs breaking MT metrics? results of\nthe WMT24 metrics shared task. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof\nMonz (eds.), _Proceedings of the Ninth Conference on Machine Translation_, pp. 47–81, Miami,\n\n\n13\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nFlorida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n[2024.wmt-1.2. URL https://aclanthology.org/2024.wmt-1.2/.](https://aclanthology.org/2024.wmt-1.2/)\n\n\nEvan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Nikolas Angelopoulos, Jiantao\nJiao, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. How to evaluate reward models for\nRLHF. In _The Thirteenth International Conference on Learning Representations_, 2025. URL\n[https://openreview.net/forum?id=cbttLtO94Q.](https://openreview.net/forum?id=cbttLtO94Q)\n\n\nSaadia Gabriel, Asli Celikyilmaz, Rahul Jha, Yejin Choi, and Jianfeng Gao. GO FIGURE: A meta\nevaluation of factuality in summarization. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto\nNavigli (eds.), _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_,\npp. 478–487, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/\n[v1/2021.findings-acl.42. URL https://aclanthology.org/2021.findings-acl.](https://aclanthology.org/2021.findings-acl.42/)\n[42/.](https://aclanthology.org/2021.findings-acl.42/)\n\n\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In\n_International Conference on Machine Learning_, pp. 10835–10866. PMLR, 2023.\n\n\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondˇrej Duˇsek, Chris Chinenye\nEmezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak,\nAman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg,\nMoin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei,\nAnkur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo˜ao Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi\nYang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation,\nits evaluation and metrics. In Antoine Bosselut, Esin Durmus, Varun Prashant Gangal, Sebastian Gehrmann, Yacine Jernite, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu (eds.),\n_Proceedings of the First Workshop on Natural Language Generation, Evaluation, and Metrics_\n_(GEM)_, pp. 96–120, Online, August 2021. Association for Computational Linguistics. doi:\n[10.18653/v1/2021.gem-1.10. URL https://aclanthology.org/2021.gem-1.10/.](https://aclanthology.org/2021.gem-1.10/)\n\n\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foundation:\nA survey of obstacles in evaluation practices for generated text. _Journal Artificial Intelligence_\n_Research_ [, 77:103–166, 2023. doi: 10.1613/JAIR.1.13715. URL https://doi.org/10.](https://doi.org/10.1613/jair.1.13715)\n[1613/jair.1.13715.](https://doi.org/10.1613/jair.1.13715)\n\n\nSebastian Gehrmann, Claire Huang, Xian Teng, Sergei Yurovski, Arjun Bhorkar, Naveen Thomas,\nJohn Doucette, David Rosenberg, Mark Dredze, and David Rabinowitz. Understanding\nand mitigating risks of generative ai in financial services. _Proceedings of the 2025 ACM_\n_Conference on Fairness, Accountability, and Transparency_, 2025. [URL https://api.](https://api.semanticscholar.org/CorpusID:278170940)\n[semanticscholar.org/CorpusID:278170940.](https://api.semanticscholar.org/CorpusID:278170940)\n\n\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In\nZ. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), _Ad-_\n_vances in Neural Information Processing Systems_, volume 27. Curran Associates, Inc.,\n2014. [URL https://proceedings.neurips.cc/paper_files/paper/2014/](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)\n[file/f033ed80deb0234979a61f95710dbe25-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf)\n\n\nCharles AE Goodhart. Problems of monetary management: the uk experience. In _Monetary theory_\n_and practice: The UK experience_, pp. 91–121. Springer, 1984.\n\n\nAnisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Bing Liu, and Sean Hendryx. Rubrics\nas rewards: Reinforcement learning beyond verifiable domains. _CoRR_, abs/2507.17746, 2025.\n[doi: 10.48550/ARXIV.2507.17746. URL https://doi.org/10.48550/arXiv.2507.](https://doi.org/10.48550/arXiv.2507.17746)\n[17746.](https://doi.org/10.48550/arXiv.2507.17746)\n\n\n14\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. _arXiv preprint arXiv:2501.12948_, 2025.\n\n\nSrishti Gureja, Lester James Validad Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti\nSharma, Gusti Triandi Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Marzieh\nFadaee. M-RewardBench: Evaluating reward models in multilingual settings. In Wanxiang Che,\nJoyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), _Proceedings of the_\n_63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_,\npp. 43–58, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979[8-89176-251-0. doi: 10.18653/v1/2025.acl-long.3. URL https://aclanthology.org/](https://aclanthology.org/2025.acl-long.3/)\n[2025.acl-long.3/.](https://aclanthology.org/2025.acl-long.3/)\n\n\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin\nKang, M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive\nsummarization for 44 languages. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli\n(eds.), _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pp. 4693–\n4703, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\n[findings-acl.413. URL https://aclanthology.org/2021.findings-acl.413/.](https://aclanthology.org/2021.findings-acl.413/)\n\n\nHelia Hashemi, Jason Eisner, Corby Rosset, Benjamin Van Durme, and Chris Kedzie. LLM-rubric:\nA multidimensional, calibrated approach to automated evaluation of natural language texts. In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting_\n_of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 13806–13834,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n[2024.acl-long.745. URL https://aclanthology.org/2024.acl-long.745/.](https://aclanthology.org/2024.acl-long.745/)\n\n\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen,\nThomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. TRUE: Re-evaluating factual consistency evaluation. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir\nMeza Ruiz (eds.), _Proceedings of the 2022 Conference of the North American Chapter of the As-_\n_sociation for Computational Linguistics: Human Language Technologies_, pp. 3905–3920, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\n[naacl-main.287. URL https://aclanthology.org/2022.naacl-main.287/.](https://aclanthology.org/2022.naacl-main.287/)\n\n\nDavid M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad\nMahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. Twenty\nyears of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Brian Davis, Yvette Graham, John Kelleher, and Yaji Sripada (eds.), _Proceedings of the_\n_13th International Conference on Natural Language Generation_, pp. 169–182, Dublin, Ireland,\nDecember 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.inlg-1.23.\n[URL https://aclanthology.org/2020.inlg-1.23/.](https://aclanthology.org/2020.inlg-1.23/)\n\n\nXinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. Are\nLLM-based evaluators confusing NLG quality criteria? In Lun-Wei Ku, Andre Martins, and\nVivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Com-_\n_putational Linguistics (Volume 1: Long Papers)_, pp. 9530–9570, Bangkok, Thailand, August\n2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.516. URL\n[https://aclanthology.org/2024.acl-long.516/.](https://aclanthology.org/2024.acl-long.516/)\n\n\nHui Huang, Xingyuan Bu, Hongli Zhou, Yingqi Qu, Jing Liu, Muyun Yang, Bing Xu, and Tiejun\nZhao. An empirical study of LLM-as-a-judge for LLM evaluation: Fine-tuned judge model\nis not a general substitute for GPT-4. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar (eds.), _Findings of the Association for Computational Lin-_\n_guistics: ACL 2025_, pp. 5880–5895, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.306. URL\n[https://aclanthology.org/2025.findings-acl.306/.](https://aclanthology.org/2025.findings-acl.306/)\n\n\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. _arXiv preprint_\n_arXiv:2410.21276_, 2024.\n\n\n15\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nHamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert,\nNoah A Smith, Yejin Choi, and Hanna Hajishirzi. Unpacking dpo and ppo: Disentangling best\npractices for learning from preference feedback. _Advances in neural information processing sys-_\n_tems_, 37:36602–36633, 2024.\n\n\nAlon Jacovi, Jasmijn Bastings, Sebastian Gehrmann, Yoav Goldberg, and Katja Filippova. Diagnosing ai explanation methods with folk concepts of behavior. _Journal of Artificial Intelligence_\n_Research_, 78:459–489, 2023.\n\n\nAlon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas,\nMichelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron\nKukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy,\nMichael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein,\nand Dipanjan Das. The FACTS grounding leaderboard: Benchmarking llms’ ability to ground\nresponses to long-form input. _CoRR_, abs/2501.03200, 2025. doi: 10.48550/ARXIV.2501.03200.\n[URL https://doi.org/10.48550/arXiv.2501.03200.](https://doi.org/10.48550/arXiv.2501.03200)\n\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM_\n_computing surveys_, 55(12):1–38, 2023.\n\n\nZixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li,\nChengwei Qin, Peifeng Wang, Silvio Savarese, et al. A survey of frontiers in llm reasoning:\nInference scaling, learning to reason, and agentic systems. _arXiv preprint arXiv:2504.09037_,\n2025.\n\n\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,\nSeongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing\nfine-grained evaluation capability in language models. In _The Twelfth International Confer-_\n_ence on Learning Representations_ [, 2024. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=8euJaTveKw)\n[8euJaTveKw.](https://openreview.net/forum?id=8euJaTveKw)\n\n\nSunghwan Kim, Dongjin Kang, Taeyoon Kwon, Hyungjoo Chae, Dongha Lee, and Jinyoung\nYeo. Rethinking reward model evaluation through the lens of reward overoptimization. In\nWanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), _Pro-_\n_ceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Vol-_\n_ume 1: Long Papers)_, pp. 13252–13280, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.649. URL\n[https://aclanthology.org/2025.acl-long.649/.](https://aclanthology.org/2025.acl-long.649/)\n\n\nKimi, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru\nChen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. _arXiv preprint_\n_arXiv:2507.20534_, 2025.\n\n\nRodney Michael Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles\nCrawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph\nGorney, David W. Graham, F.Q. Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey\nKuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler C. Murray, Christopher Newell, Smita R Rao, Shaurya Rohatgi, Paul Sayre, Zejiang\nShen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, A. Tanaka, Alex D Wade,\nLinda M. Wagner, Lucy Lu Wang, Christopher Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine van Zuylen, and Daniel S. Weld. The semantic scholar open data\nplatform. _ArXiv_ [, abs/2301.10140, 2023. URL https://api.semanticscholar.org/](https://api.semanticscholar.org/CorpusID:256194545)\n[CorpusID:256194545.](https://api.semanticscholar.org/CorpusID:256194545)\n\n\nBernard J. Koch and David Peterson. From protoscience to epistemic monoculture: How benchmarking set the stage for the deep learning revolution. _CoRR_, abs/2404.06647, 2024. doi: 10.\n[48550/ARXIV.2404.06647. URL https://doi.org/10.48550/arXiv.2404.06647.](https://doi.org/10.48550/arXiv.2404.06647)\n\n\n16\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nTom Kocmi, Vil´em Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze:\nReconciling score magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Lin-_\n_guistics (Volume 1: Long Papers)_, pp. 1999–2014, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.110. [URL https:](https://aclanthology.org/2024.acl-long.110/)\n[//aclanthology.org/2024.acl-long.110/.](https://aclanthology.org/2024.acl-long.110/)\n\n\nVictoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt,\nRamana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specification gaming:\nthe flip side of ai ingenuity. [https://deepmind.google/discover/blog/](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)\n[specification-gaming-the-flip-side-of-ai-ingenuity/, 2020.](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)\n\n\nMichael Krumdick, Charles Lovering, Varshini Reddy, Seth Ebner, and Chris Tanner. No free labels:\nLimitations of llm-as-a-judge without human grounding. _arXiv preprint arXiv:2503.05061_, 2025.\n\n\nWojciech Kry´sci´nski, Romain Paulus, Caiming Xiong, and Richard Socher. Improving abstraction in\ntext summarization. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.),\n_Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pp.\n1808–1817, Brussels, Belgium, October-November 2018. Association for Computational Lin[guistics. doi: 10.18653/v1/D18-1207. URL https://aclanthology.org/D18-1207/.](https://aclanthology.org/D18-1207/)\n\n\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Trevor Cohn, Yulan He, and Yang Liu\n(eds.), _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 4034–4048,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.360. [URL https://aclanthology.org/2020.findings-emnlp.](https://aclanthology.org/2020.findings-emnlp.360/)\n[360/.](https://aclanthology.org/2020.findings-emnlp.360/)\n\n\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers\nin open language model post-training. _arXiv preprint arXiv:2411.15124_, 2024.\n\n\nNathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu,\nNouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi.\nRewardBench: Evaluating reward models for language modeling. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), _Findings of the Association for Computational Linguistics: NAACL_\n_2025_, pp. 1755–1797, Albuquerque, New Mexico, April 2025. Association for Computational\n[Linguistics. ISBN 979-8-89176-195-7. doi: 10.18653/v1/2025.findings-naacl.96. URL https:](https://aclanthology.org/2025.findings-naacl.96/)\n[//aclanthology.org/2025.findings-naacl.96/.](https://aclanthology.org/2025.findings-naacl.96/)\n\n\nYann LeCun. Predictive learning. [https://www.youtube.com/watch?v=](https://www.youtube.com/watch?v=Ount2Y4qxQo&t=1072s)\n[Ount2Y4qxQo&t=1072s, 2016.](https://www.youtube.com/watch?v=Ount2Y4qxQo&t=1072s)\n\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton\nBishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs. RLHF:\nscaling reinforcement learning from human feedback with AI feedback. In _Forty-first Interna-_\n_tional Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_ . Open[Review.net, 2024a. URL https://openreview.net/forum?id=uydQ2W41KO.](https://openreview.net/forum?id=uydQ2W41KO)\n\n\nYukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Jaewook Kang, Pilsung Kang, and Najoung Kim. Checkeval: A reliable llm-as-a-judge framework for evaluating text generation using\nchecklists. _arXiv preprint arXiv:2403.18771_, 2024b.\n\n\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable\nagent alignment via reward modeling: a research direction. _ArXiv_, abs/1811.07871, 2018. URL\n[https://api.semanticscholar.org/CorpusID:53745764.](https://api.semanticscholar.org/CorpusID:53745764)\n\n\nJixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. Taming overconfidence in LLMs:\nReward calibration in RLHF. In _The Thirteenth International Conference on Learning Represen-_\n_tations_ [, 2025. URL https://openreview.net/forum?id=l0tg0jzsdL.](https://openreview.net/forum?id=l0tg0jzsdL)\n\n\n17\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Dan Jurafsky,\nJoyce Chai, Natalie Schluter, and Joel Tetreault (eds.), _Proceedings of the 58th Annual Meet-_\n_ing of the Association for Computational Linguistics_, pp. 7871–7880, Online, July 2020. As[sociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703. URL https:](https://aclanthology.org/2020.acl-main.703/)\n[//aclanthology.org/2020.acl-main.703/.](https://aclanthology.org/2020.acl-main.703/)\n\n\nPiji Li, Lidong Bing, and Wai Lam. Actor-critic based training framework for abstractive summarization. _CoRR_ [, abs/1803.11070, 2018. URL http://arxiv.org/abs/1803.11070.](http://arxiv.org/abs/1803.11070)\n\n\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In _The Twelfth_\n_International Conference on Learning Representations_ [, 2024. URL https://openreview.](https://openreview.net/forum?id=v8L0pN6EOi)\n[net/forum?id=v8L0pN6EOi.](https://openreview.net/forum?id=v8L0pN6EOi)\n\n\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization_\n_Branches Out_, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguis[tics. URL https://aclanthology.org/W04-1013/.](https://aclanthology.org/W04-1013/)\n\n\nChris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang\nLiu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. _arXiv preprint_\n_arXiv:2410.18451_, 2024.\n\n\nSiqi Liu, Ian Gemp, Luke Marris, Georgios Piliouras, Nicolas Heess, and Marc Lanctot. Reevaluating open-ended evaluation of large language models. In _The Thirteenth International Con-_\n_ference on Learning Representations_ [, 2025a. URL https://openreview.net/forum?](https://openreview.net/forum?id=kbOAIXKWgx)\n[id=kbOAIXKWgx.](https://openreview.net/forum?id=kbOAIXKWgx)\n\n\nTianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen,\nZhen Qin, Tianhe Yu, Daniel Sohn, Anastasia Makarova, Jeremiah Zhe Liu, Yuan Liu, Bilal Piot,\nAbe Ittycheriah, Aviral Kumar, and Mohammad Saleh. RRM: Robust reward model training mitigates reward hacking. In _The Thirteenth International Conference on Learning Representations_,\n[2025b. URL https://openreview.net/forum?id=88AS5MQnmC.](https://openreview.net/forum?id=88AS5MQnmC)\n\n\nYantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking\nreward models of language models with subtlety and style. In _The Thirteenth International Con-_\n_ference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025_ . OpenReview.net,\n[2025c. URL https://openreview.net/forum?id=QEHrmQPBdd.](https://openreview.net/forum?id=QEHrmQPBdd)\n\n\nQingsong Ma, Ondˇrej Bojar, and Yvette Graham. Results of the WMT18 metrics shared task:\nBoth characters and embeddings achieve good performance. In Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur´elie N´ev´eol, Mariana\nNeves, Matt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), _Proceedings of the_\n_Third Conference on Machine Translation: Shared Task Papers_, pp. 671–688, Belgium, Brussels,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6450. URL\n[https://aclanthology.org/W18-6450/.](https://aclanthology.org/W18-6450/)\n\n\nQingsong Ma, Johnny Wei, Ondˇrej Bojar, and Yvette Graham. Results of the WMT19 metrics\nshared task: Segment-level and strong MT systems pose big challenges. In Ondˇrej Bojar, Rajen\nChatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck,\nAntonio Jimeno Yepes, Philipp Koehn, Andr´e Martins, Christof Monz, Matteo Negri, Aur´elie\nN´ev´eol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), _Proceedings of_\n_the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)_, pp. 62–\n90, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/\n[W19-5302. URL https://aclanthology.org/W19-5302/.](https://aclanthology.org/W19-5302/)\n\n\nSaumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah A. Smith, Hannaneh Hajishirzi, and Nathan Lambert. Rewardbench 2: Advancing reward model evaluation. _CoRR_,\n[abs/2506.01937, 2025. doi: 10.48550/ARXIV.2506.01937. URL https://doi.org/10.](https://doi.org/10.48550/arXiv.2506.01937)\n[48550/arXiv.2506.01937.](https://doi.org/10.48550/arXiv.2506.01937)\n\n\n18\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nDavid Manheim and Scott Garrabrant. Categorizing variants of goodhart’s law. _arXiv preprint_\n_arXiv:1803.04585_, 2018.\n\n\nNitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ondˇrej Bojar. Results of the\nWMT20 metrics shared task. In Lo¨ıc Barrault, Ondˇrej Bojar, Fethi Bougares, Rajen Chatterjee,\nMarta R. Costa-juss`a, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco\nGuzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Andr´e Martins,\nMakoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri (eds.),\n_Proceedings of the Fifth Conference on Machine Translation_, pp. 688–725, Online, November\n2020. Association for Computational Linguistics. [URL https://aclanthology.org/](https://aclanthology.org/2020.wmt-1.77/)\n[2020.wmt-1.77/.](https://aclanthology.org/2020.wmt-1.77/)\n\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality\nin abstractive summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault\n(eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_,\npp. 1906–1919, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n[2020.acl-main.173. URL https://aclanthology.org/2020.acl-main.173/.](https://aclanthology.org/2020.acl-main.173/)\n\n\nR. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Anna Korhonen, David Traum, and Llu´ıs M`arquez\n(eds.), _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_,\npp. 3428–3448, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.\n[18653/v1/P19-1334. URL https://aclanthology.org/P19-1334/.](https://aclanthology.org/P19-1334/)\n\n\nYannick Metz, Andras Geiszl, Rapha¨el Baur, and Mennatallah El-Assady. Reward learning from\nmultiple feedback types. In _The Thirteenth International Conference on Learning Representa-_\n_tions_ [, 2025. URL https://openreview.net/forum?id=9Ieq8jQNAl.](https://openreview.net/forum?id=9Ieq8jQNAl)\n\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Houda Bouamor, Juan Pino, and Kalika Bali\n(eds.), _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-_\n_cessing_, pp. 12076–12100, Singapore, December 2023. Association for Computational Linguis[tics. doi: 10.18653/v1/2023.emnlp-main.741. URL https://aclanthology.org/2023.](https://aclanthology.org/2023.emnlp-main.741/)\n[emnlp-main.741/.](https://aclanthology.org/2023.emnlp-main.741/)\n\n\nVolodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual attention. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), _Advances in Neural Information Processing Systems 27: Annual Con-_\n_ference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Que-_\n_bec, Canada_ [, pp. 2204–2212, 2014. URL https://proceedings.neurips.cc/paper/](https://proceedings.neurips.cc/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html)\n[2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html.](https://proceedings.neurips.cc/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html)\n\n\nBhuvanashree Murugadoss, Christian P¨olitz, Ian Drosos, Vu Le, Nick McKenna, Carina Suzana\nNegreanu, Chris Parnin, and Advait Sarkar. Evaluating the evaluator: Measuring llms’ adherence\nto task evaluation instructions. In Toby Walsh, Julie Shah, and Zico Kolter (eds.), _AAAI-25,_\n_Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March_\n_4, 2025, Philadelphia, PA, USA_, pp. 19589–19597. AAAI Press, 2025. doi: 10.1609/AAAI.\n[V39I18.34157. URL https://doi.org/10.1609/aaai.v39i18.34157.](https://doi.org/10.1609/aaai.v39i18.34157)\n\n\nVaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure\nmodes of out-of-distribution generalization. In _International Conference on Learning Repre-_\n_sentations_ [, 2021. URL https://openreview.net/forum?id=fSTD6NFIW_b.](https://openreview.net/forum?id=fSTD6NFIW_b)\n\n\nOpenAI. Gpt-5 system card, Aug 2025. URL [https://cdn.openai.com/](https://cdn.openai.com/gpt-5-system-card.pdf)\n[gpt-5-system-card.pdf.](https://cdn.openai.com/gpt-5-system-card.pdf)\n\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike,\nand Ryan Lowe. Training language models to follow instructions with human feedback. In _Pro-_\n_ceedings of the 36th International Conference on Neural Information Processing Systems_, NIPS\n’22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088.\n\n\n19\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.),\n_Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics_, pp.\n311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguis[tics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040/.](https://aclanthology.org/P02-1040/)\n\n\nRamakanth Pasunuru and Mohit Bansal. Reinforced video captioning with entailment rewards. In\nMartha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), _Proceedings of the 2017 Conference_\n_on Empirical Methods in Natural Language Processing_, pp. 979–985, Copenhagen, Denmark,\nSeptember 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1103. URL\n[https://aclanthology.org/D17-1103/.](https://aclanthology.org/D17-1103/)\n\n\nRamakanth Pasunuru and Mohit Bansal. Multi-reward reinforced summarization with saliency and\nentailment. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), _Proceedings of the 2018 Con-_\n_ference of the North American Chapter of the Association for Computational Linguistics: Hu-_\n_man Language Technologies, Volume 2 (Short Papers)_, pp. 646–653, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2102. URL\n[https://aclanthology.org/N18-2102/.](https://aclanthology.org/N18-2102/)\n\n\nRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. In _International Conference on Learning Representations_ [, 2018. URL https:](https://openreview.net/forum?id=HkAClQgA-)\n[//openreview.net/forum?id=HkAClQgA-.](https://openreview.net/forum?id=HkAClQgA-)\n\n\nStefano Perrella, Lorenzo Proietti, Alessandro Scir`e, Edoardo Barba, and Roberto Navigli.\nGuardians of the machine translation meta-evaluation: Sentinel metrics fall in! In Lun-Wei Ku,\nAndre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Associ-_\n_ation for Computational Linguistics (Volume 1: Long Papers)_, pp. 16216–16244, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.\n[856. URL https://aclanthology.org/2024.acl-long.856/.](https://aclanthology.org/2024.acl-long.856/)\n\n\nMatt Post. A call for clarity in reporting BLEU scores. In Ondˇrej Bojar, Rajen Chatterjee,\nChristian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, Matteo Negri, Aur´elie N´ev´eol, Mariana Neves,\nMatt Post, Lucia Specia, Marco Turchi, and Karin Verspoor (eds.), _Proceedings of the Third_\n_Conference on Machine Translation: Research Papers_, pp. 186–191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL\n[https://aclanthology.org/W18-6319/.](https://aclanthology.org/W18-6319/)\n\n\nZhaopeng Qiu, Xian Wu, and Wei Fan. Automatic distractor generation for multiple choice questions\nin standard tests. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), _Proceedings of the_\n_28th International Conference on Computational Linguistics_, pp. 2096–2106, Barcelona, Spain\n(Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/\n[v1/2020.coling-main.189. URL https://aclanthology.org/2020.coling-main.](https://aclanthology.org/2020.coling-main.189/)\n[189/.](https://aclanthology.org/2020.coling-main.189/)\n\n\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\nIn Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine\n(eds.), _Advances in Neural Information Processing Systems 36: Annual Conference on Neural_\n_Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -_\n_16, 2023_ [, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/](http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)\n[a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html.](http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)\n\n\nDeborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne\nPaullada. Ai and the everything in the whole wide world benchmark. In J. Vanschoren and S. Yeung (eds.), _Proceedings_ _of_ _the_ _Neural_ _Information_ _Processing_\n_Systems_ _Track_ _on_ _Datasets_ _and_ _Benchmarks_, volume 1, 2021. URL [https:](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf)\n[//datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf)\n[2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf.](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf)\n\n\nPeter A. Rankel, John M. Conroy, Hoa Trang Dang, and Ani Nenkova. A decade of automatic\ncontent evaluation of news summaries: Reassessing the state of the art. In Hinrich Schuetze,\n\n\n20\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nPascale Fung, and Massimo Poesio (eds.), _Proceedings of the 51st Annual Meeting of the Asso-_\n_ciation for Computational Linguistics (Volume 2: Short Papers)_, pp. 131–136, Sofia, Bulgaria,\n[August 2013. Association for Computational Linguistics. URL https://aclanthology.](https://aclanthology.org/P13-2024/)\n[org/P13-2024/.](https://aclanthology.org/P13-2024/)\n\n\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. In Yoshua Bengio and Yann LeCun (eds.), _4th International_\n_Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,_\n_Conference Track Proceedings_ [, 2016. URL http://arxiv.org/abs/1511.06732.](http://arxiv.org/abs/1511.06732)\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das,\nSlav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural\nlanguage generation models. _Computational Linguistics_, 49(4):777–840, December 2023. doi:\n10.1162/coli a ~~0~~ [0486. URL https://aclanthology.org/2023.cl-4.2/.](https://aclanthology.org/2023.cl-4.2/)\n\n\nCharvi Rastogi, Tian Huey Teh, Pushkar Mishra, Roma Patel, Zoe Ashwood, Aida Mostafazadeh\nDavani, Mark Diaz, Michela Paganini, Alicia Parrish, Ding Wang, Vinodkumar Prabhakaran,\nLora Aroyo, and Verena Rieser. Insights on disagreement patterns in multimodal safety perception\nacross diverse rater groups. In _Neurips Safe Generative AI Workshop 2024_ [, 2024. URL https:](https://openreview.net/forum?id=8TI0lUrJBP)\n[//openreview.net/forum?id=8TI0lUrJBP.](https://openreview.net/forum?id=8TI0lUrJBP)\n\n\nCharvi Rastogi, Tian Huey Teh, Pushkar Mishra, Roma Patel, Ding Wang, Mark D´ıaz, Alicia Parrish,\nAida Mostafazadeh Davani, Zoe Ashwood, Michela Paganini, et al. Whose view of safety? a deep\ndive dataset for pluralistic alignment of text-to-image models. _arXiv preprint arXiv:2507.13383_,\n2025.\n\n\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine\nMaroti, Jos´e G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie,\nand Andr´e F. T. Martins. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Philipp Koehn, Lo¨ıc Barrault, Ondˇrej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-juss`a, Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag,\nYvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio\nJimeno Yepes, Tom Kocmi, Andr´e Martins, Makoto Morishita, Christof Monz, Masaaki Nagata,\nToshiaki Nakazawa, Matteo Negri, Aur´elie N´ev´eol, Mariana Neves, Martin Popel, Marco Turchi,\nand Marcos Zampieri (eds.), _Proceedings of the Seventh Conference on Machine Translation_\n_(WMT)_, pp. 634–645, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association\n[for Computational Linguistics. URL https://aclanthology.org/2022.wmt-1.60/.](https://aclanthology.org/2022.wmt-1.60/)\n\n\nEhud Reiter. A structured review of the validity of BLEU. _Comput. Linguistics_, 44(3), 2018. doi:\n10.1162/COLI _\\_ ~~A~~ _\\_ [00322. URL https://doi.org/10.1162/coli_a_00322.](https://doi.org/10.1162/coli_a_00322)\n\n\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical\nsequence training for image captioning. In _2017 IEEE Conference on Computer Vision and Pat-_\n_tern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017_, pp. 1179–1195. IEEE Com[puter Society, 2017. doi: 10.1109/CVPR.2017.131. URL https://doi.org/10.1109/](https://doi.org/10.1109/CVPR.2017.131)\n[CVPR.2017.131.](https://doi.org/10.1109/CVPR.2017.131)\n\n\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the\npredictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference_\n_on knowledge discovery and data mining_, pp. 1135–1144, 2016.\n\n\nSt´ephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and\nstructured prediction to no-regret online learning. In Geoffrey J. Gordon, David B. Dunson,\nand Miroslav Dud´ık (eds.), _Proceedings of the Fourteenth International Conference on Artificial_\n_Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011_, volume 15\nof _JMLR Proceedings_ [, pp. 627–635. JMLR.org, 2011. URL http://proceedings.mlr.](http://proceedings.mlr.press/v15/ross11a/ross11a.pdf)\n[press/v15/ross11a/ross11a.pdf.](http://proceedings.mlr.press/v15/ross11a/ross11a.pdf)\n\n\nKeita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. Verbosity bias in preference\nlabeling by large language models. _CoRR_, abs/2310.10076, 2023. doi: 10.48550/ARXIV.2310.\n[10076. URL https://doi.org/10.48550/arXiv.2310.10076.](https://doi.org/10.48550/arXiv.2310.10076)\n\n\n21\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. _CoRR_ [, abs/1707.06347, 2017. URL http://arxiv.org/abs/](http://arxiv.org/abs/1707.06347)\n[1707.06347.](http://arxiv.org/abs/1707.06347)\n\n\nThomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. Answers unite! unsupervised metrics for reinforced summarization models. In Kentaro Inui, Jing Jiang, Vincent Ng,\nand Xiaojun Wan (eds.), _Proceedings of the 2019 Conference on Empirical Methods in Natural_\n_Language Processing and the 9th International Joint Conference on Natural Language Process-_\n_ing (EMNLP-IJCNLP)_, pp. 3246–3256, Hong Kong, China, November 2019. Association for\n[Computational Linguistics. doi: 10.18653/v1/D19-1320. URL https://aclanthology.](https://aclanthology.org/D19-1320/)\n[org/D19-1320/.](https://aclanthology.org/D19-1320/)\n\n\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. MLSUM: The multilingual summarization corpus. In Bonnie Webber, Trevor Cohn,\nYulan He, and Yang Liu (eds.), _Proceedings of the 2020 Conference on Empirical Methods_\n_in Natural Language Processing (EMNLP)_, pp. 8051–8067, Online, November 2020. Associ[ation for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.647. URL https:](https://aclanthology.org/2020.emnlp-main.647/)\n[//aclanthology.org/2020.emnlp-main.647/.](https://aclanthology.org/2020.emnlp-main.647/)\n\n\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex\nWang, and Patrick Gallinari. QuestEval: Summarization asks for fact-based evaluation. In MarieFrancine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), _Proceedings of_\n_the 2021 Conference on Empirical Methods in Natural Language Processing_, pp. 6594–6604,\nOnline and Punta Cana, Dominican Republic, November 2021. Association for Computational\n[Linguistics. doi: 10.18653/v1/2021.emnlp-main.529. URL https://aclanthology.org/](https://aclanthology.org/2021.emnlp-main.529/)\n[2021.emnlp-main.529/.](https://aclanthology.org/2021.emnlp-main.529/)\n\n\nDonia Scott and Johanna Moore. An nlg evaluation competition? eight reasons to be cautious.\n_Proceedings of the Workshop on Shared Tasks and Comparative Evaluation in Natural Language_\n_Generation_, 2006.\n\n\nThibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text\ngeneration. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), _Proceedings_\n_of the 58th Annual Meeting of the Association for Computational Linguistics_, pp. 7881–7892,\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.\n[704. URL https://aclanthology.org/2020.acl-main.704/.](https://aclanthology.org/2020.acl-main.704/)\n\n\nMrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R.\nBowman, Esin DURMUS, Zac Hatfield-Dodds, Scott R Johnston, Shauna M Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan,\nMiranda Zhang, and Ethan Perez. Towards understanding sycophancy in language models.\nIn _The Twelfth International Conference on Learning Representations_, 2024. [URL https:](https://openreview.net/forum?id=tvhaxkMKAn)\n[//openreview.net/forum?id=tvhaxkMKAn.](https://openreview.net/forum?id=tvhaxkMKAn)\n\n\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum\nrisk training for neural machine translation. In Katrin Erk and Noah A. Smith (eds.), _Proceedings_\n_of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-_\n_pers)_, pp. 1683–1692, Berlin, Germany, August 2016. Association for Computational Linguistics.\n[doi: 10.18653/v1/P16-1159. URL https://aclanthology.org/P16-1159/.](https://aclanthology.org/P16-1159/)\n\n\nRaphael Shu, Kang Min Yoo, and Jung-Woo Ha. Reward optimization for neural machine translation with learned metrics. _CoRR_ [, abs/2104.07541, 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2104.07541)\n[2104.07541.](https://arxiv.org/abs/2104.07541)\n\n\nShivalika Singh, Yiyang Nan, Alex Wang, Daniel D’Souza, Sayash Kapoor, Ahmet Ust¨un, Sanmi [¨]\nKoyejo, Yuntian Deng, Shayne Longpre, Noah A Smith, et al. The leaderboard illusion. _arXiv_\n_preprint arXiv:2504.20879_, 2025.\n\n\nJoar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\nand Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022. URL\n[https://openreview.net/forum?id=yb3HOXO3lX2.](https://openreview.net/forum?id=yb3HOXO3lX2)\n\n\n22\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. Learning to summarize from human feedback. In _Proceed-_\n_ings of the 34th International Conference on Neural Information Processing Systems_, NIPS ’20,\nRed Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\n\n\nHao Sun, Yunyi Shen, and Jean-Francois Ton. Rethinking reward modeling in preference-based\nlarge language model alignment. In _The Thirteenth International Conference on Learning Repre-_\n_sentations_ [, 2025. URL https://openreview.net/forum?id=rfdblE10qm.](https://openreview.net/forum?id=rfdblE10qm)\n\n\nJiao Sun, Thibault Sellam, Elizabeth Clark, Tu Vu, Timothy Dozat, Dan Garrette, Aditya Siddhant,\nJacob Eisenstein, and Sebastian Gehrmann. Dialect-robust evaluation of generated text. In Anna\nRogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meet-_\n_ing of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 6010–6028,\nToronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\n[acl-long.331. URL https://aclanthology.org/2023.acl-long.331/.](https://aclanthology.org/2023.acl-long.331/)\n\n\nSimeng Sun, Ori Shapira, Ido Dagan, and Ani Nenkova. How to compare summarizers without\ntarget length? pitfalls, solutions and re-examination of the neural summarization literature. In\nAntoine Bosselut, Asli Celikyilmaz, Marjan Ghazvininejad, Srinivasan Iyer, Urvashi Khandelwal, Hannah Rashkin, and Thomas Wolf (eds.), _Proceedings of the Workshop on Methods for_\n_Optimizing and Evaluating Neural Language Generation_, pp. 21–29, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-2303. URL\n[https://aclanthology.org/W19-2303/.](https://aclanthology.org/W19-2303/)\n\n\nRichard S. Sutton and Andrew G. Barto. _Reinforcement learning - an introduction, 2nd Edition_ . MIT\nPress, 2018. [URL http://www.incompleteideas.net/book/the-book-2nd.](http://www.incompleteideas.net/book/the-book-2nd.html)\n\n[html.](http://www.incompleteideas.net/book/the-book-2nd.html)\n\n\nRachel Thomas and David Uminsky. The problem with metrics is a fundamental problem for ai.\n_arXiv preprint arXiv:2002.08512_, 2020.\n\n\nBrian Thompson, Nitika Mathur, Daniel Deutsch, and Huda Khayrallah. Improving statistical significance in human evaluation of automatic metrics via soft pairwise accuracy. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz (eds.), _Proceedings of the Ninth Con-_\n_ference on Machine Translation_, pp. 1222–1234, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.wmt-1.118. [URL https:](https://aclanthology.org/2024.wmt-1.118/)\n[//aclanthology.org/2024.wmt-1.118/.](https://aclanthology.org/2024.wmt-1.118/)\n\n\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image\ndescription evaluation. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_\n_2015, Boston, MA, USA, June 7-12, 2015_, pp. 4566–4575. IEEE Computer Society, 2015. doi: 10.\n[1109/CVPR.2015.7299087. URL https://doi.org/10.1109/CVPR.2015.7299087.](https://doi.org/10.1109/CVPR.2015.7299087)\n\n\nAlex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault\n(eds.), _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_,\npp. 5008–5020, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/\n[2020.acl-main.450. URL https://aclanthology.org/2020.acl-main.450/.](https://aclanthology.org/2020.acl-main.450/)\n\n\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng\nKong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In\nLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meet-_\n_ing of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 9440–9450,\nBangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/\n[2024.acl-long.511. URL https://aclanthology.org/2024.acl-long.511/.](https://aclanthology.org/2024.acl-long.511/)\n\n\nJerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran,\nDaiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave,\nAngela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), _Advances in_\n_Neural Information Processing Systems 38:_ _Annual Conference on Neural Information_\n_Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15,_\n\n\n23\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n_2024_, 2024. [URL http://papers.nips.cc/paper_files/paper/2024/hash/](http://papers.nips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html)\n[937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html.](http://papers.nips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html)\n\n\nJohnny Wei and Robin Jia. The statistical advantage of automatic NLG metrics at the system level.\nIn Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), _Proceedings of the 59th_\n_Annual Meeting of the Association for Computational Linguistics and the 11th International Joint_\n_Conference on Natural Language Processing (Volume 1: Long Papers)_, pp. 6840–6854, Online,\nAugust 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.533.\n[URL https://aclanthology.org/2021.acl-long.533/.](https://aclanthology.org/2021.acl-long.533/)\n\n\nTianjun Wei, Wei Wen, Ruizhi Qiao, Xing Sun, and Jianghong Ma. Rocketeval: Efficient automated\nLLM evaluation via grading checklist. In _The Thirteenth International Conference on Learning_\n_Representations_ [, 2025. URL https://openreview.net/forum?id=zJjzNj6QUe.](https://openreview.net/forum?id=zJjzNj6QUe)\n\n\nJiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R.\nBowman, He He, and Shi Feng. Language models learn to mislead humans via RLHF. In\n_The Thirteenth International Conference on Learning Representations_ [, 2025a. URL https:](https://openreview.net/forum?id=xJljiPE6dg)\n[//openreview.net/forum?id=xJljiPE6dg.](https://openreview.net/forum?id=xJljiPE6dg)\n\n\nXueru Wen, Jie Lou, Zichao Li, Yaojie Lu, XingYu XingYu, Yuqiu Ji, Guohai Xu, Hongyu Lin,\nBen He, Xianpei Han, Le Sun, and Debing Zhang. Cheems: A practical guidance for building\nand evaluating Chinese reward models from scratch. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), _Proceedings of the 63rd Annual Meeting_\n_of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 15187–15211,\nVienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176[251-0. doi: 10.18653/v1/2025.acl-long.737. URL https://aclanthology.org/2025.](https://aclanthology.org/2025.acl-long.737/)\n[acl-long.737/.](https://aclanthology.org/2025.acl-long.737/)\n\n\nXueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, XingYu, Xinyu Lu, Ben He, Xianpei Han, Debing\nZhang, and Le Sun. Rethinking reward model evaluation: Are we barking up the wrong tree?\nIn _The Thirteenth International Conference on Learning Representations_ [, 2025c. URL https:](https://openreview.net/forum?id=Cnwz9jONi5)\n[//openreview.net/forum?id=Cnwz9jONi5.](https://openreview.net/forum?id=Cnwz9jONi5)\n\n\nRonald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. _Machine Learning_ [, 8:229–256, 1992. doi: 10.1007/BF00992696. URL https://](https://doi.org/10.1007/BF00992696)\n[doi.org/10.1007/BF00992696.](https://doi.org/10.1007/BF00992696)\n\n\nSam Wiseman and Alexander M. Rush. Sequence-to-sequence learning as beam-search optimization. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), _Proceedings of the 2016 Confer-_\n_ence on Empirical Methods in Natural Language Processing_, pp. 1296–1306, Austin, Texas,\nNovember 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1137. URL\n[https://aclanthology.org/D16-1137/.](https://aclanthology.org/D16-1137/)\n\n\nLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning\nfor neural machine translation. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi\nTsujii (eds.), _Proceedings of the 2018 Conference on Empirical Methods in Natural Language_\n_Processing_, pp. 3612–3621, Brussels, Belgium, October-November 2018a. Association for Com[putational Linguistics. doi: 10.18653/v1/D18-1397. URL https://aclanthology.org/](https://aclanthology.org/D18-1397/)\n[D18-1397/.](https://aclanthology.org/D18-1397/)\n\n\nLijun Wu, Yingce Xia, Fei Tian, Li Zhao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. Adversarial\nneural machine translation. In Jun Zhu and Ichiro Takeuchi (eds.), _Proceedings of The 10th Asian_\n_Conference on Machine Learning_, volume 95 of _Proceedings of Machine Learning Research_, pp.\n[534–549. PMLR, 14–16 Nov 2018b. URL https://proceedings.mlr.press/v95/](https://proceedings.mlr.press/v95/wu18a.html)\n[wu18a.html.](https://proceedings.mlr.press/v95/wu18a.html)\n\n\nxAI. Grok 4 model card, Aug 2025. URL [https://data.x.ai/](https://data.x.ai/2025-08-20-grok-4-model-card.pdf)\n[2025-08-20-grok-4-model-card.pdf.](https://data.x.ai/2025-08-20-grok-4-model-card.pdf)\n\n\nGo Yasui, Yoshimasa Tsuruoka, and Masaaki Nagata. Using semantic similarity as reward for reinforcement learning in sentence generation. In Fernando Alva-Manchego, Eunsol Choi, and Daniel\nKhashabi (eds.), _Proceedings of the 57th Annual Meeting of the Association for Computational_\n\n\n24\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n_Linguistics: Student Research Workshop_, pp. 400–406, Florence, Italy, July 2019. Association for\n[Computational Linguistics. doi: 10.18653/v1/P19-2056. URL https://aclanthology.](https://aclanthology.org/P19-2056/)\n[org/P19-2056/.](https://aclanthology.org/P19-2056/)\n\n\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In Satinder Singh and Shaul Markovitch (eds.), _Proceedings of the Thirty-_\n_First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California,_\n_USA_, pp. 2852–2858. AAAI Press, 2017. doi: 10.1609/AAAI.V31I1.10804. [URL https:](https://doi.org/10.1609/aaai.v31i1.10804)\n[//doi.org/10.1609/aaai.v31i1.10804.](https://doi.org/10.1609/aaai.v31i1.10804)\n\n\nWojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. _CoRR_,\n[abs/1505.00521, 2015. URL http://arxiv.org/abs/1505.00521.](http://arxiv.org/abs/1505.00521)\n\n\nLining Zhang, Simon Mille, Yufang Hou, Daniel Deutsch, Elizabeth Clark, Yixin Liu, Saad Mahamood, Sebastian Gehrmann, Miruna Clinciu, Khyathi Raghavi Chandu, and Jo˜ao Sedoc. A\nneedle in a haystack: An analysis of high-agreement workers on MTurk for summarization. In\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual_\n_Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 14944–\n14982, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/\n[v1/2023.acl-long.835. URL https://aclanthology.org/2023.acl-long.835/.](https://aclanthology.org/2023.acl-long.835/)\n\n\nYinan Zhang, Eric Tzeng, Yilun Du, and Dmitry Kislyuk. Large-scale reinforcement learning for\ndiffusion models. In _European Conference on Computer Vision_ [, 2024. URL https://api.](https://api.semanticscholar.org/CorpusID:267095304)\n[semanticscholar.org/CorpusID:267095304.](https://api.semanticscholar.org/CorpusID:267095304)\n\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. _Advances in neural information processing systems_, 36:46595–46623, 2023.\n\n\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\nLIMA: Less is more for alignment. In _Thirty-seventh Conference on Neural Information Process-_\n_ing Systems_ [, 2023. URL https://openreview.net/forum?id=KBMOKmX2he.](https://openreview.net/forum?id=KBMOKmX2he)\n\n\nEnyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang.\nRMB: comprehensively benchmarking reward models in LLM alignment. In _The Thirteenth In-_\n_ternational Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025_ .\n[OpenReview.net, 2025. URL https://openreview.net/forum?id=kmgrlG9TR0.](https://openreview.net/forum?id=kmgrlG9TR0)\n\n\nDawei Zhu, Sony Trenous, Xiaoyu Shen, Dietrich Klakow, Bill Byrne, and Eva Hasler. A\npreference-driven paradigm for enhanced translation with large language models. In Kevin Duh,\nHelena Gomez, and Steven Bethard (eds.), _Proceedings of the 2024 Conference of the North_\n_American Chapter of the Association for Computational Linguistics: Human Language Tech-_\n_nologies (Volume 1: Long Papers)_, pp. 3385–3403, Mexico City, Mexico, June 2024. Asso[ciation for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.186. URL https:](https://aclanthology.org/2024.naacl-long.186/)\n[//aclanthology.org/2024.naacl-long.186/.](https://aclanthology.org/2024.naacl-long.186/)\n\n\nVil´em Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan Wang, and Brian Thompson. Fine-tuned machine translation metrics struggle in unseen domains. In Lun-Wei Ku, Andre\nMartins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association_\n_for Computational Linguistics (Volume 2: Short Papers)_, pp. 488–500, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-short.45. URL\n[https://aclanthology.org/2024.acl-short.45/.](https://aclanthology.org/2024.acl-short.45/)\n\n\n25\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n||Fi<br>~~Evalu~~|ld<br>~~tion Metric~~||||||||||\n||Rewa<br>|rd Model<br>||||||||||\n\n\n\nFigure 3: We show the percentage of citations to papers that were published more than three years\nago. Reward model literature exhibits outlier behavior in which this percentage is decreasing drastically every year.\n\n\nA D ETAILED M ETHODS FOR C ITATION A NALYSIS\n\n\nThe results in the main text present a high-level overview of the key findings of our citation analysis.\nWe chose to present results for the last 5 years since reward models that resemble those discussed in\nthis paper were only popularized in this time frame. While the Semantic Scholar API yields results\nfor the years prior, they are mostly irrelevant to the discussion at hand. We extended the analysis\nfor evaluation metrics back to 2016 but find no significant difference in results. Therefore, we omit\nthem for readability.\n\n\nSimilarly, we choose to present results of an analysis of up to 300 papers per year as a result of\nqualitative assessment of the data. A qualitative assessment found that, beyond the first 300 papers,\nsearch results became too noisy, with irrelevant papers being retrieved. We repeated the analysis\nwith only the top 100 and 200 papers with no significant differences in results.\n\n\nThe specific keywords for the analysis in Figure 2a were selected based on repeated trials to maximize precision at the cost of recall. For example, including “evaluation” as a proxy keyword for\nevaluation metrics would have yielded any paper that discusses their evaluation results, not necessarily discusses how to build evaluation metrics. Similarly, we included generic RL-related terms\nlike “policy” for reward models since the terminology was evolving and papers only fairly recently\nconverged on this term. To not miss citations to relevant papers prior to 2020, we included them at\nthe risk of overestimating the true citation count.\n\n\nAs a result, the specific numerical results are a side effect of this keyword-based identification and\nshould be interpreted with caution. While an LLM-based identification process may yield more\naccurate results, it would require processing a significant number of tokens. Since we were only interested in aggregate trend information, we found the results from keyword-based searches sufficient\nand stable across many variants.\n\n\nB R ECENCY B IAS IN CITATIONS\n\n\nWe quantify recency bias in citations across the three fields of study. Citations only to recent papers\nwould provide an additional piece of evidence that insights from work before LLMs became popular\nare not being considered. Indeed, we find that while the average age of a cited paper for evaluation\nmetrics published in 2025 is 5.0 years, cited papers by reward modeling and LLM-as-a-judge papers\nare only 3.6 and 3.8 years old. 62.2% of citations in reward modeling papers are to papers published\nless than 2 years ago (68.5% for LLM-as-a-Judge), in contrast to 46.8% for evaluation metrics.\nCritically, Figure 3 shows how citations to older papers have been decreasing, especially in literature\non reward models. This result is an indicator that reward modeling research is evolving quickly and\nthat benchmarks are quickly made irrelevant by new results.\n\n\n26\n\n\nReward Models are Metrics in a Trench Coat - Preprint\n\n\nC A SSIGNMENTS OF CONFERENCE TO SUBFIELD\n\n\nFor our analysis of citations to research areas in Section 3, we assign academic venues to an area\nif the venue is clearly affiliated with it. For example, AAAI’s scope is all of AI and we therefore\ndo not include it in this analysis. We include a venue in this analysis if papers published there\nreceived at least 50 citations among all the 50,000+ papers included in our analysis. We account for\nvarious misspellings, capitalization differences, and abbreviations, but only list each venue once in\nthe following list of assignments:\n\n\n**Machine Learning** COLT, ICLR, ICML, JMLR, NeurIPS / NIPS, TMLR, TNNLS\n\n\n**Natural Language Processing** ACL (including Findings of ACL), CONLL, EACL, EAMT,\nEMNLP, INLG, LREC, NAACL, SemEval, WMT\n\n\n**Robotics** CoRL, ICRA, IROS, IJRR, RSS\n\n\n**Vision** CVPR, ECCV, ICCV, IJCV, MICCAI, TIP, TOG, WACV\n\n\n27\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03230v1",
      "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
      "authors": [
        "Suyuchen Wang",
        "Tianyu Zhang",
        "Ahmed Masry",
        "Christopher Pal",
        "Spandana Gella",
        "Bang Liu",
        "Perouz Taslakian"
      ],
      "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
      "published": "2025-10-03T17:59:34Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03230v1"
    },
    "text": "## I MPROVING GUI G ROUNDING WITH E XPLICIT - P OSITION TO -C OORDINATE M APPING\n\n**Suyuchen Wang** [1,2,3] **, Tianyu Zhang** [1,2,3] **, Ahmed Masry** [1,4] **, Christopher Pal** [1,2,5,7] **,**\n**Spandana Gella** [1] **, Bang Liu** [2,3,7] **, Perouz Taslakian** [1,6]\n\n1 ServiceNow 2 Mila - Quebec AI Institute 3 Universit´e de Montr´eal 4 York University\n5 Polytechnique Montr´eal 6 McGill University 7 CIFAR AI Chair\n\n\n**A** **BSTRACT**\n\n\nGUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for\nautonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-topixel mapping, which breaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual features, forcing the\nmodel to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and\nfailures proliferate on new resolutions. We address this with two complementary innovations. First,\n**R** **ULER** **tokens** serve as explicit coordinate markers, letting the model reference positions similar to\ngridlines on a map and _adjust_ rather than generate coordinates from scratch. Second, **Interleaved**\n**MRoPE (I-MR** **O** **PE)** improves spatial encoding by ensuring that width and height dimensions are\nrepresented equally, addressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with\nthe largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather\nthan relying on implicit learning, our approach enables more reliable GUI automation across diverse\nresolutions and platforms.\n\n\n**1** **Introduction**\n\n\nGUI grounding is the task of mapping natural language instructions to precise pixel coordinates in graphical user\ninterfaces, enabling autonomous agents to interact with software as humans do (Zhang et al., 2025a; Wang et al., 2024a;\nZheng et al., 2024). This capability is fundamental for computer automation: without accurate grounding, agents\ncannot click buttons, fill forms, or navigate interfaces reliably. Although early approaches relied on structured metadata\nfrom HTML/DOM trees or accessibility APIs (Li et al., 2020; Deng et al., 2023), these methods face significant\nlimitations: they require access to the underlying UI structure, which is often unavailable in desktop applications,\ninconsistent across platforms, or completely absent in legacy systems. Pure vision-based grounding, which operates\ndirectly on screenshots, offers universal applicability across any visual interface without requiring special access or\ninstrumentation (Qin et al., 2025; Wang et al., 2025b; Guo et al., 2025). This approach mirrors human interaction\nwith GUIs and enables automation of any software visible on screen, from modern web applications to legacy desktop\ntools.\n\n\nCurrent vision-based approaches typically formulate GUI grounding as a coordinate generation task, where models output pixel positions as text tokens (e.g., “x=523, y=217”). This paradigm, adopted by models such as\nSeeClick (Cheng et al., 2024), CogAgent (Hong et al., 2024), and UI-TARS (Qin et al., 2025), treats coordinate prediction as a standard language modeling problem. However, this approach faces a fundamental challenge illustrated in\nFigure 1: models must learn to map from high-dimensional visual positional embeddings to precise numerical coordinates as token outputs without explicit spatial guidance. The mapping is entirely _implicit_ : the model receives visual\npatches with positional embeddings and must learn to translate these abstract and similar representations into exact\nand distinct pixel value tokens through its language modeling head.\n\n\nThis implicit approach leads to two critical problems. First, **unreliable coordinate prediction** : Without explicit\nguidance linking positions to coordinates, models struggle to learn stable mappings, requiring extensive training data\nand still producing inconsistent results (Gou et al., 2025; Wu et al., 2025a). Second, **poor resolution generalization** :\nModels trained on specific resolutions generally fail when deployed on different screen sizes, as the implicit mapping\nfunction learned during training does not transfer to new coordinate ranges (Nayak et al., 2025; Li et al., 2025b).\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\n\n“Record the sound”\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: A comparison between traditional direct positional embedding-to-pixel coordinate mapping and R ULER ’s\nexplicit coordinate mapping.\n\n\nWe also identify a technical limitation in the way current VLMs encode spatial information. Standard Multidimensional Rotary Positional Embedding (MRoPE), used in state-of-the-art models like Qwen2-VL and Qwen2.5VL (Wang et al., 2024b; Bai et al., 2025), assigns different frequency bands to height and width dimensions sequentially. This creates an imbalance where one dimension receives only high-frequency components while another\nreceives only low-frequency components, leading to uneven spatial modeling capabilities across axes, a previously\noverlooked issue that impacts grounding precision.\n\n\nTo address these challenges, we introduce a framework that provides explicit spatial guidance for GUI grounding\nthrough two key innovations:\n\n\nFirstly, **R** **ULER** **(Rotary position-to-pixeL mappER) tokens** establish an explicit coordinate reference system within\nthe model. As illustrated in Figure 1, these auxiliary tokens encode pixel coordinates directly and share positional\nembeddings with the corresponding image patches. Instead of regressing the coordinates from abstract features, the\nmodels can now refer to the nearest R ULER token and perform simple bounded arithmetic to determine exact positions.\nThis transforms an unstable regression problem into a robust reference-and-adjustment mechanism, similar to how\nhumans might use gridlines on a map.\n\n\nSecondly, **Interleaved MRoPE (I-MR** **O** **PE)** addresses frequency imbalance in standard positional encodings. By\ninterleaving rather than sequentially assigning frequency components across spatial dimensions, it distributes highand low-frequency signals uniformly across width and height. This produces balanced spatial representations and\nimproves the model’s ability to distinguish positions along both axes equally.\n\n\nTraining models from scratch with our framework and finetuning existing VLMs with R ULER tokens, we perform\nextensive evaluation on ScreenSpot (Cheng et al., 2024), ScreenSpot-V2 (Wu et al., 2025b), and ScreenSpot-Pro (Li\net al., 2025b). Our approach achieves significant improvements: on the challenging ScreenSpot Pro benchmark with\nhigh-resolution displays exceeding our training resolution, we improve accuracy from 31.1% to 37.2% through finetuning alone, demonstrating strong generalization capability. These gains are achieved with minimal computational\noverhead, as R ULER tokens add less than 1% to the total token count even for 8K displays.\n\n\nOur work makes three key contributions: (1) We identify and formalize the implicit mapping problem in current GUI\ngrounding approaches, showing how it leads to poor accuracy and resolution brittleness; (2) We introduce R ULER\ntokens, an explicit coordinate reference mechanism that transforms unstable regression into robust spatial referencing;\n(3) We present I-MR O PE, a balanced positional embedding scheme that provides equal spatial modeling capacity\nacross dimensions. Together, these innovations establish a more principled approach to GUI grounding that treats\npixel-level precision as an explicit architectural concern rather than an emergent property.\n\n\n**2** **Related Work**\n\n\n**Positional Embeddings in Vision-Languge Models.** Rotary Positional Embedding (RoPE) (Su et al., 2024) encodes positions by rotating embedding dimension pairs with angles proportional to token indices, but suffers from\na long-term decay bias in low-frequency components. HoPE (Li et al., 2025a) zeros out these low-frequency terms\nto prevent long-range bias. For vision-language models, abundant visual tokens exhaust RoPE’s context window;\nV2PE (Ge et al., 2024) rescales step sizes for vision tokens, while CircleRoPE (Wang et al., 2025a) projects image\n\n\n2\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\n“How do I switch to French?”\n\n\n```\n(10,42)\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|LLM Decoder<br>RULER Tokens Interleaved MRoPE<br>Height Width High-frequency Low-frequency<br>“How do I switch to French?” Reference Reference RoPE<br>H W H W H W H W H W H W H W<br>0 28 Vector<br>(21, 21) (23, 23)<br>Height Dimension Width Dimension<br>(21, 25) (10,42)<br>(21, 25)<br>… 0 28 56 … …|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n\n\n\n**(0, 0) (1, 1)** **(20, 20)**\n\n\n\n**(21, 21)(23, 23)(25, 25)**\n\n\n\n**(21, 21)(21, 22)(21, 23)** **(23, 25)**\n\n\n\n**(26, 26)**\n\n\n\n**(27, 27)(28, 28)** **(38, 38)(39, 39)(40, 40)(41, 41)**\n\n\n\nOutputs\n\n\n\nUser Query\n\n\n\nSystem Prompt\n\n\n\n**RULER Tokens**\n\n\n\nVision Tokens\n\n\n|Col1|Col2|Col3|Adding<br>Multi-dimensional<br>Position IDs|Col5|Col6|\n|---|---|---|---|---|---|\n|(0, 0)|(0, 1)|(0, 2)|(0, 3)|(0, 4)|(0, 5)|\n|(1, 0)|(1, 1)|(1, 2)|(1, 3)|(1, 4)|(1, 5)|\n|(2, 0)|(2, 1)|(2, 2)|(2, 3)|(2, 4)|(2, 5)|\n|(3, 0)|(3, 1)|(3, 2)|(3, 3)|(3, 4)|(3, 5)|\n\n\n\nFigure 2: **Model architecture.** Our framework augments vision-language models with two key innovations: (1)\nR ULER tokens that provide explicit position-to-coordinate mappings, transforming coordinate prediction from regression to retrieval, and (2) I-MR O PE that rebalances positional embeddings by interleaving frequency components\nacross spatial dimensions, ensuring equal representational capacity for width and height, and\n\n\ntokens into circular space orthogonal to text, ensuring equal cross-modal distances. For video, M-RoPE (Wang et al.,\n2024b) separately encodes spatial-temporal dimensions but disrupts cross-modal alignment by offsetting text tokens.\nVideo RoPE (Liu et al., 2025) addresses this by rotating spatial positions while preserving text-video continuity and\nrelative spatial information. Currently, Qwen2-VL and Qwen2.5-VL’s MRoPE (Wang et al., 2024b; Bai et al., 2025)\nis one of the most prevailing multidimensional positional embedding due to the popularity of these models. However,\nthe implementation of MRoPE results in a biased partition of RoPE features for each spatial-temporal dimensions.\nOur I-MR O PE provides an elegant improvement to MRoPE that provides a full frequency spectrum of RoPE features\nfor each spatial-temporal dimension, which allows the model to perform better position perception.\n\n\nConcurrent to our work, Qwen3-VL (Qwen, 2025) independently developed Interleaved-MRoPE. We note that both\napproaches arrived at nearly identical designs through independent research paths, as confirmed through correspondence with the team members of Qwen3-VL.\n\n\n**GUI Grounding Models.** Given the limitations of general-purpose models on UI grounding tasks (Li et al., 2025b;\nNayak et al., 2025), recent work has focused on developing task-specific models. Early approaches formulated coordinate prediction (UI grounding) as a text generation problem. For example, J EDI (Xie et al., 2025) and UI-T ARS (Qin\net al., 2025) finetune open-source VLMs on synthetically generated data to enhance grounding capabilities. Building on this, GTA1 (Yang et al., 2025) and SE-GUI (Yuan et al., 2025) leverage reinforcement learning, specifically\nGRPO (Shao et al., 2024), with rule-based rewards to self-improve grounding performance. P HI -G ROUND (Zhang\net al., 2025b) introduces a label smoothing strategy that weights coordinate token predictions by their numerical\ndistance from the ground truth, while emphasizing digit positions (e.g., tens, hundreds). In contrast, some recent\napproaches have moved away from text-based coordinate generation. For example, GUI-A CTOR (Wu et al., 2025a)\nproposes coordinate-free grounding, where the model directly predicts the visual patches corresponding to the target locations. However, current methods either generate coordinates as natural language response, which requires\nmapping positional embeddings to number tokens, or requires large changes to the model architecture, which is not\ndirectly compatible with general tasks. Our introduced R ULER provides both explicit guidance for mapping position\ninformation to tokens, while keeping the model’s original autoregressive generation design to maximize compatibility\nwith other model usage scenarios.\n\n\n**3** **Method**\n\n\nWe present a framework for UI grounding that addresses fundamental limitations in how current VLMs handle spatial\nperception. Our approach introduces two complementary innovations: **(i) Interleaved Multidimensional Rotary Po-**\n\n\n3\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\n**sitional Embedding (I-MR** **O** **PE)** that provides balanced spatial representations, and **(ii) R** **ULER** tokens that establish\nexplicit position-to-pixel coordinate mappings. We provide an overview of our proposed method in Figure 2.\n\n\n**3.1** **R** **ULER** **: Explicit Position-to-Pixel Coordinate Mapping**\n\n\nCurrent VLMs predict pixel coordinates for GUI grounding by generating coordinates as text tokens (e.g., “x=523,\ny=217”). Since the source of such coordinate-related information is only recorded by image tokens’ positional\nembeddings, generating coordinate tokens requires implicit and direct mapping from high-dimensional visual features’\npositional embeddings to natural language number tokens. This approach suffers from unstable learning dynamics and\npoor generalization to unseen resolutions, as the learned regression functions are inherently resolution-specific (Gou\net al., 2025; Wu et al., 2025a).\n\n\nTo provide a more explicit guidance for the model in generating pixel coordinates, we propose R ULER, which introduces auxiliary tokens that explicitly encode pixel coordinates and share positional embeddings with corresponding\nimage patches. Inspired by the induction head mechanism in pretrained Transformers (Olsson et al., 2022), we take\nadvantage of the model’s learned capability to compare position IDs and to copy tokens according to their positions,\nand use a series of tokens with carefully designed position IDs and token values as a ruler for the image. With the help\nof these tokens, instead of regressing pixel values from positional embeddings, the model finds a R ULER token whose\npositional encoding best aligns with an image patch, and copy its value as a reference coordinate value. Based on the\nretrieved coordinate value, the model only needs to add a number bounded by a constant _b_ internally to get the final\noutput number, where _b_ is irrelevant of the image resolution, reducing the generalization gap on images with higher\nresolutions than the trained ones. An illustrated comparison between R ULER and traditional grounding methods is\nshown in Figure 1.\n\n\nSpecifically, consider an image partitioned (tokenized) into _H × W_ patches each covering _p × p_ pixels, and let **x** sys\ndenote system tokens, **x** vision the visual patch embeddings, and **x** prompt the text prompt embeddings. We augment the\ninput sequence with a set of auxiliary coordinate tokens **x** R ULER as follows:\n**x** input = � **x** sys _,_ **x** R ULER _,_ **x** vision _,_ **x** prompt � _,_ (1)\nWe construct each R ULER token _r_ _i_ _∈_ **x** R ULER so that it shares the same spatial position ID as a visual patch and\nhas the face token value of the initial pixel coordinate of the corresponding visual patch. This construction both\naligns R ULER ’s position with input visual patches and aligns its value with output coordinate tokens; thus, bridges the\nposition-to-coordinate mapping:\nPE R ULER ( _r_ _i_ ) = **R** [MRoPE] Θ _,t_ 0 + _i_ (2)\nwhere **R** [MRoPE] is a multidimensional RoPE operator, and _t_ 0 is a fixed temporal index ensuring that the height and\nwidth components match those of the vision token at spatial position _i_ . In practice, _t_ 0 is the initial spatial position\nID of the image patches. Note that R ULER only models one of the multiple dimensions of spatial position IDs, since\n_t_ 0 is the same for both height and width dimensions, and each image patch covers a square part of image. Thus,\nthe mapping between height or width to the pixel coordinate values is identical. This sharing of R ULER mapping on\nmultiple spatial dimensions helps reduce the number of R ULER tokens and improve efficiency.\n\n\nTo further manage computational cost, we introduce R ULER tokens at regular intervals _s_ instead of having them for\neach position:\n_R_ = _{r_ _i_ : _i ∈{_ 0 _, s,_ 2 _s, ..., ⌊_ max( _H, W_ ) _/s⌋· s}}_ (3)\nIn this case, the arithmetic bound is _b_ = _s × p_ . The R ULER tokens are generated during the preparation of multimodal\ninputs. When the input sequence has multiple images, we generate a R ULER token sequence before each image with\nposition ID corresponding to each image.\n\n\n**3.2** **I-MR** **O** **PE: Interleaved Multidimensional Rotary Positional Embedding**\n\n\nPositional embeddings encode spatial information in vision transformers. Multidimensional RoPE (MRoPE) (Wang\net al., 2024b; Bai et al., 2025) extends standard RoPE to VLMs by decomposing positions into multiple spatialtemporal dimensions. However, a critical limitation of MRoPE is that it creates a frequency imbalance between spatial\ndimensions.\n\n\nRotary positional embeddings (RoPE) encode relative positions by applying rotation matrices directly to the query\nand key vectors in each attention head. Let _m ∈_ N denote the position index of a token and _d_ the dimension of the\nattention head. For each 2 _×_ 2 block, RoPE rotates a pair of dimensions by a position-dependent angle _mθ_ _j_ . The\nrotation matrix **R** _θ_ _j_ _,m_ applied to the query and key vectors is thus expressed as:\n\n**R** _θ_ _j_ _,m_ = �cos(sin( _mθmθ_ _jj_ )) _−_ cos(sin( _mθmθ_ _j_ _j_ ))� _,_ _θ_ _j_ = _b_ _[−]_ [2] _[j/d]_ _,_ (4)\n\n\n4\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nwhere _b_ is a hyperparameter called RoPE base. The frequency _θ_ _j_ decreases exponentially with the dimension index\n_j_, producing a spectrum that ranges from high-frequency to low-frequency components as _j_ progresses from 0 to _d_,\nwhich is illustrated in the right part of Figure 2. In standard MRoPE, these frequencies are partitioned and assigned\nconsecutively to different spatial-temporal dimensions:\n\n\n**R** [MRoPE] Θ _,t,h,w_ [=][ diag][(] **[R]** [Θ] _t_ _[,t]_ _[,]_ **[ R]** [Θ] _h_ _[,h]_ _[,]_ **[ R]** [Θ] _w_ _[,w]_ [)] (5)\n\n\nwhere Θ _t_, Θ _h_, and Θ _w_ denote disjoint yet consecutive subsets of the frequency spectrum _θ_ _j_ . This sequential allocation leads to an imbalance: the high-, mid-, and low-frequency parts of the RoPE vector are fully and only occupied\nby the temporal, height, and width dimensions, respectively. As a result, each dimension is biased towards a limited\nand different frequency band, constraining the representational capacity and degrading grounding performance across\naxes (Liu et al., 2024c; Wang et al., 2024c). This imbalance also potentially results in different inner processing mechanisms of each spatial-temporal dimension due to the different modeling behaviors of their corresponding positional\nembedding.\n\n\nI-MR O PE addresses this imbalance by distributing the frequency spectrum uniformly across spatial dimensions\nthrough frequency interleaving. Specifically, instead of assigning consecutive frequency bands to a single axis, each\nfrequency index _j_ is cyclically mapped.\n\n\n\nDimension assignment for frequency _j_ : _p_ _j_ =\n\n\n\n _w_ if _j_ mod 3 = 0\n\n_h_ if _j_ mod 3 = 1 (6)\n\n\n\n _t_ if _j_ mod 3 = 2\n\n\n\nwhere _p_ _j_ denotes the spatial dimension (width, height, or temporal) assigned to frequency _θ_ _j_ .\n\n\nThis interleaving ensures that every dimension receives a full range of frequencies, combining high-frequency components for fine-grained localization with low-frequency components for long-range dependencies. Like vanilla MRoPE,\ntext tokens in the sequence have identical temporal, height, and width indices ( _t_ = _h_ = _w_ = _m_ ), and the formulation\nreduces exactly to standard RoPE:\n**R** Θ [I-MR] _,m,m,m_ [O] [PE] [=] **[ R]** [RoPE] Θ _,m_ (7)\n\n\nThis preserves backward compatibility with pre-trained language models while providing more balanced spatial representations for vision tasks.\n\n\n**4** **Experimental Setup**\n\n\n**Training Setup.** We conduct two sets of experiments to validate our approach: training from scratch and finetuning\nexisting VLMs. For the from-scratch experiments, we build on the LLaVA-NeXT framework (Liu et al., 2024b)\nusing SigLIP-SO400M-14@384 (Zhai et al., 2023) as vision encoder and Qwen2.5 7B Instruct (Qwen et al., 2024) as\nlanguage decoder. We replace the standard 1D positional embeddings in the language decoder in LLaVA-NeXT with\nMRoPE or I-MR O PE, and integrate R ULER tokens into the input sequence during both training and inference.\n\n\nFollowing the LLaVA-NeXT training paradigm, we employ a two-stage training process. First, we perform visionlanguage alignment pretraining on the LLaVA-558K dataset (Liu et al., 2024a), training only the MLP projection layer.\nSecond, we conduct domain-specific supervised finetuning on UI grounding tasks, training both the projection layer\nthrough full finetuning and the language model through LoRA (Hu et al., 2022) for parameter efficiency.\n\n\nFor finetuning experiments, we adapt Qwen2.5-VL 7B Instruct (Bai et al., 2025) by introducing R ULER tokens and\nfocus on verifying the significance of R ULER alone on grounding performance. We do not change the original model’s\nMRoPE to avoid dramatic changes to the learned model behaviors regarding positional embedding. We use Qwen2.5VL’s default system prompt and chat template for all the finetuning experiments.\n\n\nIn all experiments, we set the R ULER token’s default interval as _s_ = 8 in the main experiments. For I-MR O PE, since\nGUI grounding does not require a temporal dimension, we use 2D MRoPE and I-MR O PE in the from-scratch training\nexperiments. Specifically, the dimension assignment for frequency _j_ is:\n\n\n_h_ if _j_ mod 2 = 0\nDimension assignment for frequency _j_ : _p_ _j_ = � _t_ if _j_ mod 2 = 1 (8)\n\n\nThe training process follows standard VLM objectives with UI grounding tasks. The model learns to leverage R ULER\ntokens for coordinate prediction while I-MR O PE provides balanced spatial representations throughout the transformer\nlayers. This combination enables precise pixel-level grounding without compromising general vision-language capabilities. More hyperparameter settings can be found in Appendix A.\n\n\n5\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\n**Training Data.** Both experimental settings are trained on the UGround dataset (Gou et al., 2025), which provides\ncomprehensive UI grounding annotations on websites. It contains approximately 8M element annotations across 775K\nscreenshots, providing diverse training signals for robust grounding capabilities.\n\n\nTo comply with Qwen2.5-VL’s post-training settings regarding coordinates (Bai et al., 2025), we pre-process all coordinates in UGround to use raw pixel values rather than normalized ones. This choice ensures consistency with our\nR ULER token design, which requires each patch’s size in terms of the output coordinate to be a square, and avoids the\nambiguity introduced by normalization in different aspect ratios.\n\n\n**Evaluation Setup.** We evaluate our models on three UI grounding benchmarks: ScreenSpot (Cheng et al., 2024),\nScreenSpot-V2 (Wu et al., 2025b), and ScreenSpot Pro (Li et al., 2025b). Each benchmark presents screenshots paired\nwith natural language instructions that describe the target UI elements. Models must predict the pixel coordinates\ncorresponding to the described element.\n\n\nScreenSpot and ScreenSpot-V2 contain 1,272 instructions each on mobile, desktop, and web platforms, with V2\ncorrecting the annotation errors from the original. ScreenSpot-Pro presents a more challenging scenario with 1,581\ntasks from 23 professional desktop-only applications featuring higher resolution interfaces and greater domain shift\nfrom typical training data. In particular, ScreenSpot-Pro features higher-resolution images than our training data,\nmaking it a strong test of resolution generalization.\n\n\nWe preprocess all benchmarks to use raw pixel coordinates for evaluation, ensuring fair comparison between methods. [1]\nWe measure performance using _Element Accuracy_, which considers a prediction correct if the predicted point falls\nwithin the ground-truth bounding box of the target element. We use the evaluation setting and the code provided by\nWu et al. (2025a).\n\n\n**Baselines.** We compare against state-of-the-art UI grounding models of comparable scale. Our baseline models\nincludes Qwen-2-VL 7B Instruct (Wang et al., 2024b), one of the most commonly used open-source VLMs; SeeClick9.6B (Cheng et al., 2024), an early specialized UI grounding model; OS-Atlas-7B (Wu et al., 2025b), a model designed for operating system interactions; Aguvis-7B (Xu et al., 2025), which uses visual grounding with bounding\nbox supervision; UGround-V1-7B (Gou et al., 2025) trained on the same UGround dataset; UI-TARS-7B (Qin et al.,\n2025), a recent strong baseline; and GUI-Actor-7B (Wu et al., 2025a) which uses attention-based grounding instead of\noutputting coordinates. All baseline numbers are reported from original papers or reproduced using official implementations with consistent evaluation protocols. Note that our models use less training data than GUI-Actor. Besides, our\nmodels are only trained on UGround and thus have not seen data from domains other than websites, unlike UI-TARS\nand GUI-Actor.\n\n\n**5** **Results**\n\n\n**5.1** **GUI Grounding Performance**\n\n\nWe present the comparison among the models trained from scratch with R ULER and I-MR O PE, the finetuned models equipped with R ULER, and the baseline models on ScreenSpot-Pro, ScreenSpot, and ScreenSpot-V2 in Table 1,\nTable 2, and Table 3, respectively.\n\n\nFor the from-scratch training experiments, multidimensional RoPE consistently outperforms the default 1D RoPE\n(LLaVA PE) across all benchmarks. Furthermore, our proposed I-MR O PE achieves both lower training loss and\nstronger grounding performance than the original MRoPE, demonstrating the effectiveness of balancing the spectrum\nacross the spatial dimensions. R ULER tokens further enhance performance by providing guidance on position-tocoordinate mapping, achieving the best overall results among all models trained from scratch across all datasets. Noticeably, the gains from R ULER are most pronounced on ScreenSpot-Pro, reflecting how its reference-then-copy mechanism and bounded pixel coordinate arithmetic across resolutions help generalization to higher resolution grounding\ntasks.\n\n\nFor fine-tuning experiments, we also observe that adding R ULER consistently improves performance, with the largest\ngains on the higher-resolution ScreenSpot-Pro benchmark. Although R ULER does not achieve state-of-the-art results\npartly due to the limited training data and domains, our experiments nevertheless demonstrate that incorporating\nR ULER reliably enhances grounding performance under comparable training conditions.\n\n\n1 For baselines trained with normalized coordinates, we apply appropriate transformations to the output to enable comparison.\n\n\n6\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nTable 1: Grounding element accuracy on **ScreenSpot-Pro** . The results of models marked with † are adopted from Wu\net al. (2025a). Best results per column within each comparable model group are shown in **bold** . Note that results in the\nfirst two groups are not directly comparable to ours, either because the models are closed-source (weights/architectures\nunavailable) or because their training data and underlying base models are unclear or incomparable. We nevertheless\ninclude these numbers for reference.\n\n\n**Model** **Dev** **Creative** **CAD** **Scientific** **Office** **OS** **Avg**\n\n\nGPT-4o [†] 0.7 0.6 1.5 1.2 0.9 0.0 0.8\nClaude Compute [†] 12.6 16.8 11.9 25.8 26.9 8.1 17.1\n\n\nQwen2-VL-7B [†] 1.3 0.9 0.4 3.5 3.0 0.5 1.6\nSeeClick-9.6B [†] 0.3 0.6 1.9 2.0 0.9 1.5 1.1\nOS-Atlas-7B [†] 17.7 17.9 10.3 24.4 27.4 16.8 18.9\nAguvis-7B [†] 16.1 21.4 13.8 34.6 34.3 19.4 22.9\nUGround-V1-7B 28.1 31.7 14.6 39.0 49.6 24.5 31.1\n\nUI-TARS-7B 36.1 32.8 18.0 50.0 53.5 24.5 35.7\nGUI-Actor-7B + Verifier [†] 38.8 40.5 37.2 44.5 64.8 43.9 44.2\n\n\n**5.2** **Analysis on R** **ULER** **Token Interval**\n\n\nTo analyze the effect of changing the interval of the R ULER token, we provide a sensitivity analysis of _s_ in Equation 3.\nThe results are shown in Figure 3.\n\n\nIn the figure, we notice that all interval settings yield consistent improvements compared to models without R ULER\ntokens in all datasets. However, varying the R ULER token interval does not yield significant or consistent improvements on the benchmarks. Based on the results, we adopt the setting of _s_ = 8 as a good trade-off between performance\nand efficiency. However, it should be noted that in extremely low-resolution settings such as mobile phone screenshot grounding, an interval _s_ = 16 may inject only a single R ULER token, leading to reduced performance in the\nmobile-related subtasks of ScreenSpot and ScreenSpot-V2.\n\n\n**5.3** **Efficiency Analysis**\n\n\nTo demonstrate the efficiency of adding R ULER tokens, we provide an efficiency analysis in the _p_ = 8 setting in\nFigure 4. In this figure, we report the ratio of R ULER tokens to image tokens in common resolutions of mobile phones\nand computer screens under different interval settings. Even in the extreme 8K screenshot scenarios and using an\ninterval of _s_ = 2, R ULER only adds 68 additional tokens, which is merely 0.2% of the total number of vision tokens.\nFor low-resolution mobile screenshots, the highest ratio of R ULER to vision tokens observed is 2.8%, where the impact\non efficiency remains negligible. These results confirm that the introduction of tokens R ULER can effectively improve\ngrounding performance while maintaining efficiency.\n\n\n**6** **Conclusions and Limitations**\n\n\nWe presented a framework for GUI grounding that replaces implicit position-to-pixel coordinate mapping with explicit\nspatial guidance. R ULER tokens provide coordinate references that transform unstable regression into robust reference\nand adjustment, while I-MR O PE corrects frequency imbalances in the positional embeddings. Our approach achieves\nconsistent improvements across benchmarks, with particularly strong gains on high-resolution displays beyond train\n\n7\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nTable 2: Grounding element accuracy on **ScreenSpot** . The results of models marked with † are adopted from Wu\net al. (2025a). Best results per column within each group are shown in **bold** .\n\n\nM-Text M-Icon D-Text D-Icon W-Text W-Icon Avg\n\n\nGPT-4 [†] 22.6 24.5 20.2 11.8 9.2 8.8 16.2\nGPT-4o [†] 20.2 24.9 21.1 23.6 12.2 7.8 18.3\nClaude Computer Use [†] - - - - - - 83.0\nGemini 2.0 [†] - - - - - - 84.0\n\n\nQwen2-VL-7B [†] 75.5 60.7 76.3 54.3 35.2 25.7 55.3\nSeeClick-9.6B [†] 78.0 52.0 72.2 30.0 55.7 32.5 53.4\nOS-Atlas-7B [†] 93.0 72.9 91.8 62.9 90.9 74.3 82.5\nAguvis-7B 95.6 [†] 77.7 93.8 67.1 88.3 75.2 84.4\nUGround-v1-7B 93.0 79.9 93.8 76.4 90.9 84.0 86.3\n\nUI-TARS-7B 94.5 85.2 95.9 85.7 90.0 83.5 89.5\nGUI-Actor-7B + Verifier [†] 96.0 83.0 93.8 82.1 92.2 87.4 89.7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|37.237|.1|\n|---|---|---|---|---|---|---|---|---|---|---|\n|5<br>7|||||||~~34.6~~<br>~~36~~|~~34.6~~<br>~~36~~|~~.5~~||\n|3<br>|~~32.3~~<br>3|~~32.3~~<br>3|~~32.3~~<br>3|~~32.7~~<br>.1<br>|~~32.7~~<br>.1<br>|~~32.7~~<br>.1<br>|32.9<br>~~32.7~~|32.9<br>~~32.7~~|||\n|3<br>|29.2|29.2||29.<br><br>31.9|29.<br><br>31.9|4|||||\n|3<br>|||||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||88.2<br>89.2<br>8|88.2<br>89.2<br>8|88.2<br>89.2<br>8|88.2<br>89.2<br>8|9.0<br>~~88.6~~|9.0<br>~~88.6~~|9.0<br>~~88.6~~|\n|||||||88.2<br>89.2<br>8|88.2<br>89.2<br>8||||||\n|85.5<br>85|85.5<br>85|85.5<br>85|~~84.5~~<br>86.1<br>.6<br>85.4|~~84.5~~<br>86.1<br>.6<br>85.4|~~84.5~~<br>86.1<br>.6<br>85.4|86.0<br>85.7|86.0<br>85.7||||||\n|84.0|84.0||||||||||||\n||||||||||||||\n|||||||||||ne|ne|ne|\n\n\n\n|1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|9|||||||87.8<br>87|87.8<br>87|87.8<br>87|.7<br>87.1|.7<br>87.1|\n|5<br>7||||~~8~~|~~8~~|~~8~~|86.8<br>~~.5~~<br>|86.8<br>~~.5~~<br>||||\n|3<br>|~~82.5~~<br>83.7<br>8|~~82.5~~<br>83.7<br>8|~~82.5~~<br>83.7<br>8|83.0<br><br>3.3<br>83.1|83.0<br><br>3.3<br>83.1|83.0<br><br>3.3<br>83.1|83.6<br>8|.1||||\n|3<br>||||||||||||\n|3<br>||||||I~~-~~<br>aini|I~~-~~<br>aini|I~~-~~<br>aini|I~~-~~<br>aini|I~~-~~<br>aini|I~~-~~<br>aini|\n|3<br>||||||||||||\n\n\nFigure 3: Ablation study on R ULER token intervals _s_ across different benchmarks and training settings.\n\n\ning resolutions, validating its generalization capability. The minimal computational overhead (less than 1% of token\nincrease) makes deployment practical. Future work could explore adaptive token placement and extension to video\ninterfaces. The success of explicit spatial guidance over implicit learning suggests broader applications beyond GUI\nautomation for any task that requires precise visual localization.\n\n\n**References**\n\n\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun\nTang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren\nFu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and\nJunyang Lin. Qwen2.5-vl technical report. _arXiv preprint arXiv: 2502.13923_, 2025.\n\n\n8\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nTable 3: Grounding element accuracy on **ScreenSpot-V2** . The results of models marked with † are adopted from Wu\net al. (2025a). Best results per column within each group are shown in **bold** .\n\n\nM-Text M-Icon D-Text D-Icon W-Text W-Icon Avg\n\n\nGPT-4o + OmniParser-v2 [†] 95.5 74.6 92.3 60.9 88.0 59.6 80.7\n\n\nSeeClick-9.6B [†] 78.4 50.7 70.1 29.3 55.2 32.5 55.1\nOS-Atlas-7B [†] 95.2 75.8 90.7 63.6 90.6 77.3 84.1\nAguvis-7B [†] 95.5 77.3 95.4 77.9 91.0 72.4 86.0\nUGround-V1-7B 95.0 83.3 95.0 77.8 92.1 77.2 87.6\n\nUI-TARS-7B 96.9 89.1 95.4 85.0 93.6 85.2 91.6\nGUI-Actor-7B + Verifier [†] 97.2 84.8 94.3 85.0 94.0 85.2 90.9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|(%<br>Tokens<br>2.5<br>2.0 Image<br>1.5<br>#<br>/<br>Tokens<br>1.0<br>0.5 RULER<br>0.0|2.|8%|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|l 4|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>2.5<br> RULER Tokens / # Image Tokens (%|2.|%|||||||||||||RULE<br>|Interv<br>|l 4<br>|\n|0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>2.5<br> RULER Tokens / # Image Tokens (%|~~1.~~|~~%~~|||||||||||||~~RULE~~<br>RULE|~~ Interv~~<br>R Interva|~~l 8~~<br>l 16|\n|0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>2.5<br> RULER Tokens / # Image Tokens (%||1.<br>|%|||||||||||||||\n|0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>2.5<br> RULER Tokens / # Image Tokens (%|0.|9%|~~1.~~<br>|~~3%~~<br>1.|0%|0.|9%<br>|||||||||||\n|0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>2.5<br> RULER Tokens / # Image Tokens (%||0.<br><br>0.|7%<br>0.<br>5%<br>~~0.~~|6%<br>0.<br>~~3%~~<br>~~0.~~|0.<br>5%<br>~~0.~~<br>~~3%~~<br>0.|7%<br><br>~~4%~~<br>0.<br>%<br>~~0.~~|~~0.~~<br>5%<br>0.<br>~~3%~~<br>0.|~~8%~~<br>0.<br>4%<br>~~0.~~<br>2%<br>0.|7%<br>0<br>~~3%~~<br>~~0~~<br>%<br>0|.5%<br>0.<br>~~.3%~~<br>0.<br>1%<br>0.|4%<br>~~0.~~<br>2%<br>0.<br>%<br>0.|~~4%~~<br>0.<br>2%<br>~~0.~~<br>%<br>0.|5%<br>~~0.~~<br>~~3%~~<br>0.<br>%<br>|5%<br>~~0.~~<br>~~3%~~<br>0.<br>%<br>|~~3%~~<br>~~0.~~<br>2%<br>0.<br><br>|~~2%~~<br>0.2<br>%<br><br><br>|%<br><br>|\n|0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>2.5<br> RULER Tokens / # Image Tokens (%|||||||||||||0.|0.|1%<br>0.|0.<br>%<br>0.0|%<br>%|\n\n\nFigure 4: Analysis of the ratio of the number of R ULER tokens to the number of image tokens under common mobile\nphone and computer screen resolutions for different R ULER intervals. All numbers are in percentages (%).\n\n\nKanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li YanTao, Jianbing Zhang, and Zhiyong Wu. SeeClick:\nHarnessing GUI grounding for advanced visual GUI agents. In _Proceedings of the 62nd Annual Meeting of_\n_the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 9313–9332, Bangkok, Thailand,\n[August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.505. URL https:](https://aclanthology.org/2024.acl-long.505/)\n[//aclanthology.org/2024.acl-long.505/.](https://aclanthology.org/2024.acl-long.505/)\n\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web:\nTowards a generalist agent for the web. In _Thirty-seventh Conference on Neural Information Processing Systems_\n_Datasets and Benchmarks Track_ [, 2023. URL https://openreview.net/forum?id=kiYqbO3wqw.](https://openreview.net/forum?id=kiYqbO3wqw)\n\nJunqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. _arXiv preprint_\n_arXiv:2412.09616_, 2024.\n\nBoyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating\nthe digital world as humans do: Universal visual grounding for GUI agents. In _The Thirteenth International Con-_\n_ference on Learning Representations_ [, 2025. URL https://openreview.net/forum?id=kxnoqaisCT.](https://openreview.net/forum?id=kxnoqaisCT)\n\n\n9\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nDong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang,\nJiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian,\nShen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian\nWang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang,\nXuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu,\nYu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can\nHuang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei\nLou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu,\nGuodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia\nLi, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng,\nJingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai\nLiu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei\nShi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du,\nMingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan\nTao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng\nYan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao\nYang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang,\nWenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen\nZhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen,\nYifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong\nYang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun,\nZeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu,\nZhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, and Zuquan Song. Seed1.5-vl technical report, 2025. URL\n[https://arxiv.org/abs/2505.07062.](https://arxiv.org/abs/2505.07062)\n\n\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents. In _Proceedings of the_\n_IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 14281–14290, June 2024.\n\n\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_,\n[2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.](https://openreview.net/forum?id=nZeVKeeFYf9)\n\n\nHaoran Li, Yingjie Qin, Baoyuan Ou, Lai Xu, and Ruiwen Xu. Hope: Hybrid of position embedding for length\ngeneralization in vision-language models. _arXiv preprint arXiv:2505.20444_, 2025a.\n\n\nKaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua.\nScreenspot-pro: Gui grounding for professional high-resolution computer use. _arXiv preprint arXiv:2504.07981_,\n2025b.\n\n\nYang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile\n[ui action sequences, 2020. URL https://arxiv.org/abs/2005.03776.](https://arxiv.org/abs/2005.03776)\n\n\nHaogeng Liu, Quanzeng You, Xiaotian Han, Yongfei Liu, Huaibo Huang, Ran He, and Hongxia Yang.\nVisual anchors are strong information aggregators for multimodal large language model. In _Ad-_\n_vances in Neural Information Processing Systems_, volume 37, pp. 17696–17718. Curran Associates,\nInc., 2024a. URL [https://proceedings.neurips.cc/paper_files/paper/2024/file/](https://proceedings.neurips.cc/paper_files/paper/2024/file/1f84412e84da6440ca355d87184cb1b3-Paper-Conference.pdf)\n[1f84412e84da6440ca355d87184cb1b3-Paper-Conference.pdf.](https://proceedings.neurips.cc/paper_files/paper/2024/file/1f84412e84da6440ca355d87184cb1b3-Paper-Conference.pdf)\n\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _Thirty-seventh Con-_\n_ference on Neural Information Processing Systems_ [, 2023. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=w0H2xGHlkw)\n[w0H2xGHlkw.](https://openreview.net/forum?id=w0H2xGHlkw)\n\n\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im[proved reasoning, ocr, and world knowledge, January 2024b. URL https://llava-vl.github.io/blog/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n[2024-01-30-llava-next/.](https://llava-vl.github.io/blog/2024-01-30-llava-next/)\n\n\nXiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of RoPE-based extrapolation. In _The_\n_Twelfth International Conference on Learning Representations_ [, 2024c. URL https://openreview.net/](https://openreview.net/forum?id=JO7k0SJ5V6)\n[forum?id=JO7k0SJ5V6.](https://openreview.net/forum?id=JO7k0SJ5V6)\n\n\nZikang Liu, Longteng Guo, Yepeng Tang, Tongtian Yue, Junxian Cai, Kai Ma, Qingbin Liu, Xi Chen, and Jing Liu.\nVrope: Rotary position embedding for video large language models. _arXiv preprint arXiv:2502.11664_, 2025.\n\n\nShravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan A. Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, M. Tamer Ozsu, Aishwarya Agrawal, David Vazquez, Christopher Pal, Perouz Taslakian, Spandana Gella, [¨]\n\n\n10\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nand Sai Rajeswar. Ui-vision: A desktop-centric gui benchmark for visual perception and interaction, 2025. URL\n[https://arxiv.org/abs/2503.15661.](https://arxiv.org/abs/2503.15661)\n\n\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,\nScott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack\nClark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _arXiv preprint_\n_arXiv: 2209.11895_, 2022.\n\n\nYujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin\nLi, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. _arXiv preprint_\n_arXiv:2501.12326_, 2025.\n\n\nQwen. Qwen3-VL: Sharper vision, deeper thought, broader action. [https://qwen.](https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list)\n[ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.](https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list)\n[latest-advancements-list, 2025. [Accessed 01-10-2025].](https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list)\n\n\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu,\nFei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu,\nRui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su,\nYichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. _arXiv_\n_preprint arXiv: 2412.15115_, 2024.\n\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training\ntrillion parameter models. _arXiv preprint arXiv: 1910.02054_, 2019.\n\n\nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong\nLi, and Yuxiong He. Zero-offload: Democratizing billion-scale model training. _arXiv preprint arXiv: 2101.06840_,\n2021.\n\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K.\nLi, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models,\n[2024. URL https://arxiv.org/abs/2402.03300.](https://arxiv.org/abs/2402.03300)\n\n\nJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. _Neurocomput._, 568(C), February 2024. ISSN 0925-2312. doi: 10.1016/j.neucom.\n[2023.127063. URL https://doi.org/10.1016/j.neucom.2023.127063.](https://doi.org/10.1016/j.neucom.2023.127063)\n\n\nChengcheng Wang, Jianyuan Guo, Hongguang Li, Yuchuan Tian, Ying Nie, Chang Xu, and Kai Han. Circlerope: Cone-like decoupled rotary positional embedding for large vision-language models. _arXiv preprint_\n_arXiv:2505.16416_, 2025a.\n\n\nJunyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent:\nAutonomous multi-modal mobile device agent with visual perception. _arXiv preprint arXiv:2401.16158_, 2024a.\n\n\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin\nGe, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and\nJunyang Lin. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. _arXiv_\n_preprint arXiv: 2409.12191_, 2024b.\n\n\nSuyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, and Bang Liu. Resonance RoPE: Improving\ncontext length generalization of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar\n(eds.), _Findings of the Association for Computational Linguistics: ACL 2024_, pp. 586–598, Bangkok, Thailand, August 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.32. URL\n[https://aclanthology.org/2024.findings-acl.32/.](https://aclanthology.org/2024.findings-acl.32/)\n\n\nXinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng\nXu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li,\nFangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao\nYe, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu\nWang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open\n[foundations for computer-use agents, 2025b. URL https://arxiv.org/abs/2508.09123.](https://arxiv.org/abs/2508.09123)\n\n\nQianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng,\nBo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. _arXiv preprint_\n_arXiv:2506.03143_, 2025a.\n\n\n11\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\nZhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng\nChen, Paul Pu Liang, and Yu Qiao. OS-ATLAS: Foundation action model for generalist GUI agents. In _The_\n_Thirteenth International Conference on Learning Representations_ [, 2025b. URL https://openreview.net/](https://openreview.net/forum?id=n9PDaFNi8t)\n[forum?id=n9PDaFNi8t.](https://openreview.net/forum?id=n9PDaFNi8t)\n\n\nTianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui\nXu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. _arXiv_\n_preprint arXiv:2505.13227_, 2025.\n\n\nYiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming\nXiong. Aguvis: Unified pure vision agents for autonomous GUI interaction. In _Forty-second International Confer-_\n_ence on Machine Learning_ [, 2025. URL https://openreview.net/forum?id=PlihOwfx4r.](https://openreview.net/forum?id=PlihOwfx4r)\n\n\nYan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha,\nZeyuan Chen, et al. Gta1: Gui test-time scaling agent. _arXiv preprint arXiv:2507.05791_, 2025.\n\n\nXinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen,\nPeng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. _arXiv_\n_preprint arXiv:2505.12370_, 2025.\n\n\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training.\n_IEEE International Conference on Computer Vision_, 2023. doi: 10.1109/ICCV51070.2023.01100.\n\n\nChaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan\nRajmohan, et al. Ufo: A ui-focused agent for windows os interaction. In _Proceedings of the 2025 Conference of the_\n_Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies_\n_(Volume 1: Long Papers)_, pp. 597–622, 2025a.\n\n\nMiaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim\nFranklin, et al. Phi-ground tech report: Advancing perception in gui grounding. _arXiv preprint arXiv:2507.23779_,\n2025b.\n\n\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded.\nIn _ICML_ [, 2024. URL https://openreview.net/forum?id=piecKJ2DlB.](https://openreview.net/forum?id=piecKJ2DlB)\n\n\n**A** **Implementation Details**\n\n\nWe provide detailed training configurations for our experiments in the following. All experiments are performed on 8\nNVIDIA H100 GPUs.\n\n\n**A.1** **Training from Scratch**\n\n\n**Stage 1: Vision-Language Alignment Pretraining.** We follow the LLaVA-NeXT training paradigm. The model\nuses SigLIP-SO400M-14@384 (Zhai et al., 2023) as the vision encoder and Qwen2.5 7B Instruct (Qwen et al., 2024)\nas the language model. During pretraining, we train only the MLP projection layer while keeping both vision and\nlanguage models frozen. Training is performed on the LLaVA-558K dataset (Liu et al., 2023) for 1 epoch with a\nlearning rate of 1 _×_ 10 _[−]_ [3] using cosine scheduling and 3% warmup ratio. We use a per-device batch size of 4 with\ngradient accumulation steps of 2, resulting in an effective batch size of 64 across 8 GPUs. The maximum sequence\nlength is set to 8,192 tokens. Images are processed using the AnyRes configuration with a maximum of 9 patches and\ngrid pinpoints ranging from (1 _×_ 1) to (12 _×_ 6) to accommodate high-resolution images during inference. We employ\nDeepSpeed Zero-2 with CPU offload (Ren et al., 2021) and mixed precision training (bf16) for memory efficiency. For\nmodels using RULER, we set the token interval to _s_ = 8, while positional embedding configurations (default LLaVA\nPE, MRoPE, or I-MRoPE) are specified throughout the pretraining and finetuning process.\n\n\n**Stage 2: Domain-Specific Finetuning.** Using the pretrained projection layer from Stage 1, we finetune on the\nUGround dataset (Gou et al., 2025) with coordinates converted to raw pixel values to match our RULER token design.\nIn this stage, we train the projection layer with full parameter finetuning and the language model using LoRA (Hu\net al., 2022) with rank 16 for parameter efficiency. The base learning rate is set to 1 _×_ 10 _[−]_ [5] for the projection\nlayer and LoRA parameters. We use a per-device batch size of 1 with gradient accumulation steps of 4, yielding\nan effective batch size of 32. The maximum sequence length is extended to 16,384 tokens to accommodate higherresolution images. Training runs for 1 epoch with cosine learning rate scheduling and 3% warmup. We continue using\nDeepSpeed Zero-2 with CPU offload and bf16 mixed precision.\n\n\n12\n\n\nImproving GUI Grounding with Explicit Position-to-Coordinate Mapping\n\n\n**A.2** **Finetuning Qwen2.5-VL**\n\n\nFor adapting the pretrained Qwen2.5-VL 7B Instruct model (Bai et al., 2025), we use a conservative finetuning approach to preserve the existing capabilities of the model while adding RULER tokens. We maintain the model’s\noriginal MRoPE configuration to avoid disrupting learned position-aware behaviors. The model is finetuned with a\nlow learning rate of 1 _×_ 10 _[−]_ [5] using cosine scheduling with 3% warmup to ensure stable adaptation. We use a perdevice batch size of 4 with gradient accumulation steps of 4, resulting in an effective batch size of 128. The maximum\nsequence length remains at 16,384 tokens, and we utilize Qwen2.5-VL’s dynamic resolution capability with pixel\ncounts ranging from 784 to 50,176. Training runs for 1 epoch on the UGround dataset with all components (vision\nencoder, MLP projector, and language model) being trainable. We employ DeepSpeed Zero-3 (Rajbhandari et al.,\n2019) for distributed training and bf16 mixed precision. RULER tokens are integrated into the input sequence with\ninterval _s_ = 8 when specified, and we use Qwen2.5-VL’s native chat template and system prompts for consistency\nwith the pretrained model’s behavior.\n\n\n**A.3** **Evaluation Protocol**\n\n\nAll models are evaluated using greedy decoding (temperature=0) with the same maximum sequence length as training.\nFor ScreenSpot benchmarks, we preprocess all coordinates to raw pixel values and use the evaluation code from Wu\net al. (2025a). Element accuracy is computed by checking if the predicted coordinate falls within the ground-truth\nbounding box. We ensure consistent pre-processing across all baselines for fair comparison.\n\n\n13\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03228v1",
      "title": "MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture\n  Recognition",
      "authors": [
        "Ricardo T. Fares",
        "Lucas C. Ribas"
      ],
      "summary": "Randomized neural networks for representation learning have consistently\nachieved prominent results in texture recognition tasks, effectively combining\nthe advantages of both traditional techniques and learning-based approaches.\nHowever, existing approaches have so far focused mainly on improving\ncross-information prediction, without introducing significant advancements to\nthe overall randomized network architecture. In this paper, we propose Mixer, a\nnovel randomized neural network for texture representation learning. At its\ncore, the method leverages hyperspherical random embeddings coupled with a\ndual-branch learning module to capture both intra- and inter-channel\nrelationships, further enhanced by a newly formulated optimization problem for\nbuilding rich texture representations. Experimental results have shown the\ninteresting results of the proposed approach across several pure texture\nbenchmarks, each with distinct characteristics and challenges. The source code\nwill be available upon publication.",
      "published": "2025-10-03T17:58:04Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03228v1"
    },
    "text": "## **MIXER: Mixed Hyperspherical Random Embedding Neural Network for** **Texture Recognition**\n\n**Ricardo T. Fares** [1] **Lucas C. Ribas** [1]\n\n\n\n**Abstract**\n\n\nRandomized neural networks for representation\nlearning have consistently achieved prominent\nresults in texture recognition tasks, effectively\ncombining the advantages of both traditional techniques and learning-based approaches. However,\nexisting approaches have so far focused mainly on\nimproving cross-information prediction, without\nintroducing significant advancements to the overall randomized network architecture. In this paper,\nwe propose M IXER, a novel randomized neural\nnetwork for texture representation learning. At its\ncore, the method leverages hyperspherical random\nembeddings coupled with a dual-branch learning\nmodule to capture both intra- and inter-channel\nrelationships, further enhanced by a newly formulated optimization problem for building rich\ntexture representations. Experimental results have\nshown the interesting results of the proposed approach across several pure texture benchmarks,\neach with distinct characteristics and challenges.\nThe source code will be available upon publication.\n\n\n**1. Introduction**\n\n\nTexture is a widely studied visual feature in both research\nfields of computer vision (Haralick et al., 1973) and neuroscience (Julesz, 1981). Its study has not only enabled\na deeper understanding of human texture perception but\nalso allowed computers to efficiently represent them in their\nown language for later use in _downstream tasks_, such as\nmaterial recognition (Sharan et al., 2013; Chen et al., 2021),\nland classification (Akiva et al., 2022), and biological image\nanalysis (Oiticica et al., 2025).\n\n\nTo tackle these tasks, many texture representation approaches have been proposed in the literature, which currently can be broadly classified in three groups: _handcrafted_\n(Haralick et al., 1973; Manjunath & Ma, 1996; Leung &\nMalik, 2001; Ojala et al., 2002b), _learning-based_ (Cimpoi\net al., 2015; Zhang et al., 2017; Zhai et al., 2020), and _hybrid_\ntechniques (Junior & Backes, 2016; Scabini et al., 2023; Su\n\n\n1 ˜\nInstitute of Biosciences, Humanities and Exact Sciences, Sao\nPaulo State University, Sao Jos ˜ e do Rio Preto, Brazil. Correspon- ´\ndence to: Lucas C. Ribas _<_ lucas.ribas@unesp.br _>_ .\n\n\n\net al., 2023). The first ones are characterized by approaches\nmanually designed from scratch, which provide greater explainability but generally have limited descriptive power.\nOn the other hand, the second group is characterized by the\nuse of neural networks to automatically extract the essential\nattributes. Although they offer greater descriptive power,\nthese methods are generally less interpretable and demand\nsubstantial amounts of data and energy.\n\n\nIn this context, the third group of approaches seeks to leverage the advantages of the previous ones while simultaneously addressing their inherent limitations. In this group,\napproaches are characterized by those that either combine\ntraditional descriptors with neural networks as in Su et al.\n(2023) or those as in Junior & Backes (2016); Scabini et al.\n(2023) that use neural networks while maintaining inherent\ncharacteristics of some _handcrafted_ descriptors, such as the\nabsence of a dataset to work properly as for instance in LBP\n(Ojala et al., 2002b), i.e., given an image a LBP representation is promptly returned without the need of a pre-defined\ntraining and test split. Therefore, thinking in a Venn diagram, _hybrid_ approaches can be seen as the intersection of\nthe _handcrafted_ and _learning-based_ approaches, such that\nsome of the mentioned _learning-based_ techniques may be\nconsidered _hybrid_ ones.\n\n\nCurrently, interesting hybrid texture recognition approaches\nare those based on shallow and fast neural networks. In\n\nparticular, to the best of our knowledge, Junior & Backes\n(2016) proposed the first texture representation learning technique using a Randomized Neural Network (RNN). This is\na shallow 1-hidden-layer neural network, where the hidden\nlayer is randomly initialized and the output layer weights\nare computed using a closed-form solution. This leads to a\nrapid training procedure, and the resulting weights are subsequently used to compose the image representation. This\ntechnique has been successfully applied in representation\nlearning tasks, such as in color-texture (Ribas et al., 2024),\nand shape representation (Junior et al., 2018; Fujiwara &\nHashimoto, 2020; Ribas & Bruno, 2024).\n\n\nHowever, most of these representation learning techniques\nhave focused primarily on constructing improved input and\noutput feature matrices to enhance the learning of weights\nthrough better cross-information prediction, that is, guiding\nthe network to learn more effective weights by forcing it\nto predict more complex relationships between input and\noutput content. In contrast, little to no research in texture\nrepresentation learning has simultaneously explored archi\n\n\n1\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\ntectural improvements, such as enhancements to the random\nprojector or the underlying optimization problem.\n\n\nIn this work, we introduce M IXER, a novel texture representation learning random neural network. It is composed\nof four core modules. For any given image, the first module, called the _local pattern extractor_, is responsible for\ndensely extracting small patches from each image channel\nindependently, aiming to capture raw local texture information. Next, the second module, called the _hyperspherical_\n_random projector_, encodes the extracted patches into random embeddings and constrains them to lie on the surface\nof a hypersphere through embedding normalization. The\nthird module, referred to as the _learning_ module, consists of\ntwo branches that capture intra- and inter-channel intensity\nrelationships. The first branch employs a purely randomized autoencoder, while the second addresses this learning\nthrough a novel optimization problem. Finally, the last module, called _compression_ module, compresses (summarizes)\nthe decoders’ learned weights into a useful color-texture\nrepresentation for recognition tasks.\n\n\n**Contributions of this paper** . Our contributions are summarized as follows:\n\n\n- We introduced a novel hyperspherical multi-head random\nprojector that: (i) distinctly encodes the local patches of\ndifferent image channels, thereby implicitly introducing\nchannel ownership information; and (ii) constrains the\nrandom embeddings to lie on the surface of a hypersphere,\nretaining only their directional information.\n\n\n- We proposed a novel learning module composed of the D I \nRECT and M IXED learning branches. The former captures\nintra-channel intensity relationships, whereas the latter\nfocuses on inter-channel relationships, achieved through\na new optimization problem applied to an intermediate\nfused representation.\n\n\n- Experimental results demonstrated that our color-texture\nrepresentation consistently outperformed the existing approaches, being the only among the compared approaches\nto surpass the 97% barrier of Outex, and the 99% barrier\nof the average accuracy.\n\n\n**Structure of the paper** . Section 2 presents the notation and\nthe required background. Section 3 introduces the method\nof our novel texture representation. Section 4 exhibits the\nexperimental setup. Section 5 shows the results and presents\nthe discussion. To conclude, Section 6 terminates the work\n\nwith the conclusions and future works.\n\n\n**2. Background and Notation**\n\n\n**Notation** . Throughout the paper, we adopt the following\nnotation conventions for readability. Let the lowercase let\n\n\nters _a ∈_ R be scalars, and the boldface lowercase letters\n**a** _∈_ R _[n]_ be _n_ -dimensional vectors, where _a_ _k_ _∈_ R is its\n_k_ -th component. We define the boldface uppercase letters\n**A** _∈_ R _[m][×][n]_ as a real matrix of size _m × n_, where _a_ _ij_ _∈_ R\nis its ( _i, j_ ) -entry. Further, let calligraphic uppercase letters\n_A_ denote a set of elements, with the following exceptions\nthat R _[n]_ is the set of all _n_ -dimensional real vectors, R _[m][×][n]_\n\nis the set of all real matrices of size _m × n_, and N is the set\n\nof natural numbers.\n\n\n**2.1. Texture Recognition**\n\n\n**Classical Texture Recognition** . The early days of texture\nrecognition relied heavily on _classical_ or _handcrafted_ feature extractors, which were manually designed by specialists\nand typically based on predefined steps. To begin with, the\npioneering work by Haralick et al. (1973) not only proposed\na feature extraction technique based on measures such as\nangular second moment, contrast, and correlation derived\nfrom gray-level co-occurrence matrices, but also introduced\nthe notion of texture as the spatial distribution of gray-level\ntones, although no formal and universally accepted definition exists to this day.\n\n\nLater, studies on the visual information processing system\n(Daugman, 1985; Jones & Palmer, 1987) discussed the receptive fields of simple cells in the visual cortex, showing\nthat they can be expressed in terms of Gabor filters. This\ninspired researchers in proposing texture feature extractors\nusing these filters or wavelets (Bovik et al., 1990; Manjunath & Ma, 1996). In general, these approaches consist of\ngenerating a filter bank with filters of distinct orientations\nand frequencies, and then convolving the image with each\nfilter. Subsequently, measures such as the mean and standard deviation can be computed from the filter responses to\nserve as features of the input image.\n\n\nFollowing this, Ojala et al. (1996) proposed the Local Binary Pattern (LBP). At its core, a local neighborhood scans\nthe entire image with overlap, and the values within each\nneighborhood are thresholded relative to the central pixel\nvalue. The numbers resulting from the binary patterns of\neach neighborhood are aggregated into a histogram, which\nserves as the texture representation. In addition, many improved and robuster versions and similar techniques to LBP\nwere proposed, such as Ojala et al. (2002b) that introduced\nthe widely known rotation invariant version of the former\napproach, Ojansivu & Heikkila ¨ (2008) proposed the Local\nPhase Quantization (LPQ) technique, a texture descriptor\nrobust to blurring, and Guo et al. (2010) presented the complete LBP (CLBP).\n\n\nAround the early 2000s, techniques such as Bag of Textons\n(BoT) (Leung & Malik, 2001) and Bag of Visual Words\n(BoVW) (Csurka et al., 2004) emerged, representing an\nimage as a histogram over a learned or predefined code\n\n\n2\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\nbook (Liu et al., 2019). Generally, given a training set,\nthese methods select and describe local patches from the\nimages and then build a codebook by applying a vector\nquantization algorithm, such as _k_ -means, over the set of\nlocal patch descriptors. Later, this codebook is used to generate the histogram from a test image, which serves as its\nfeature vector. In addition, with the rise of the deep-learning\nmodels, the phase of local patch description turned to use\ndeep-learned features, such as in FV-CNN (Cimpoi et al.,\n2015), DeepTEN (Zhang et al., 2017) and BoFF (Florindo\n& Laureano, 2023).\n\n\n**Deep-learning-based Texture Recognition** . Since the\nrapid emergence of deep learning methods in the last decade,\nthe research community has seen a variety of approaches\nadopting Convolutional Neural Networks (CNNs) and, more\nrecently, Vision Transformers (ViTs) for texture recognition.\nThis is largely attributed to their ability to automatically\nlearn and extract key features from images, in contrast to\nthe manually designed descriptors discussed earlier.\n\n\nThese approaches can be divided into two categories: feature\nextraction and end-to-end learning. The former is characterized by the use of the extracted features from a off-the-shelf\npre-trained backbone followed by the use of a classic classifier, such as KNN and SVM. The latter involves training\na complete architecture from scratch or fine-tuning a pretrained model.\n\n\nRemarkable examples of feature extraction approaches are:\nFV-CNN (Cimpoi et al., 2015) which uses the Fisher Vector\n(FV) to pool the local features from a convolutional layer\nof a CNN backbone, ignoring the spatial information, thus\ncreating an orderless representation; and RADAM (Scabini\net al., 2023) that randomly encodes multi-depth aggregated\nfeatures from CNN backbones to later compose the texture\nrepresentation.\n\n\nConversely, in the end-to-end learning category, notable\ntechniques are: DeepTEN (Zhang et al., 2017) where the\nauthors proposed a single network that performs the feature\nextraction, the codebook (or dictionary) learning (in a supervised manner) and the encoding of the representation;\nDEPNet (Xue et al., 2018) where authors discussed that\nthe textures in surfaces preserve some spatial information.\nThus, guided by this starting point, the authors proposed\nthe texture encoding branch and the global average pooling\nbranch. The former addresses the orderless representation\nof the image, while the latter captures local spatial cues.\nLater, DSRNet (Zhai et al., 2020) is a texture recognition\nnetwork that learns the spatial dependency between texture\nprimitives. This was motivated by the fact that multiple\ntexture primitives appear differently in their spatial context;\nhowever, they present an inherent spatial dependency. Following, Chen et al. (2024) proposed the DTP (Deep Tracing\nPattern) module. This module is responsible for leverag\n\n\ning simultaneously shallow and deep layer features, thus\nobtaining rich cross-layer features for texture recognition.\nTo this end, the DTP module perform a cross-layer feature\naggregation, and inspired by LBP, it densely samples 3D\npatches of the grouped tensor to capture spatial and crosslayer information. After, a spatial pyramid histogram is\nused to comprise the feature vector.\n\n\n**2.2. Randomized Neural Network**\n\n\n**General Background** . Randomized neural networks proposed in (Huang et al., 2006; Pao & Takefuji, 1992; Pao\net al., 1994; Schmidt et al., 1992) are continuously being\nused by the research community for pattern recognition\ntasks. In particular, canonical application areas are _classifi-_\n_cation_ and _regression_ . However, as previously mentioned,\nin the last decade novel approaches employing randomized\nneural networks have been proposed for representation learning. These methods have been applied primarily to textures,\ndynamic textures, and shape representations, diverging from\ntheir more common applications.\n\n\nIn general, the large adoption of the community is due to\nthree essential aspects that attracted interest in randomized\nneural networks. In the first place, the simplicity, since the\nmost common cases are architectures composed by a single fully-connected hidden layer with randomly generated\nweights, and whose responsibility is to perform a non-linear\nrandom projection of the input data. Another important\naspect is its speed and low computational cost, which result\nfrom computing the output layer weights through a closedform solution. This enables fast training without the need\nfor backpropagation. Finally, these models also exhibit good\ngeneralization capabilities for certain types of problems.\n\n\nCommonly, there are two widely utilized randomized neural\nnetworks in the literature: Randomized Vector Functional\n\nLink (RVFL), and the Extreme Learning Machine (ELM).\nBoth architectures share the common aspect of non-linearly\nrandom project the input data into another dimensional\nspace, and the characteristic of a fast-training phase conducted via a closed-form solution by using the (regularized)\nleast-squares solution.\n\n\nNevertheless, their main difference lies in how the output\nnodes are connected. The RVFL architecture has the output\nnodes connected both with the hidden nodes and the input\nones, while in the ELM architecture, the output nodes are\nonly connected with the hidden ones. Although its strengths\nand weaknesses have been extensively discussed, particularly regarding how the output layer should be connected.\nIn this work, the output layer is connected only with the\nhidden nodes, following the previous literature in texture\nrepresentation learning using randomized neural networks.\n\n\n**Formalism** . In this part, we present a mathematical back\n\n\n3\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\nground of the randomized neural network.\n\n\nLet _D_ = _{_ ( **x** _i_ _,_ **y** _i_ ) _}_ _[N]_ _i_ =1 _[⊂X ×Y]_ [ be the dataset consisting of]\n_N_ samples, where **x** _i_ _∈X_ is a _p_ -dimensional input feature\nvector, and **y** _i_ _∈Y_ is a _r_ -dimensional output feature vector.\nFrom _D_ it is assembled the input and output feature matrices\nrepresented by **X** = hconcat( **x** 1 _,_ **x** 2 _, . . .,_ **x** _N_ ) _∈_ R _[p][×][N]_,\nand **Y** = hconcat( **y** 1 _,_ **y** 2 _, . . .,_ **y** _N_ ) _∈_ R _[r][×][N]_, respectively,\nwhere hconcat( _·_ ) is the horizontal concatenation operation.\n\n\nFollowing this, the network non-linearly projects these input\nfeature vectors into another dimensional space. To this end,\na matrix _**ψ**_ _∈_ R _[ω][×][p]_ is randomly generated by some strategy,\nsuch as sampling each matrix’s entry from some distribution, e.g., _**ψ**_ _ij_ _∼N_ ( _µ, σ_ [2] ), or using some pseudorandom\nnumber generator procedure. Further, defining this matrix\nis a fundamental step, because the number of neurons in\nthe hidden layer ( _ω_ ), more specifically the dimension of the\nspace where the features are projected into, shall be specified. In this context, upon the specification of the random\nweight matrix _**ψ**_, the hidden or projected feature matrix is\ngiven by:\n**Z** = _α_ ( _**ψ**_ **X** ) _∈_ [0 _,_ 1] _[ω][×][N]_ _,_ (1)\n\n\nwhere _α_ ( _x_ ) := 1 _/_ (1 + _e_ _[−][x]_ ) is the sigmoid function applied\nelement-wise. In general, one can choose another non-linear\nfunction, such as hyperbolic tangent. Accordingly, from\nEquation (1), the projected matrix consists of all real vectors\n**z** _i_ = _α_ ( _**ψ**_ **x** _i_ ) _∈_ R _[ω]_, which represent the _ω_ -dimensional\ninternal representation of the input feature vector **x** _i_ _∈_ R _[p]_ .\n\n\nSubsequently, the learning phase to obtain the trained\nweights of the output layer shall be conducted. In this\nsense, the main objective of a randomized neural network\nis to seek for a matrix _**φ**_ _∈_ R _[r][×][ω]_ of a linear transformation\n_T_ : R _[ω]_ _→_ R _[r]_, that takes the image of **z** _i_, being **z** _i_ the internal representation of **x** _i_, as close as possible to **y** _i_ under the\nsquared _ℓ_ 2 norm. More precisely, this involves solving the\nfollowing minimization problem:\n\n\n**Optimization 1** (Least-squares) **.** Let **X** _∈_ R _[p][×][N]_, **Y** _∈_\nR _[r][×][N]_ and **Z** _∈_ R _[ω][×][N]_ be the previously defined input, output, and projected feature matrices. We seek for a matrix\n_**φ**_ _[∗]_ _∈_ R _[r][×][ω]_ that solves the following minimization problem:\nmin _∥_ **Y** _−_ _**φ**_ **Z** _∥_ _F_ [2] [+] _[ γ]_ [ Ω(] _**[φ]**_ [)] _[,]_ (2)\n_**φ**_\n\n\nwhere _∥·∥_ _F_ is the Frobenius norm, Ω( _·_ ) is the regularization\nterm, and _γ >_ 0 is the regularization parameter.\n\n\nIn particular, when the regularization term is set to **Ω** ( _**φ**_ ) =\n0, i.e., no regularization is applied. The solution for the\nminimization problem in Equation (2) is given by the leastsquares solution: _**φ**_ _[∗]_ = **YZ** _[†]_, where **Z** _[†]_ = **Z** _[T]_ ( **ZZ** _[T]_ ) _[−]_ [1] is\nthe Moore-Penrose pseudoinverse (Penrose, 1955).\n\n\n\nAlternatively, a regularization term may be introduced for\ntwo main reasons. First, the regularization contributes in balancing the model complexity by adjusting the model’s biasvariance trade-off. Second, inverting the matrix **ZZ** _[T]_ may\nbe challenging when it is ill-conditioned. Under these circumstances, one may utilize the Tikhonov’s regularization\n(Calvetti et al., 2000) without losing the closed-form aspect.\nTo this end, the regularization term is set to Ω( _**φ**_ ) = _∥_ _**φ**_ _∥_ [2] _F_ [.]\nIn this case, the solution of Problem (2) is given by the\nregularized least-squares solution:\n\n\n_**φ**_ _[∗]_ = **YZ** _[T]_ ( **ZZ** _[T]_ + _γ_ **Id** ) _[−]_ [1] _,_ (3)\n\n\nwhere **Id** is the identity matrix of size _ω × ω_ .\n\n\n**3. Method**\n\n\nIn this section, we introduce M IXER, our novel approach for\ntexture representation learning. As illustrated in Figure 1,\nM IXER consists of four core modules: 1) Local Pattern Extractor (LPE) module; 2) Hyperspherical Random Projector\n(HRP) module; 3) Learning module; and 4) Compression\nModule. Thus, the remainder of this section is dedicated to\npresenting the internal mechanisms of these modules.\n\n\n**Local Pattern Extractor Module** . Given an image\n**I** _∈_ R _[C][×][H][×][W]_ . This module, depicted in Figure 1(b), is responsible for extracting local intensity patterns from the image by densely sampling multiple small patches. Its purpose\nis to capture variations and implicit spatial relationships\namong pixel intensities, thereby inherently representing raw\ntexture information to be subsequently used as input for the\nremainder of the network.\n\n\nTo this end, the module extracts multiple _J × J_ patches\ncentered at every pixel from each image channel of **I**, thus\nwith overlapping. However, centering a pixel at or near the\nimage border is not feasible because the patch size would\noverflow the image bounds. Therefore, the image is padded\nbefore patch extraction. Assuming that _J_ is an odd natural\nnumber, the padding size applied in all image channel sides\nis _[J][−]_ [1]\n\n2 [, and the utilized padding mode is replication. In]\ngeneral, this module is expressed by:\n\n\nPad _J_ ( **I** ) _∈_ R _[C][×]_ [(] _[H]_ [+] _[J][−]_ [1)] _[×]_ [(] _[W]_ [ +] _[J][−]_ [1)] (4)\n\nLPE(Pad _J_ ( **I** )) _∈_ R _[C][×][H][×][W][ ×][J][×][J]_ _._ (5)\n\n\nTherefore, the output of Equation (5) is a 5 -dimensional tensor that, basically, stores every _J_ _×J_ image patches centered\nat every pixel of the image channel of dimension _H ×_ _W_ for\nevery image channel among the _C_ available. Finally, this\nmodule generalizes the widely used patch-based extraction\ntechnique, extending it to images with an arbitrary number\nof channels, as commonly employed in RNN-based texture\nrepresentation learning approaches (Junior & Backes, 2016;\n\n\n\n4\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n_Figure 1._ Overview of the M IXER pipeline. The input image **I** _∈_ R _[C][×][H][×][W]_ is fed to the Local Pattern Extractor (LPE) module, which\npads the image and subsequently performs the extraction of tiny patches to record the raw texture information. Thereafter, these patches\nare fed to the Hyperspherical Random Projector (HRP) module that maps these patches in hyperspherical random embeddings composing\nthe random projected matrices **Z** _[′′]_ _i_ _[∈]_ [R] [(] _[ω]_ [+1)] _[×][HW]_ [ . The projected matrices are fed to both D] [IRECT] [ and M] [IXED] [ branches responsible for]\nlearning the intra- and inter-channel local intensity relationships, respectively. The linear decoder’s learned weights from both modules are\nfed to the compression module, which is responsible for vertically concatenating them, resulting in the aggregated learned weight matrix,\nand is responsible for applying selected compression functions to compress the weight matrix into a useful color-texture representation.\n\n\n\nRibas & Bruno, 2024). We refer the reader to Section A of\nthe supplementary materials for further details about this\nmodule.\n\n\n**Hyperspherical Random Projector Module** . This module, illustrated in Figure 1(c), encodes the raw pixel intensities of the extracted patches into internal representations,\nreferred to as hyperspherical random embeddings. These\nembeddings are subsequently used during the training phase\nof the proposed learning modules. In this context, to obtain\nthese internal representations, the module performs nonlinear random projections of the patch intensities, encoding\nthem into a generally higher _ω_ -dimensional space. These\nprojections are then constrained to lie on the surface of a\nunit hypersphere through embedding normalization. This\nnormalization step helps computational and training stability\n(Zhang et al., 2023; Wang & Isola, 2020), and here it does\nby adjusting potential column-norm distortions in the projected (hidden) matrix and by enhancing the conditioning\nof the matrix during the inversion step.\n\n\nTo accomplish this, let **L** = LPE(Pad _J_ ( **I** )) be the 5 dimensional tensor output of the previous layer, which contains the pixel intensities of the extracted patches for every\nchannel of the image. We reshape this tensor by independently merging the image spatial dimensions and the patch\nsize dimensions. Following this, we interchange these last\ntwo reshaped dimensions. We express these steps by:\n\n\n**X** = Reshape( **L** ) _∈_ R _[C][×][J]_ [2] _[×][HW]_ _._ (6)\n\n\n\nHence, after the reshaping process, we obtain\n**X** 1 _,_ **X** 2 _, . . .,_ **X** _C_, where each **X** _k_ _∈_ R _[J]_ [2] _[×][HW]_ is a\nmatrix containing the flattened, densely extracted patches\nover the spatial dimensions of the _k_ -th image channel. Subsequently, each of these matrices is used during the random\nprojection phase to obtain the internal representation of\neach flattened patch. To this end, we perform:\n\n\n**Z** _k_ = _α_ ( _**ψ**_ _k_ **X** _[′′]_ _k_ [)] _[ ∈]_ [[0] _[,]_ [ 1]] _[ω][×][HW]_ _[,]_ (7)\n\n\nwhere _**ψ**_ _k_ _∈_ R _[ω][∈]_ [(] _[J]_ [2] [+1)] is the random weight matrix [1] used\nonly to project the _k_ -th pre-processed input feature matrix:\n\n\n**X** _[′′]_ _k_ [=][ vconcat][(] _[{−]_ **[1]** _[HW]_ _[,]_ **[ X]** _[′]_ _k_ _[}]_ [)] _[ ∈]_ [R] [(] _[J]_ [2] [+1)] _[×][HW]_ _[,]_ (8)\n\n\nwhere vconcat( _·_ ) is the vertical concatenation operation,\n**1** _HW_ _∈_ R [1] _[×][HW]_ is a row matrix with all entries set to one,\nrepresenting the bias terms, and **X** _[′]_ _k_ [is the unit normal scaled]\ninput feature matrix defined by:\n\n\n[ **X** _[′]_ _k_ []] _[ij]_ [=] [ **X** _k_ ] _ij_ _−_ [ **X** _k_ ] _i,_ : _,_ _k ∈{_ 1 _,_ 2 _, . . ., C},_ (9)\n\n_s_ _k,i_ + _ϵ_\n\n\nwhere [ **X** _k_ ] _i,_ : = _HW_ 1 � _HWj_ =1 [[] **[X]** _[k]_ []] _[ij]_ [ is the sample mean of]\nthe _i_ -th row of the _k_ -th input feature matrix **X** _k_, _s_ [2] _k,i_ [=]\n\n_HW_ 1 _−_ 1 � _HWj_ =1 [([] **[X]** _[k]_ []] _[ij]_ _[ −]_ [[] **[X]** _[k]_ []] _i,_ : [)] [2] [ is the sample variance,]\n\n\n1 We refer the reader to the Section B of the Supplementary\nMaterial for the random weight matrix generation process.\n\n\n\n5\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\nand _ϵ_ = 10 _[−]_ [10] is a small positive number to prevent division\nby zero.\n\n\nIn this context, we have that **Z** _k_ consists of all the _ω_ dimensional random embedded patch intensities. Following this, before entering the learning modules, these projected feature matrices **Z** 1 _,_ **Z** 2 _, . . .,_ **Z** _C_ have their column\nvectors projected onto the unit hypersphere S _[ω][−]_ [1] = _{_ **v** _∈_\nR _[ω]_ _| ∥_ **v** _∥_ 2 = 1 _}_ . This process is expressed by:\n\n\n[ **Z** _k_ ] _ij_\n\n[ **Z** _[′]_ _k_ []] _[ij]_ [=] _k ∈{_ 1 _,_ 2 _, . . ., C},_ (10)\nmax( _∥_ [ **Z** _k_ ] : _,j_ _∥_ 2 _, ϵ_ ) _[,]_\n\n\nwhere _∥·∥_ 2 is the Euclidean norm, and [ **Z** _k_ ] : _,j_ is the _j_ -th\ncolumn of **Z** _k_ . Finally, as done for the input feature matrix,\nwe also add a bias weight by vertically concatenating a row\nmatrix filled with _−_ 1, such that the output of this module is:\n\n\n**Z** _[′′]_ _k_ [=][ vconcat][(] _[{−]_ **[1]** _[HW]_ _[,]_ **[ Z]** _[′]_ _k_ _[}]_ [)] _[ ∈]_ [R] [(] _[ω]_ [+1)] _[×][HW]_ _[ .]_ (11)\n\n\nTo simplify the notation, we denote **Z** _[′′]_ _k_ [, the output of this]\nmodule for a input feature matrix **X** _k_, as _f_ _**ψ**_ _k_ ( **X** _k_ ), where\n_f_ _**ψ**_ _k_ ( _·_ ) is designated as the _k_ -th random encoder whose\nrandom weight matrix is given by _**ψ**_ _k_ _∈_ R _[ω][×]_ [(] _[J]_ [2] [+1)] .\n\n\n**Learning Module** . This module represents one of the\nmain components of our novel approach, and is shown in\nFigure 1(d). This is partitioned into two learning branches\ncalled the D IRECT and M IXED branches. These are responsible for learning multiple linear decoder networks, where\ntheir learned layer weights (scalars) are used later by the\ncompression (summarization) module to assemble a useful\ncolor-texture representation.\n\n\n- **D** **IRECT** : This branch is responsible for learning a mapping that reconstructs the local intensity patterns of the\nextracted _J × J_ patches from the hyperspherical random\nembeddings, with the intent to induce the network to learn\nthe relationship between the hidden (projected) space and\nthe intensities (input) space, from the point of view of the\nraw spatial texture present in the input matrix **X** .\n\n\nTo this end, this branch learns the mapping through the\nfollowing optimization problem.\n\n\n**Optimization 2** (Direct Branch) **.** Let _g_ _**θ**_ ( **Z** ) = _**θ**_ **Z** be\na linear decoder network with parameters _**θ**_ . We seek\nto learn the parameters _**θ**_ _[∗]_ _k_ _[∈]_ [R] _[J]_ [2] _[×]_ [(] _[ω]_ [+1)] [ for each image]\nchannel by solving the following minimization problem:\n\n\nmin _**θ**_ _k_ _[∥]_ **[X]** _[k]_ _[ −]_ _[g]_ _**[θ]**_ _[k]_ [(] _[f]_ _**[ψ]**_ _[k]_ [(] **[X]** _[k]_ [))] _[∥]_ _F_ [2] [+] _[ γ]_ _[D]_ _[∥]_ _**[θ]**_ _[k]_ _[∥]_ [2] _F_ _[,]_ (12)\n\n\nwhere _γ_ _D_ _>_ 0 is the regularization parameter of the\ndirect branch. This optimization problem acts as a pure\nrandomized autoencoder, where we force the network to\nlearn how to reconstruct the original patches intensities\nfrom a non-linear random projection of themselves.\n\n\n\nConsequently, from Equation (15), we have that the\nshared internal representation **s** _k_ of some patch _k_, emerge\nas the average of the internal representations of the same\n\n\n\nThe optimization problem presented in Equation (12) has\na closed-form solution given by the following regularized\nleast-squares solution:\n\n\n_**θ**_ _[∗]_ _k_ [=] **[ X]** _[k]_ **[Z]** _[′′]_ _k_ _T_ ( **Z** _′′k_ **[Z]** _[′′]_ _k_ _T_ + _γ_ _D_ **Id** ) _−_ 1 _._ (13)\n\n\nFinally, the subscript is emphasized because a linear decoder network is found for each channel of the input\nimage. Thus, a set of linear decoders that learned the spatial texture relationships of each channel independently\nis obtained. In this sense, this branch outputs the set\n_S_ _D_ = _{_ _**θ**_ _[∗]_ 1 _[,]_ _**[ θ]**_ _[∗]_ 2 _[, . . .,]_ _**[ θ]**_ _[∗]_ _C_ _[}]_ [ that characterizes the local inten-]\nsity relationships present in the input image channels.\n\n\n- **M** **IXED** : Unlike the D IRECT branch that uses its own\n\npatch internal representation to reconstruct itself. This\nbranch aims to reconstruct each patch not only from its\nown random embedding but also from the random embeddings of the corresponding patch across all image\nchannels.\n\n\nIn this context, this branch first constructs a shared representation by combining the internal representations, serving as an intermediate fusion phase. Following this, the\nresult of the mixing process is used as input to the decoder\nnetwork, whose main role is to reconstruct the original\npatch intensities. Therefore, the network is guided to\nreconstruct the original patch information of a specific\nimage channel by leveraging all available information\nfrom the corresponding patch across all channels, thereby\nlearning inter-channel relationships.\n\n\nTo achieve this, this branch learns the reconstruction\nthrough the following optimization problem:\n\n\n**Optimization 3** (Mixed Branch) **.** Let _g_ _**ϕ**_ ( **Z** ) = _**ϕ**_ **Z** be\na linear decoder network with parameters _**ϕ**_ . We seek\nto learn the parameters _**ϕ**_ _[∗]_ _k_ _[∈]_ [R] _[J]_ [2] _[×]_ [(] _[ω]_ [+1)] [ for each image]\nchannel by solving the following minimization problem:\n\n\nmin _**ϕ**_ _k_ _∥_ **X** _k_ _−_ _g_ _**ϕ**_ _k_ ( **S** ) _∥_ _F_ [2] [+] _[ γ]_ _[M]_ _[∥]_ _**[ϕ]**_ _k_ _[∥]_ _F_ [2] _[,]_ (14)\n\n\nwhere _γ_ _M_ _>_ 0 is the regularization parameter of the\nmixed branch, and **S** _∈_ R [(] _[ω]_ [+1)] _[×][N]_ is the mixed random\nembedding built by averaging all internal random embeddings obtained for each image channel. That is,\n\n\n\n**S** = [1]\n\n_C_\n\n\n\n_C_\n� _f_ _**ψ**_ _j_ ( **X** _j_ ) _._ (15)\n\n_j_ =1\n\n\n\n6\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\npatch for all image channels. Next, similarly to the preceding optimization problem, the closed-form solution for\nthe Equation (14) is given by the following regularized\nleast-squares solution:\n\n\n_**ϕ**_ _[∗]_ _k_ [=] **[ X]** _[k]_ **[S]** _[T]_ [ (] **[SS]** _[T]_ [ +] _[ γ]_ _[M]_ **[Id]** [)] _[−]_ [1] _[ .]_ (16)\n\n\nFinally, as in the direct branch, the output of this branch is\na set of linear decoders _S_ _M_ = _{_ _**ϕ**_ _[∗]_ 1 _[,]_ _**[ ϕ]**_ _[∗]_ 2 _[, . . .,]_ _**[ ϕ]**_ _[∗]_ _C_ _[}]_ [ trained]\nto reconstruct the local intensity patterns of each image\nchannel from a common mixed representation.\n\n\n**Compression Module** . Given an image **I** _∈_ R _[C][×][H][×][W]_ its\nimage representation should be defined by a _n_ -dimensional\nreal vector that carries useful information about **I** to be\n\nused in downstream tasks. Nevertheless, as was observed,\na set _S_ = _S_ _D_ _∪S_ _M_ _⊂_ R _[J]_ [2] _[×]_ [(] _[ω]_ [+1)] containing the learned\nlinear decoders’ weights is obtained as output of the learning\nmodule (i.e., previous module). In this context, the role\nof this module, illustrated in Figure 1(e), is to compress\n(summarize) the learned weights of these linear decoders,\nrepresented by real matrices, in real vectors to serve as a\nuseful color-texture representation. To this end, we begin\nby defining a compression function:\n\n\n**Definition 3.1** (Compression Function) **.** Every scalarvalued function _h_ : _A ⊂_ R _[n]_ _→_ R that maps a _n_ dimensional real vector **x** _∈_ _A_ to a scalar _h_ ( **x** ) _∈_ R, where\n_h_ ( **x** ) embeds some structural or semantic information about\n**x** is called a compression (summarization) function.\n\n\nBy defining the compression function, also called the summarization function, we express that _h_ must extract some\nuseful knowledge (simple or complex) about the input vector. In this sense, a function that receives any real vector\nand simply returns a random number cannot be considered\na compression function, since it does not embed any information.\n\n\nIn this context, let **F** = vconcat( _S_ ) _∈_ R _[|S|][J]_ [2] _[×]_ [(] _[ω]_ [+1)] be\nthe real matrix resulting from the vertical concatenation of\nevery matrix in _S_, where _|S|_ = 2 _C_, since _C_ decoders are\nlearned in each branch. As a result, the matrix **F** contains\nthe weights of the all decoders learned in both proposed\nbranches. Since these weights contain valuable content\nabout the texture information of the input image, a set of\nsummarization functions _H_ are applied to compress the\nmatrix **F** in a real vector serving as representation of the\ntexture image **I** . Therefore, this compression process is\nrepresented by:\n\n\n**s** _h_ = ( _h_ ( **f** 1 ) _, h_ ( **f** 2 ) _, . . ., h_ ( **f** _ω_ +1 )) _∈_ R _[ω]_ [+1] (17)\n\n**Ω** _ω_ ( **I** ) = hconcat( _{_ **s** _h_ _| h ∈H}_ ) _∈_ R _[|H|]_ [(] _[ω]_ [+1)] _,_ (18)\n\n\n\nwhere **f** _k_ _∈_ R _[|S|][J]_ [2] is the _k_ -th column of **F**, **s** _h_ is the real\nvector representing the compressed matrix **F** under _h ∈H_,\nand **Ω** _ω_ ( **I** ) is the texture representation of the image **I** using\n_ω_ -dimensional random embeddings.\n\n\nIn the present work, the set of compression functions employed is composed by four statistical measures _h_ _µ_, _h_ _σ_, _h_ _γ_\nand _h_ _κ_ which corresponds to the mean, standard deviation,\nskewness, and excess kurtosis, respectively. Thus, the dimensionality of the texture representation in Equation (18)\nis given by 4( _ω_ + 1) . For more information about the employed statistical measures, such as their formulas, we refer\nthe reader to Section C of the supplementary material.\n\n\nFinally, we assemble a new texture representation by late fusing the learned representations from distinct _ω_ 1 _, ω_ 2 _, . . ., ω_ _n_\nrandom embeddings sizes. This is a common utilized approach in previous RNN-based techniques, such as (Junior\n& Backes, 2016; Ribas et al., 2020), which shown to improve the performance of the texture recognition task. Thus,\nwe define this late fused color-texture representation as:\n\n\n**Υ** _W_ ( **I** ) = hconcat( _{_ **Ω** _ω_ ( **I** ) _| ω ∈W}_ ) _∈_ R _[m]_ _,_ (19)\n\n\nwhere _W_ = _{ω_ 1 _, ω_ 2 _, . . ., ω_ _n_ _} ⊂_ N is the set containing the\nrandom embeddings sizes used to compose the late fused\nrepresentation, and _m_ = 4 _·_ [�] _ω∈W_ [(] _[ω]_ [ + 1)] [ is the dimen-]\n\nsionality of the proposed color-texture representation.\n\n\n**4. Experiments**\n\n\nIn this section, we present the server infrastructure used\nto conduct the experiments, along with the versions of the\nmain libraries employed to develop the proposed approach.\nWe then describe the benchmark datasets used to evaluate\n\nthe technique, highlighting their main characteristics. Finally, we detail the cross-validation strategy, the evaluation\nmetrics, and the classifier.\n\n\n**Setup** . The experiments were conducted on a server running Ubuntu 22.04.03 LTS operating system equipped with\na Intel i9-14900K processor, 128 GB of RAM, and a single\ngraphic card NVIDIA RTX 4090. Specifically, the approach\nruns on the specified graphic card. Furthermore, the proposed technique is implemented using P Y T ORCH v2.6.0,\nand the evaluation process used the SCIKIT - LEARN v1.6.1\npackage.\n\n\n**Benchmarks** . We evaluated our approach using four wellknown texture datasets exhibiting distinct textural challenges, attributes, and characteristics. In this context, this\nenabled us to analyze the novel approach over a broad range\nof scenarios and how well the proposed approach generalized in the texture recognition task. Next, we briefly detail\nthe peculiarities of each employed texture dataset:\n\n\n- **OutexTC13** (Ojala et al., 2002a): This is the thirteenth\n\n\n\n7\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\ntexture classification suite proposed from the Outex surfaces and is widely used in both grayscale and colored\nversions. This dataset has proven to be hard to categorize\ndue to the need for an efficient micro-textural characterization. Thus, as done in (Backes et al., 2013), the employed\ndataset configuration is composed by 1360 color-texture\nimages obtained by cropping 20 non-overlapping windows of size 128 _×_ 128 pixels from 68 color-texture source\nimages of size 538 _×_ 746 pixels.\n\n\n- **CUReT** (Dana et al., 1999): It stands for ColumbiaUtrecht Reflectance and Texture dataset. This dataset\nwas proposed to study the appearances of the textured\nsurfaces under distinct illuminations and viewing angles,\ntherefore, tackling conditions that most datasets would\nnot. In this sense, the presence of real-world surface textures (e.g., aluminum, leather, corduroy, paper etc.) under\nmultiple light conditions and viewing angles makes the\ntexture recognition task harder, thereby resulting in a suitable dataset for evaluation. The utilized dataset setup\nconsists of 5612 color-texture images resulting from 61\nexisting classes with each 92 samples with dimensions\n200 _×_ 200 pixels. This is the same setup performed in\n(Scabini et al., 2020).\n\n\n- **USPtex** (Backes et al., 2012): This natural image dataset,\nbuilt by the University of Sao Paulo focused on obtaining ˜\nsamples found day-to-day (e.g., rice, vegetation, walls,\nbricks etc.). The intent was to obtain a dataset with a\nhigher degree of texture samples that exhibit irregular,\nrandom, or non-periodic patterns. The utilized configuration is composed by 2292 color-texture images divided\nin 191 classes, each containing 12 samples of dimensions\n128 _×_ 128 pixels.\n\n\n- **MBT** (Abdelmounaime & Dong-Chen, 2013): The Multiband Texture (MBT) dataset was proposed for the analysis\nof textural approaches focusing in samples with intra- and\ninter-band spatial variations. The colors present in these\nsamples of MBT are due to the distinct textural content\nthat comprises each color channel. In this context, this\nis an interesting dataset to evaluate how well our novel\napproach simultaneously describes these intra- and interband spatial variations. As done in (Scabini et al., 2020),\nwe employed the dataset configuration that comprises\n2464 samples with dimension 160 _×_ 160 pixels by cropping 16 non-overlapping windows from each of the 154\noriginal images of size 640 _×_ 640 pixels.\n\n\n**Cross-validation, Metrics and Classifier** . To evaluate the\nperformance of the novel approach we utilized the Linear\nDiscriminant Analysis (LDA) classifier with the Leave-OneOut (LOO) cross-validation strategy using the accuracy as\nperformance metric. This metric was used since all utilized benchmark datasets are balanced, and is a well-known\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|M ( ) 10|5<br>4<br>3<br>2<br>1<br>0<br>1<br>2<br>3<br>4<br>5|Col3|\n|---|---|---|\n|log|log|log|\n|log|log|~~2~~<br>~~1 0~~<br>~~1~~<br>~~2~~<br>~~3~~<br>~~4~~<br>~~5~~<br>log10 (<br>D)|\n\n\n|M ( ) 10|5<br>4<br>3<br>2<br>1<br>0<br>1<br>2<br>3<br>4<br>5|Col3|\n|---|---|---|\n|log|log|log|\n|log|log|~~2~~<br>~~1 0~~<br>~~1~~<br>~~2~~<br>~~3~~<br>~~4~~<br>~~5~~<br>log10 (<br>D)|\n\n\n\n_Figure 2._ Accuracy (%) behavior of the color-texture representation **Ω** 59 ( **I** ) for all the benchmark datasets as the regularization of\nthe D IRECT and M IXED branches varies.\n\nperformance proxy used for evaluation of texture recognition methods. In addition, we adopted this linear decision\nboundary classifier owing to its simplicity, and we used\nthe default hyperparameter configuration provided by the\nSCIKIT - LEARN v1.6.1 package, which uses the Singular\nValue Decomposition (SVD) as solver. For more information about the setting, we refer the reader to the supplementary material.\n\n\n**5. Results and Discussions**\n\n\n**Regularization Analysis** . To understand the effect of the\nregularization on the decoder’s learned weights, and consequently, in the color-texture representation. We analyzed\nthe behavior of the obtained accuracies of the texture representation **Ω** 59 ( **I** ) across all benchmark datasets, as the\nregularization hyperparameters _γ_ _D_ and _γ_ _M_ for the direct\nand mixed branches, respectively, varies over the region\n_R × R_, where _R_ = _{_ 10 _[k]_ _| k ∈_ Z _∧−_ 5 _≤_ _k ≤_ 5 _}_ .\n\n\nFigure 2 exhibits a greater performance of the proposed\ncolor-texture representation in Outex and CUReT as the\nregularization values for both branches increased. This is\nobserved by warmer colors, representing greater accuracy,\npresent on the upper right region, which is determined by\nhigh regularization on both branches. In that region, accuracies of up to 96.9% were achieved in Outex, and up\nto 98.0% in CUReT, thus indicating a good performance\nof the texture representation in this condition. In addition,\nanother common aspect in these datasets was the relatively\nlow performance identified by the presence of cooler colors\nin the remaining region, corresponding to the cases in which\neither one of the branches exhibited low regularization or\nboth branches did.\n\n\n\n8\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_Figure 3._ Accuracy (%) behavior of the color-texture representation **Ω** 59 ( **I** ) for the Outex & CUReT and USPtex & MBT dataset pairs as\nthe regularization of the D IRECT and M IXED branches varies. The behavior is the average of the accuracies obtained by the datasets in\neach configuration. The inset plot in each figure refers to the region near to the highest average accuracy. **Note** : The color bar of the inset\nplot was adjusted to help the visualization of the highest average accuracy.\n\n\n\nThese findings suggest that Outex and CUReT favor from\nthe use of both branches, and while these are operating in\nconjunction, they had a stronger preference towards higher\nregularization to obtain good performances. This may be\nattributed to the fact that stronger regularization induces\nthe decoders to reduce the fitting of the original patches\nintensities as well as possible noise, thus forcing the decoder\nto focus on the general discriminative information about\nthe texture on the image, instead of the small peculiarities\npotentially present in it.\n\n\nFollowing, USPtex shown a common aspect with the last\ntwo evaluated benchmarks, wherein all three presented a\nlow performance in the upper left region, corresponding to\nlow regularization on the D IRECT branch and high on the\nM IXED one. However, the fact that the proposed representation in USPtex shown a moderate performance when both\nregularization values were low, and this performance weakened as only the M IXED branch regularization increased\nindicates that this branch played a key role in the generalizability of the USPtex benchmark, and might be attributed to\nthe fact that the increased regularization resulted in underfit,\nand consequently the decoder did not properly learn the\ntextural content of the image under the mixing optimization problem, thus weakening the overall representation. In\naddition, unlike Outex and CUReT, USPtex has achieved\nthe highest accuracy of 99.5% with moderate values for\nregularization in both branches.\n\n\nSubsequently, the performance behavior for the MBT\ndataset was basically the opposite of that observed in Outex\nand CUReT. The higher accuracies occurred outside the\nhigh regularization region, whereas the lower accuracies\nwere obtained there. This denotes that both branches may\nbe suffering from underfit when strongly regularized, as\nin the case of USPtex for the M IXED branch. Neverthe\nless, unlike the last three benchmarks, the MBT exhibited\ngood performance on a broad range of regularization values,\n\n\n\nparticularly those characterized by low to moderate levels. Among these regularization levels, the representation\nachieved the maximum accuracy of 99.5%.\n\n\nIn addition to the preceding analysis, Figure 2 shows two\npairs of datasets that exhibit similar suitable regularization\nvalues. The first pair comprises Outex and CUReT, while\nthe second consists of USPtex and MBT. Thus, to ensure\nfair hyperparameter selection within each pair, we adopt\nthe same regularization values across the datasets. We analyzed their average accuracies through a joint evaluation as\nthe regularization values varied within the same parameter\n\nspace.\n\n\nFigure 3 presents the joint analysis for both pairs of datasets.\nFor Outex and CUReT, the inset plot highlighting the region\nwith the highest accuracy clearly shows that _γ_ _D_ = 10 [4] and\n_γ_ _M_ = 10 [5] yield the best average configuration. Similarly,\nfor USPtex and MBT, the plot reveals that _γ_ _D_ = 10 [0], and\n_γ_ _M_ = 10 [0] are the optimal configurations on average as well.\nTherefore, these configurations will be used for each pair\nfor the remainder of the paper.\n\n\n**Ablation Analysis** . Figure 4 presents the results for the ablation analysis of the proposed texture representation **Ω** _ω_ ( **I** ) .\nIn Outex and CUReT, it can be observed that the representation obtained using both branches increased the performance, being more notable for Outex. Furthermore, although the M IXED branch achieved the lowest accuracies\non these datasets, the final representations were strengthened when both learning branches were combined. In other\nwords, accuracy improved when both branches were used,\nsuggesting that the features learned in each branch complemented one another.\n\n\nIn USPtex and MBT, it was observed that using only a single\nbranch was sufficient to achieve the highest performance.\nThis indicates that one of the learning branches is already\ncapable of extracting the meaningful features from the tex\n\n\n9\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\n\n\n\n|Both<br>Direct|Col2|Col3|\n|---|---|---|\n||~~M~~|~~xed~~|\n\n\n\n|Both<br>Direct|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|||||||||\n|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|Both<br>Direct<br>|||||||||\n|~~Mixed~~|~~Mixed~~|~~Mixed~~|~~Mixed~~|~~Mixed~~|~~Mixed~~|~~Mixed~~|~~Mixed~~|~~Mixed~~|||||||||\n|~~Mixed~~|||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n||||||||||||||||||\n\n\n_Figure 4._ Average accuracy (%) behavior of the proposed texture\nrepresentation **Ω** _ω_ ( **I** ) when either one of the branches are being\nused or both of them are.\n\n\nture image. Interestingly, while USPtex shows the M IXED\nbranch achieving the highest accuracy, MBT exhibits the\nbest performance with the D IRECT branch.\n\n\nTherefore, this brief analysis demonstrates the beneficial\nimpact of employing both learning branches, particularly in\nthe Outex and CUReT datasets, as well as the usefulness of\nhaving both branches available. This design highlights the\nadaptability of the approach to extract features from diverse\ntextural patterns, as illustrated by the USPtex and MBT\ndatasets, where a single branch was sufficient. However, the\nremainder of the paper uses the color-texture representation\nobtained when both learning branches are activated.\n\n\n**Embedding Size Analysis** . We evaluated how the accuracy\nof the proposed texture representation **Ω** _ω_ ( **I** ) behaves as its\nunique parameter _ω ∈_ N, representing the random embedding size, varies. To this end, the parameter space where\n_ω_ varied over was defined as _O_ = _{_ 9 _,_ 19 _, . . .,_ 109 _}_, and\nis broader than those presented in previous studies (Junior\n& Backes, 2016; Ribas et al., 2020). We also presented\nthe obtained accuracies for all benchmark datasets, thereby\nproviding a thorough analysis of the parameter impact.\n\n\nFigure 5 presents the performance for all datasets. In particular, Outex generally exhibited an increase in accuracy\nas the embedding size grew. However, performance drops\nwere observed in the intervals [29 _,_ 39]N and [59 _,_ 89]N . The\nformer showed a more pronounced drop, while the latter exhibited only a subtle decrease. Conversely, CUReT, USPtex,\nand MBT, unlike Outex, displayed a sharp increase in performance at the initial values, followed by relative stability\nacross the remaining parameter space.\n\n\nThis suggests that the embedding size had a beneficial impact on the overall performance of the proposed representation. This is largely motivated by the increasing number\nof learned features which depends on _ω_, and which are\n\n\n\ndiscriminative enough to almost saturate the CUReT, USPtex and MBT simultaneously, and surpass the 97% barrier,\nand almost the 98% one, performance of the Outex dataset,\nwhere to the best of our knowledge is the unique RNN-based\napproach to achieve this as shall be discussed later.\n\n\nIn addition, following previous studies (Junior & Backes,\n2016; Ribas et al., 2020), we also investigated the performance of the novel approach **Υ** _W_ ( **I** ) when more random\nembedding sizes are incorporated. We performed this analysis because previous studies have shown that combining\nfeatures learned from lower-dimensional embeddings generally yields representations with higher performance, even\nwhen the total number of features remains the same. In particular, our study considered only two random embedding\nsizes to avoid generating excessively large feature vectors,\nwith the selected pair of embedding sizes satisfying:\n\n\n( _ω_ 1 _, ω_ 2 ) _∈{_ ( _ω_ 1 _, ω_ 2 ) _∈O × O | ω_ 1 _< ω_ 2 _} ._ (20)\n\n\nTable 1 shows the performance of the approach for all benchmark datasets over the combined embeddings. Although\nthere were a performance improvement of **Υ** _W_ ( **I** ) over\n**Ω** _ω_ ( **I** ), the increase was modest. Particularly, the greater improvement occurred in CUReT with 99.7% ( **Υ** 99 _,_ 109 ( **I** )) _vs._\n99.0% ( **Ω** 109 ( **I** )), followed by Outex 97.9% ( **Υ** 39 _,_ 79 ( **I** )) _vs._\n97.7% ( **Ω** 109 ( **I** )), and MBT 99.8% ( **Υ** 69 _,_ 79 ( **I** )) _vs._ 99.6%\n( **Ω** 109 ( **I** )) . There was no performance gain in USPtex. This\nsuggests that our simpler approach **Ω** _ω_ ( **I** ) with only one\nembedding already contains enough discriminative features\nfor classification.\n\n\nNevertheless, our experiments are consistent with previous\nfindings indicating that combining lower-dimensional embeddings can yield better performance while maintaining\nthe same number of features. For instance, in Outex, the\nrepresentation **Υ** 29 _,_ 59 ( **I** ) achieved 97.4% with 360 descriptors, where the representation **Ω** 89 ( **I** ) achieved 96.7% with\nthe same amount of descriptors.\n\n\nWe end this section by selecting the texture representation\nthat achieved the highest average accuracy, for a fairness\ncomparison with the literature methods. This representation\nwas **Υ** 39 _,_ 109 ( **I** ) and achieved the average accuracy of 99.2%\nwith 600 descriptors.\n\n\n**Literature Comparison** . We compared our novel approach\nwith two types of texture recognition methods: the _hand-_\n_crafted_ ones, and those based on _randomized neural net-_\n_works_ . Included among the handcrafted are the OpponentGabor (Jain & Healey, 1998), composed by opponent features from Gabor-filter outputs; the ones based in local\npatterns, such as LPQ (Ojansivu & Heikkila ¨, 2008) and\nCLBP (Guo et al., 2010); and those that models the texture\nas complex networks, building the representation upon network’s topological measures, such as MCND (Scabini et al.,\n\n\n\n10\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n||||||||||~~O~~|~~ute~~||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n||||||||||~~CU~~|~~Re~~||\n\n\n\n_Figure 5._ Accuracy (%)\nbehavior of the proposed texture representation **Ω** _ω_ ( **I** ) as the random embedding size _ω_\nvaries in the defined parameter space. This behavior analysis is presented for all benchmark\n\ndatasets.\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||\n||||||||||||||\n||||||||||||||\n|||||||||||~~US~~|~~Pte~~|~~x~~|\n||||||||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||~~B~~||\n|||||||||||||\n\n\n\n2019), and SSN (Scabini et al., 2020).\n\n\n\nFollowing this, among the RNN-based approaches, the main\nones to be compared, include: SSR (Ribas et al., 2024),\nwhich combines complex network theory with the randomized neural network, the SST (Fares & Ribas, 2024) that\nperforms a spatio-spectral texture learning by cross-channel\npredicting, and the VCTex (Fares & Ribas, 2025) that uses\nvolumetric (3D) color cubes for simultaneous color-texture\nencoding, thereby requiring only a single neural network\nfor learning the raw texture representation, differing from\nthe last two approaches that require more.\n\n\nTable 2 presents the accuracy comparison among the literature methods. In Outex, our novel approach attained the\nhighest accuracy of 97.8%, being 1.0% greater than SSN,\nthe second-best technique, and being the unique approach\nto surpass the 97% barrier, and almost the 98%. Similarly\nto Outex, in CUReT, our approach also achieved the highest\naccuracy of 99.5%, being 0.9% greater than the two runnerup approaches, which were SSN and SSR, achieving 98.6%.\nThus, in both discussed datasets, the better performances\nindicate increases of approximately 13, and 50 images correctly classified, respectively, with all this achieved using a\nsmaller number of descriptors.\n\n\nIn USPtex and MBT, our proposed technique achieved the\nhighest accuracy on the former and the second-best performance on the latter. The increase and the respective decrease\nwere minimal, a difference of only 0.1% in both datasets.\nThus, taking into account implementation and hardware\nvariability, it is reasonable to state that no significant improvements or declines were observed. However, it should\nbe noted that our approach achieved the best accuracy in the\nlast benchmarks with a smaller number of features, indicating that it provides a more efficient descriptor.\n\n\nFinally, our approach presented the highest average accu\n\n\nracy of 99.2% among the compared methods, representing\nan increase of 0.7% over the second-best accuracy of 98.5%\nachieved by SSN, and being the only technique to surpass\nthe 99%. Therefore, this emphasizes its greater discriminative power and its adaptability to diverse textural patterns\nand conditions present in the tackled benchmarks.\n\n\n**6. Conclusions and Future Works**\n\n\nIn this work, we introduced M IXER, a texture representation\nlearning approach that learns by decoding the encoded texture information from hyperspherical random embeddings.\nBy leveraging two learning branches, called D IRECT and\nM IXED branches, trained by distinct optimization objectives, and using as representation the weights of the learned\nlinear decoder networks, our approach was able to learn a\nrobust and efficient texture descriptor, achieving the highest\naverage accuracy in the texture benchmarks.\n\n\nTo achieve this, we first densely extract local patches from\nthe image to capture the raw texture information. Following, these patches are randomly encoded and restricted to\nlie on the surface of a unit hypersphere, emphasizing the\ndirectional information. Next, the D IRECT module learns to\nreconstruct the original patch information just by looking\nat its own representation. Conversely, the M IXED module\nlearns to reconstruct the original patch of a specific channel\nby mixing the internal representation of the same patch in\nall image channels, thus performing a simultaneous crosschannel reconstruction by means of an intermediate fused\nrepresentation.\n\n\n**Future Works** . There are two interesting points for future\nworks. First, we believe that there might be better ways to\ncreate the input and output matrices by leveraging distinct\ntexture modeling techniques, such as those using _complex_\n_networks_ . Further, modeling these matrices differently im\n\n\n11\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\nAccuracy ( _↑_ )\n( _ω_ 1 _, ω_ 2 ) Outex13 CUReT USPtex MBT Avg.\n_Line left for adjustment_\n(9,19) 95.4 95.4 98.8 98.8 97.1\n(9,29) 96.2 96.2 99.2 98.9 97.6\n(9,39) 95.5 97.4 99.2 99.4 97.9\n(9,49) 96.5 98.0 99.3 99.2 98.3\n(9,59) 96.8 98.4 99.5 99.5 98.6\n(9,69) 96.5 98.6 99.5 99.6 98.6\n(9,79) 96.8 98.7 99.4 99.5 98.6\n(9,89) 96.9 99.1 99.6 99.4 98.8\n(9,99) 97.4 99.1 99.5 99.4 98.9\n(9,109) 97.5 99.3 99.7 99.6 99.0\n(19,29) 96.4 97.6 99.3 99.2 98.1\n(19,39) 96.4 98.2 99.2 99.4 98.3\n(19,49) 96.5 98.3 99.3 99.3 98.4\n(19,59) 97.1 98.6 99.6 99.5 98.7\n(19,69) 97.2 98.8 99.5 99.4 98.7\n(19,79) 97.3 99.0 99.6 99.5 98.9\n(19,89) 97.1 99.2 99.6 99.5 98.9\n(19,99) 97.4 99.3 99.5 99.6 99.0\n(19,109) 97.5 99.2 99.7 99.7 99.0\n(29,39) 96.6 98.4 99.6 99.4 98.5\n(29,49) 96.8 98.5 99.5 99.3 98.5\n(29,59) 97.4 98.9 99.6 99.3 98.8\n(29,69) 97.3 98.9 99.6 99.6 98.9\n(29,79) 97.6 99.1 99.5 99.5 98.9\n(29,89) 97.3 99.3 99.7 99.4 98.9\n(29,99) 97.1 99.3 99.7 99.4 98.9\n(29,109) 97.8 99.4 99.6 99.5 99.1\n\n\n\nAccuracy ( _↑_ )\n( _ω_ 1 _, ω_ 2 ) Outex13 CUReT USPtex MBT Avg.\n(39,49) 97.1 98.8 99.6 99.5 98.8\n(39,59) 97.6 99.1 99.6 99.7 99.0\n(39,69) 97.6 99.1 99.6 99.7 99.0\n(39,79) 97.9 99.2 99.5 99.6 99.1\n(39,89) 97.1 99.3 99.7 99.5 98.9\n(39,99) 97.4 99.4 99.5 99.6 99.0\n(39,109) 97.8 99.5 99.7 99.7 99.2\n(49,59) 97.4 99.1 99.7 99.5 98.9\n(49,69) 97.1 99.1 99.6 99.6 98.9\n(49,79) 97.5 99.2 99.5 99.6 99.0\n(49,89) 97.1 99.3 99.5 99.4 98.8\n(49,99) 97.6 99.4 99.7 99.6 99.1\n(49,109) 97.6 99.4 99.5 99.6 99.0\n(59,69) 97.6 99.2 99.7 99.6 99.0\n(59,79) 97.6 99.5 99.6 99.6 99.1\n(59,89) 97.4 99.5 99.6 99.6 99.0\n(59,99) 97.6 99.5 99.5 99.6 99.1\n(59,109) 97.4 99.6 99.6 99.7 99.1\n(69,79) 97.8 99.3 99.5 99.8 99.1\n(69,89) 97.4 99.4 99.7 99.6 99.0\n(69,99) 97.2 99.5 99.6 99.5 99.0\n(69,109) 97.7 99.6 99.6 99.7 99.2\n(79,89) 97.7 99.5 99.7 99.6 99.1\n(79,99) 97.3 99.6 99.6 99.5 99.0\n(79,109) 97.5 99.6 99.7 99.6 99.1\n(89,99) 97.3 99.6 99.6 99.6 99.0\n(89,109) 97.4 99.6 99.7 99.7 99.1\n(99,109) 97.4 99.7 99.7 99.7 99.1\n\n\n\n_Table 1._ Accuracy (%) of the proposed texture representation **Υ** _ω_ 1 _,ω_ 2 ( **I** ) for all the benchmark datasets. The left table presents the\naccuracies for the random embedding sizes combinations where _ω_ 1 _∈{_ 9 _,_ 19 _,_ 29 _}_ . The right table exhibits the accuracies for the remaining\nof the combinations of the previously defined space for ( _ω_ 1 _, ω_ 2 ).\n\n\n\nposes the network to cross this information, which might\nproduce better representations. Second, we suggest that\ndirectly adjusting the optimization problem accounting for\na multi-objective least squares, maintaining the closed-form\nexpressiveness, might produce interesting representations\ndepending on how each objective is specified.\n\n\n**Acknowledgements**\n\n\nR. T. Fares acknowledges support from FAPESP (grant\n#2024/01744-8), L. C. Ribas acknowledges support from\nFAPESP (grants #2023/04583-2 and 2018/22214-6).\n\n\n**References**\n\n\nAbdelmounaime, S. and Dong-Chen, H. New brodatz-based\nimage databases for grayscale color and multiband texture\nanalysis. _International Scholarly Research Notices_, 2013\n\n\n\n(1):876386, 2013.\n\n\nAkiva, P., Purri, M., and Leotta, M. Self-supervised material and texture representation learning for remote sensing tasks. In _Proceedings of the IEEE/CVF Conference_\n_on Computer Vision and Pattern Recognition_, pp. 8203–\n8215, 2022.\n\n\nBackes, A. R., Casanova, D., and Bruno, O. M. Color\ntexture analysis based on fractal descriptors. _Pattern_\n_Recognition_, 45(5):1984–1992, 2012.\n\n\nBackes, A. R., Casanova, D., and Bruno, O. M. Texture\nanalysis and classification: A complex network-based\napproach. _Information Sciences_, 219:168–180, 2013.\n\n\nBovik, A., Clark, M., and Geisler, W. Multichannel texture\nanalysis using localized spatial filters. _IEEE Transactions_\n_on Pattern Analysis and Machine Intelligence_, 12(1):55–\n73, 1990.\n\n\n\n12\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\nMethod Outex13 CUReT USPtex MBT Acc #\nOpponent-Gabor ~~_[†]_~~ 93.5 95.8 99.1 97.6 96.5 _−_\nLPQ _[†][,i]_ 80.1 91.7 90.4 95.7 89.5 _−_\nCLBP _[†][,i]_ 89.6 91.8 97.4 98.2 94.3 _−_\nCNTD _[†][,i]_ 92.3 91.9 97.9 98.5 95.2 _−_\nMCND _[†]_ 95.4 97.0 99.0 97.1 97.1 _−_\n\nSSN 96.8 98.6 99.5 99.0 98.5 648\n\nSSR 96.7 98.6 99.3 98.2 98.2 630\n\nSST 96.8 95.6 _[∗]_ 99.1 99.8 97.8 756\n\nVCTex 96.0 96.0 _[∗]_ 99.6 99.1 97.7 648\n\n**Mixer (Ours)** **97.8** **99.5** **99.7** 99.7 **99.2** 600\n\n\n\n_Table 2._ Accuracy comparison with literature\nmethods. Approaches with a _†_ symbol were\nsourced from (Scabini et al., 2020), and those\nwith symbol _[i]_ indicate an integrative approach.\nThe results with the symbol _∗_ were calculated.\nThe remaining approaches’ results were obtained\nfrom their original papers. Acc refers to the average accuracy among the benchmark datasets.\n\n\n\nCalvetti, D., Morigi, S., Reichel, L., and Sgallari, F.\nTikhonov regularization and the l-curve for large discrete ill-posed problems. _Journal of Computational and_\n_Applied Mathematics_, 123(1-2):423–446, 2000.\n\n\nChen, Z., Li, F., Quan, Y., Xu, Y., and Ji, H. Deep texture recognition via exploiting cross-layer statistical selfsimilarity. In _Proceedings of the IEEE/CVF Conference_\n_on Computer Vision and Pattern Recognition_, pp. 5231–\n5240, 2021.\n\n\nChen, Z., Quan, Y., Xu, R., Jin, L., and Xu, Y. Enhancing\ntexture representation with deep tracing pattern encoding.\n_Pattern Recognition_, 146:109959, 2024.\n\n\nCimpoi, M., Maji, S., and Vedaldi, A. Deep filter banks for\ntexture recognition and segmentation. In _Proceedings of_\n_the IEEE Conference on Computer Vision and Pattern_\n_Recognition_, pp. 3828–3836, 2015.\n\n\nCsurka, G., Dance, C., Fan, L., Willamowski, J., and Bray,\nC. Visual categorization with bags of keypoints. In\n_Workshop on Statistical Learning in Computer Vision,_\n_ECCV_, volume 1, pp. 1–2. Prague, 2004.\n\n\nDana, K. J., Van Ginneken, B., Nayar, S. K., and Koenderink, J. J. Reflectance and texture of real-world surfaces. _ACM Transactions On Graphics (TOG)_, 18(1):\n1–34, 1999.\n\n\nDaugman, J. G. Uncertainty relation for resolution in\nspace, spatial frequency, and orientation optimized by\ntwo-dimensional visual cortical filters. _Journal of the_\n_Optical Society of America A_, 2(7):1160–1169, 1985.\n\n\nFares, R. T. and Ribas, L. C. A new approach to learn spatiospectral texture representation with randomized networks:\nApplication to brazilian plant species identification. In\n_International Conference on Engineering Applications of_\n_Neural Networks_, pp. 435–449. Springer, 2024.\n\n\nFares, R. T. and Ribas, L. C. Volumetric color-texture representation for colorectal polyp classification in histopathology images. In _20th International Conference on Com-_\n_puter Vision Theory and Applications_, pp. 210–221, 2025.\n\n\n\nFlorindo, J. B. and Laureano, E. E. Boff: A bag of fuzzy\ndeep features for texture recognition. _Expert Systems with_\n_Applications_, 219:119627, 2023.\n\n\nFujiwara, K. and Hashimoto, T. Neural implicit embedding\nfor point cloud analysis. In _Proceedings of the IEEE/CVF_\n_Conference on Computer Vision and Pattern Recognition_,\npp. 11734–11743, 2020.\n\n\nGuo, Z., Zhang, L., and Zhang, D. A completed modeling\nof local binary pattern operator for texture classification.\n_IEEE Transactions on Image Processing_, 19(6):1657–\n1663, 2010.\n\n\nHaralick, R. M., Shanmugam, K., and Dinstein, I. Textural features for image classification. _IEEE Transactions_\n_on Systems, Man, and Cybernetics_, SMC-3(6):610–621,\n1973.\n\n\nHuang, G.-B., Zhu, Q.-Y., and Siew, C.-K. Extreme learning\nmachine: theory and applications. _Neurocomputing_, 70\n(1-3):489–501, 2006.\n\n\nJain, A. and Healey, G. A multiscale representation including opponent color features for texture recognition. _IEEE_\n_Transactions on Image Processing_, 7(1):124–128, 1998.\n\n\nJoanes, D. N. and Gill, C. A. Comparing measures of sample skewness and kurtosis. _Journal of the Royal Statisti-_\n_cal Society: Series D (The Statistician)_, 47(1):183–189,\n1998.\n\n\nJones, J. P. and Palmer, L. A. An evaluation of the twodimensional gabor filter model of simple receptive fields\nin cat striate cortex. _Journal of Neurophysiology_, 58(6):\n1233–1258, 1987.\n\n\nJulesz, B. Textons, the elements of texture perception, and\ntheir interactions. _Nature_, 290(5802):91–97, 1981.\n\n\nJunior, J. J. d. M. S. and Backes, A. R. Elm based signature\nfor texture classification. _Pattern Recognition_, 51:395–\n401, 2016.\n\n\n\n13\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\nJunior, J. J. d. M. S., Backes, A. R., and Bruno, O. M.\nRandomized neural network based descriptors for shape\nclassification. _Neurocomputing_, 312:201–209, 2018.\n\n\nLeung, T. and Malik, J. Representing and recognizing the\nvisual appearance of materials using three-dimensional\ntextons. _International Journal of Computer Vision_, 43(1):\n29–44, 2001.\n\n\nLiu, L., Chen, J., Fieguth, P., Zhao, G., Chellappa, R., and\nPietikainen, M. From bow to cnn: Two decades of texture ¨\nrepresentation for texture classification. _International_\n_Journal of Computer Vision_, 127(1):74–109, 2019.\n\n\nManjunath, B. and Ma, W. Texture features for browsing\nand retrieval of image data. _IEEE Transactions on Pattern_\n_Analysis and Machine Intelligence_, 18(8):837–842, 1996.\n\n\nOiticica, P. R., Angelim, M. K., Soares, J. C., Soares, A. C.,\nProen c¸ a-Modena, J. L., Bruno, O. M., and Oliveira Jr, ´\nO. N. Using machine learning and optical microscopy\nimage analysis of immunosensors made on plasmonic\nsubstrates: Application to detect the sars-cov-2 virus.\n_ACS Sensors_, 10(2):1407–1418, 2025.\n\n\nOjala, T., Pietikainen, M., and Harwood, D. A comparative ¨\nstudy of texture measures with classification based on\nfeatured distributions. _Pattern Recognition_, 29(1):51–59,\n1996.\n\n\nOjala, T., Maenpaa, T., Pietikainen, M., Viertola, J., Kyllonen, J., and Huovinen, S. Outex-new framework for\nempirical evaluation of texture analysis algorithms. In _In-_\n_ternational Conference on Pattern Recognition_, volume 1,\npp. 701–706. IEEE, 2002a.\n\n\nOjala, T., Pietikainen, M., and Maenpaa, T. Multiresolution\ngray-scale and rotation invariant texture classification\nwith local binary patterns. _IEEE Transactions on Pat-_\n_tern Analysis and Machine Intelligence_, 24(7):971–987,\n2002b.\n\n\nOjansivu, V. and Heikkila, J. Blur insensitive texture classi- ¨\nfication using local phase quantization. In _International_\n_Conference on Image and Signal Processing_, pp. 236–243.\nSpringer, 2008.\n\n\nPao, Y.-H. and Takefuji, Y. Functional-link net computing:\ntheory, system architecture, and functionalities. _Com-_\n_puter_, 25(5):76–79, 1992. doi: 10.1109/2.144401.\n\n\nPao, Y.-H., Park, G.-H., and Sobajic, D. J. Learning and generalization characteristics of the random vector functional\nlink net. _Neurocomputing_, 6(2):163–180, 1994.\n\n\nPenrose, R. A generalized inverse for matrices. _Mathemati-_\n_cal Proceedings of the Cambridge Philosophical Society_,\n51(3):406–413, 1955.\n\n\n\nRibas, L. C. and Bruno, O. M. Learning a complex network\nrepresentation for shape classification. _Pattern Recogni-_\n_tion_, 154:110566, 2024.\n\n\nRibas, L. C., Junior, J. J. d. M. S., Scabini, L. F., and Bruno,\nO. M. Fusion of complex networks and randomized\nneural networks for texture analysis. _Pattern Recognition_,\n103:107189, 2020.\n\n\nRibas, L. C., Scabini, L. F., Condori, R. H., and Bruno, O. M.\nColor-texture classification based on spatio-spectral complex network representations. _Physica A: Statistical Me-_\n_chanics and its Applications_, 635:129518, 2024.\n\n\nScabini, L., Zielinski, K. M., Ribas, L. C., Gon c¸ alves, W. N.,\nDe Baets, B., and Bruno, O. M. Radam: Texture recognition through randomized aggregated encoding of deep\nactivation maps. _Pattern Recognition_, 143:109802, 2023.\n\n\nScabini, L. F., Condori, R. H., Gon c¸ alves, W. N., and Bruno,\nO. M. Multilayer complex network descriptors for color–\ntexture characterization. _Information Sciences_, 491:30–\n47, 2019.\n\n\nScabini, L. F., Ribas, L. C., and Bruno, O. M. Spatiospectral networks for color-texture analysis. _Information_\n_Sciences_, 515:64–79, 2020.\n\n\nSchmidt, W. F., Kraaijveld, M. A., Duin, R. P., et al. Feed\nforward neural networks with random weights. In _In-_\n_ternational Conference on Pattern Recognition_, pp. 1–4.\nIEEE Computer Society Press, 1992.\n\n\nSharan, L., Liu, C., Rosenholtz, R., and Adelson, E. H.\nRecognizing materials using perceptually inspired features. _International Journal of Computer Vision_, 103(3):\n348–371, 2013.\n\n\nSu, Z., Zhang, J., Wang, L., Zhang, H., Liu, Z., Pietikainen, ¨\nM., and Liu, L. Lightweight pixel difference networks\nfor efficient visual representation learning. _IEEE Trans-_\n_actions on Pattern Analysis and Machine Intelligence_, 45\n(12):14956–14974, 2023.\n\n\nWang, T. and Isola, P. Understanding contrastive representation learning through alignment and uniformity on the\nhypersphere. In _International Conference on Machine_\n_Learning_, pp. 9929–9939. PMLR, 2020.\n\n\nXue, J., Zhang, H., and Dana, K. Deep texture manifold for\nground terrain recognition. In _Proceedings of the IEEE_\n_Conference on Computer Vision and Pattern Recognition_,\npp. 558–567, 2018.\n\n\nZhai, W., Cao, Y., Zha, Z.-J., Xie, H., and Wu, F. Deep\nstructure-revealed network for texture recognition. In\n_Proceedings of the IEEE/CVF Conference on Computer_\n_Vision and Pattern Recognition_, pp. 11010–11019, 2020.\n\n\n\n14\n\n\n**MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\nZhang, H., Xue, J., and Dana, K. Deep ten: Texture encoding network. In _Proceedings of the IEEE Conference on_\n_Computer Vision and Pattern Recognition_, pp. 708–717,\n2017.\n\n\nZhang, J., Zhang, H., Vasudevan, R., and Johnson-Roberson,\nM. Hyperspherical embedding for point cloud completion.\nIn _Proceedings of the IEEE/CVF Conference on Com-_\n_puter Vision and Pattern Recognition_, pp. 5323–5332,\n2023.\n\n\n15\n\n\n**Supplementary Material for MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n**A. Local Pattern Extractor Module**\n\n\n\n\n\nIn this section, we explore in greater depth some nuances of\nthe proposed Local Pattern Extractor (LPE) module, whose\nresponsibility is to extract the local intensity patterns present\nin each input image channel, and subsequently use this valuable information as input to the remaining of the network.\n\n\n**Padding** . To begin with, as expressed in the main text, this\nprocess starts by centralizing _J × J_ patches at every pixel\nposition for a given channel of dimensions _H × W_ of the\ninput image. Nevertheless, as can be observed if no action\nwere taken in relation to the channel’s original dimensions,\nit would not be possible to center the patches next to or at\nthe image margin.\n\n\nIn this regard, to enable us to centralize the patches at every\npixel position of the image channel, a padding process is\nperformed. Thus, if the patch side is _J_, being _J_ an odd\nnumber, then a padding size of _[J][−]_ 2 [1] is applied at every\n\nchannel side. This process is depicted in Figure A.1. As a\nresult, after the padding process the original image channel\ndimensions _H × W_ change to ( _H_ + _J −_ 1) _×_ ( _W_ + _J −_ 1) .\n\n\n_Figure A.1._ Illustration of the padding process of an arbitrary matrix. In this present study, this arbitrary matrix stores the pixel\nintensity values of an image channel.\n\n\nParticularly, the padding is performed employing the replication mode, where the channel’s edge values are replicated. This process of replication is depicted in Figure\nA.2. Furthermore, as expected, since in the concrete image example _H_ = _W_ = 3, and the patch side size\nis _J_ = 3, the dimension after the padding process is\n( _H_ + _J −_ 1) = (3 + 3 _−_ 1) = 5 (which is equal for the\nwidth dimension), matching the dimensions of the matrix of\nthe right side of Figure A.2.\n\n\nTherefore, mathematically, the exhibited process in Figure\nA.2 clearly represents the input and output of the function\n\n\nPad _J_ : R _[C][×][H][×][W]_ _→_ R _[C][×]_ [(] _[H]_ [+] _[J][−]_ [1)] _[×]_ [(] _[W]_ [ +] _[J][−]_ [1)] _._ (A.1)\n\n\nIn the presented case, the input image dimension is\n**I** _∈_ R [1] _[×]_ [3] _[×]_ [3], and the output padded image dimension is\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n||||\n||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_Figure A.2._ Concrete example of the padding process using the\nreplication mode of a 3 _×_ 3 image, and assuming a patch side size\nof _J_ = 3. After the padding the dimension is 5 _×_ 5.\n\n\nPad _J_ ( **I** ) _∈_ R [1] _[×]_ [5] _[×]_ [5] . Thus, it should be noted that in the\nexemplified process the image consists of only one channel.\nHowever, for an arbitrary channel-sized image, this process is performed for each channel independently. Finally,\nfor more information on the replication mode we refer the\nreader to the PyTorch v2.6.0 official documentation [2] .\n\n\n**Patch-based Extraction** . After the image padding, multiple _J × J_ patches are centered at every pixel of the padded\nimage Pad _J_ ( **I** ), which corresponds to all pixels of the original image **I** . This is mathematically expressed by:\n\n\nLPE(Pad _J_ ( **I** )) _∈_ R _[C][×][H][×][W][ ×][J][×][J]_ _._ (A.2)\n\n\nAs expressed in the main text, this 5 -dimensional output\ntensor stores all of these _J × J_ patches centered at every\nposition for each image channel.\n\n\nIn this sense, with the intent to provide a concrete example\nof this process. Let **I** be the same image as the concrete\nexample represented in Figure A.2, and let _J_ = 3 . Hence,\nthe Figure A.3 shows the all nine possible patches of size\n3 _×_ 3 . Each of these patches represents the content of the last\ntwo dimensions of the 5 -dimensional tensor. Nevertheless,\none may visualize the overall figure as a matrix of matrices,\nwhich represents the last four dimensions of that tensor.\n\n\nTherefore, it can be clearly visualized how the LPE module\ncaptures the essential information of the raw pixels intensities information present in the input image.\n\n\n**B. Hyperspherical Random Projector Module**\n\n\n**Reshaping** . We show with a concrete example the result of\nthe reshaping process of the output of the previous module.\nLet the content in Figure A.3 be the 5 -dimensional tensor\n**L** _∈_ R [1] _[×]_ [3] _[×]_ [3] _[×]_ [3] _[×]_ [3] containing the extracted 3 _×_ 3 patches centered in every pixel of the image’s spatial dimension. Note\nthat the unitary dimension refers to the number of channels.\nFor the sake of clarity, although our concrete example deals\n\n\n2 https://docs.pytorch.org/docs/2.6/generated/torch.nn.functional.pad.html\n\n\n\n16\n\n\n**Supplementary Material for MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n(0, 0) (0, 1) (0, 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(1, 0) (1, 1) (1, 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(2, 0) (2, 1) (2, 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_Figure A.3._ Concrete example of the information captured by the\noutput of the LPE module represented by LPE(Pad( **I** )) . Each of\nthe matrix represents a 3 _×_ 3 patch obtained by centering it at the\nblue colored pixel, while also represents the content of the last two\ndimensions of the tensor. In addition, the overall figure can be seen\nas a matrix of matrices, representing the last four dimensions.\n\n\nwith only one channel, this is done for each channel independently. Thus, the matrix **X** = Reshape( **L** ) _∈_ R _[C][×][J]_ [2] _[×][HW]_\n\nresulting from the reshaping process of the 5 -dimensional\ntensor **L** is illustrated in Figure B.4.\n\n\nFrom this matrix, it can be observed that the intensities of\neach patch are along the column of the matrix **X**, while the\nrows refers to the intensities of a specific position inside\neach patch. It shall be noted that although most of the pixels\nare padding pixels, those colored in red, this is attributed\nto the fact that our initial image has a spatial dimension of\n3 _×_ 3 . In general, for larger texture images such 128 _×_ 128\nmost of the columns would be similar to the fifth column,\ni.e., without any padding pixels. To conclude, this matrix is\nused in the remaining of the random projector module.\n\n\n**Random Weight Matrix Generation** . In this part, it is\npresented how the random weight matrix used in the process\nof projecting the input data into another dimensional space\nis built. Although in our proposed approach we need a set\nof these random weight matrices, i.e., one for each image\nchannel, which would correspond to a 3-dimensional tensor,\nwe will present the steps and an algorithm for generating a\n\n\n\n_Figure B.4._ Concrete example of the matrix **X** = Reshape( **L** ) _∈_\nR [1] _[×]_ [9] _[×]_ [9] obtained after the reshaping process of the 5 -dimensional\ntensor **L** _∈_ R [1] _[×]_ [3] _[×]_ [3] _[×]_ [3] _[×]_ [3] represented in Figure A.3.\n\n\nrandom _N_ -dimensional tensor **T** _∈_ R _[n]_ [1] _[×][n]_ [2] _[×···×][n]_ _[N]_ .\n\n\nLet **v** _∈_ R _[L]_, where _L_ = [�] _[N]_ _i_ =1 _[n]_ _[i]_ [, with] _[ n]_ _[i]_ [ being the size]\nof the _i_ -th dimension of **T** . This vector will be filled using\nthe same procedure and configuration as in Junior & Backes\n(2016). In particular, the authors used a Linear Congruential\nGenerator (LCG) for generating pseudorandom numbers.\nSpecifically, utilizing this approach, the elements of the\nvector **v** satisfies the following recurrence relation:\n\n\n**v** _k_ +1 := ( _a_ **v** _k_ + _b_ ) mod _c,_ _k ∈{_ 0 _,_ 1 _, . . ., L−_ 2 _}_ (B.3)\n\n\nwhere the parameters are initialized as **v** 0 = _L_ + 1 _, a_ =\n_L_ + 2 _, b_ = _L_ + 3, and _c_ = _L_ [2] following Junior & Backes\n(2016). After filling the entire vector, a standardization\nprocess is applied upon **v**, i.e.,\n\n\n**v** _k_ = **[v]** _[k]_ _[ −]_ ~~**[v]**~~ _,_ _k ∈{_ 0 _,_ 1 _, . . ., L −_ 1 _}_ (B.4)\n\n_s_ **v**\n\n\nwhere ~~**v**~~ = _L_ 1 � _Lk_ =0 _−_ 1 **[v]** _[k]_ [, is the sample mean of] **[ v]** [, and]\n1 _L−_ 1\n_s_ [2] **v** [=] _L−_ 1 � _k_ =0 [(] **[v]** _[k]_ _[ −]_ ~~**[v]**~~ [)] [2] [ is the sample variance of] **[ v]** [.]\n\n\nIn this context, to obtain the tensor **T**, the vector **v** is reshaped to present _N_ dimensions, where the _i_ -th dimension\nhas _n_ _i_ elements. Following, we present in Algorithm 1 a\nPyTorch-like pseudocode for the previous taken steps.\n\n\nIn this context, if we let LCG( _n_ 1 _, n_ 2 _, . . ., n_ _N_ ) be the _N_ dimensional tensor representing the output of the pseudocode, then for any input texture image **I** _∈_ R _[C][×][H][×][W]_,\nthe random weight matrices used in the random projector\nmodule is given by LCG( _C, ω, J_ [2] + 1) _∈_ R _[C][×][ω][×]_ [(] _[J]_ [2] [+1)] .\n\n\n\n17\n\n\n**Supplementary Material for MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition**\n\n\n\n**Algorithm 1** Random Weight Matrix Generator\n\n\n**Input:** Each dimension size _n_ 1 _, n_ 2 _, . . ., n_ _N_\n**Output:** Random tensor **T** _∈_ R _[n]_ [1] _[×][n]_ [2] _[×···×][n]_ _[N]_\n**v** _←_ torch.zeros( [�] _[N]_ _i_ =1 _[n]_ _[i]_ [)]\n**v** 0 _←_ _L_ + 1\n_a ←_ _L_ + 2\n\n_b ←_ _L_ + 3\n_c ←_ _L_ [2]\n\n\n**for** _k ←_ 0 **to** _L −_ 2 **do**\n\n**v** _k_ +1 _←_ ( _a_ **v** _k_ + _b_ ) mod _c_\n**end for**\n**v** _←_ ( **v** _−_ torch.mean( **v** )) _/_ torch.std( **v** )\n**return** torch.reshape( **v** _,_ ( _n_ 1 _, n_ 2 _, . . ., n_ _N_ ))\n\n\n**C. Compression Module**\n\n\nIn this section, we present the set _H_ of compression functions used to summarize the weights of the learned decoders to assemble the texture representation. Specifically,\nthis set consists mostly of statistical measure functions\n_h_ 1 _, h_ 2 _, . . ., h_ _|H|_ used to compress the column vectors of\nthe resulting matrix **F** _∈_ R _[|S|][J]_ [2] _[×][HW]_ from the vertical concatenation of the learned decoders’ weights present in the\nset _S_ = _S_ _D_ _∪S_ _M_, where _S_ _D_ and _S_ _M_ are the sets containing the learned decoders’s weights from the D IRECT and\nM IXED branches, respectively.\n\n\n_Figure C.5._ Illustration of a compression function _h_ : _A ⊂_ R _[n]_ _→_\nR . The compression function is responsible for compressing (or\nsummarizing) a _n_ -dimensional real vector **x** _∈_ _A_ in a real number\n_h_ ( **x** ). The complete definition is presented in the main text.\n\n\nIn this context, as presented in the main text, four statistical\nmeasures are employed: mean, standard deviation, skewness, and excess kurtosis. The mean gives a central tendency\nof the sample, while the standard deviation is a dispersion\nmeasure. Following this, the skewness and kurtosis are measures about the shape of the distribution. In this sense, these\nmeasures complement each other, and are used to create a\n\n\n\nmore robust texture representation. Next, their formulas are\npresented.\n\n\nLet _m_ _r_ ( **x** ) be the _n_ -th sample central moment, which following (Joanes & Gill, 1998), is defined by:\n\n\n\n_m_ _r_ ( **x** ) := [1]\n\n_N_\n\n\n\n_N_\n� ( _x_ _k_ _−_ ~~**x**~~ ) _[r]_ _,_ (C.5)\n\n\n_k_ =1\n\n\n\n1 _N_\nwhere ~~**x**~~ = _N_ � _k_ =1 _[x]_ _[k]_ [ is the sample mean of the] _[ N]_ [-]\ndimensional real vector **x** . Thus, the utilized statistical\nmeasures’ formulas are given by:\n\n\n_h_ _µ_ ( **x** ) := ~~**x**~~ (C.6)\n\n_h_ _σ_ ( **x** ) := ( _m_ 2 ( **x** )) [1] _[/]_ [2] (C.7)\n\n\n_m_ 3 ( **x** )\n_h_ _γ_ ( **x** ) := (C.8)\n( _m_ 2 ( **x** )) [3] _[/]_ [2]\n\n_m_ 4 ( **x** )\n_h_ _κ_ ( **x** ) := ( _m_ 2 ( **x** )) [2] _[−]_ [3] (C.9)\n\n\nIt can be noted that the utilized statistical measures formulas\n\nof standard deviation and kurtosis are not unbiased esti\nmators. Nevertheless, it is not necessary to have the most\naccurate estimator, the objective here is solely a mechanism\nto compress (summarize) an information present in a real\nvector to a real number.\n\n\n\n18\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03226v1",
      "title": "A fast non-reversible sampler for Bayesian finite mixture models",
      "authors": [
        "Filippo Ascolani",
        "Giacomo Zanella"
      ],
      "summary": "Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known\nthat sampling from the resulting posterior distribution can be a hard task. In\nparticular, popular reversible Markov chain Monte Carlo schemes are often slow\nto converge when the number of observations $n$ is large. In this paper we\nintroduce a novel and simple non-reversible sampling scheme for Bayesian finite\nmixture models, which is shown to drastically outperform classical samplers in\nmany scenarios of interest, especially during convergence phase and when\ncomponents in the mixture have non-negligible overlap. At the theoretical\nlevel, we show that the performance of the proposed non-reversible scheme\ncannot be worse than the standard one, in terms of asymptotic variance, by more\nthan a factor of four; and we provide a scaling limit analysis suggesting that\nthe non-reversible sampler can reduce the convergence time from O$(n^2)$ to\nO$(n)$. We also discuss why the statistical features of mixture models make\nthem an ideal case for the use of non-reversible discrete samplers.",
      "published": "2025-10-03T17:57:44Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03226v1"
    },
    "text": "# **A fast non-reversible sampler for Bayesian** **finite mixture models**\n\n#### Filippo Ascolani [∗] and Giacomo Zanella [†] October 6, 2025\n\n**Abstract**\n\n\nFinite mixtures are a cornerstone of Bayesian modelling, and it is well-known that\nsampling from the resulting posterior distribution can be a hard task. In particular,\npopular reversible Markov chain Monte Carlo schemes are often slow to converge when\nthe number of observations _n_ is large. In this paper we introduce a novel and simple\nnon-reversible sampling scheme for Bayesian finite mixture models, which is shown to\ndrastically outperform classical samplers in many scenarios of interest, especially during\nconvergence phase and when components in the mixture have non-negligible overlap.\nAt the theoretical level, we show that the performance of the proposed non-reversible\nscheme cannot be worse than the standard one, in terms of asymptotic variance, by\nmore than a factor of four; and we provide a scaling limit analysis suggesting that the\nnon-reversible sampler can reduce the convergence time from _O_ ( _n_ [2] ) to _O_ ( _n_ ). We also\ndiscuss why the statistical features of mixture models make them an ideal case for the\nuse of non-reversible discrete samplers.\n\n\n∗ Duke University, Department of Statistical Science, Durham, NC, United States (filippo.ascolani@duke.edu)\n\n  - Bocconi University, Department of Decision Sciences and BIDSA, Milan, Italy (giacomo.zanella@unibocconi.it)\nGZ acknowledges support from the European Research Council (ERC), through StG “PrSc-HDBayLe”\ngrant ID 101076564.\n\n\n### **1 Introduction**\n\n#### **1.1 Bayesian finite mixture models**\n\nLet _K ∈_ N and consider a finite mixture model (Marin et al., 2005; Fr¨uhwirthSchnatter, 2006; McLachlan et al., 2019) defined as\n\n\n\n_Y_ _i_ _|_ _**θ**_ _,_ _**w**_ [i.i.d.] _∼_\n\n\n\n_K_\n� _w_ _k_ _f_ _θ_ _k_ ( _·_ ) _i_ = 1 _, . . ., n_\n\n\n_k_ =1\n\n\n\n(1)\n\n\n\ni.i.d.\n_θ_ _k_ _∼_ _p_ 0 _k_ = 1 _, . . ., K_\n\n\n_**w**_ _∼_ Dir( _**α**_ ) _,_\n\n\n\nwhere _**θ**_ = ( _θ_ 1 _, . . ., θ_ _K_ ), _**w**_ = ( _w_ 1 _, . . ., w_ _K_ ), _**α**_ = ( _α_ 1 _, . . ., α_ _K_ ) and Dir( _**α**_ ) denotes the\nDirichlet distribution on the ( _K_ _−_ 1)-dimensional simplex ∆ _K−_ 1 _⊂_ R _[K]_ with parameters\n_**α**_ . Here _f_ _θ_ is a probability density on a space _Y_ depending on a parameter _θ ∈_ Θ _⊂_ R _[d]_,\nto which a prior distribution with density _p_ 0 is assigned. For example, one could have\n_Y_ = Θ = R _[d]_ and _f_ _θ_ ( _y_ ) = _N_ ( _y | θ,_ Σ) for some fixed Σ, where _N_ ( _y | θ,_ Σ) denotes the\ndensity of the multivariate normal with mean vector _θ_ and covariance matrix Σ at a\npoint _y_ .\nA popular strategy to perform posterior computations with model (1) (also for\nmaximum likelihood estimation, as originally noted in Dempster et al. (1977)) consists\nin augmenting the model as follows\n\n\ni.i.d.\n(2) _Y_ _i_ _| c,_ _**θ**_ _,_ _**w**_ [i.i.d.] _∼_ _f_ _θ_ _ci_ ( _y_ ) _,_ _c_ _i_ _|_ _**θ**_ _,_ _**w**_ [i.i.d.] _∼_ Cat( _**w**_ ) _,_ _**w**_ _∼_ Dir( _**α**_ ) _,_ _θ_ _k_ _∼_ _p_ 0 _,_\n\n\nwhere _c_ = ( _c_ 1 _, . . ., c_ _n_ ) _∈_ [ _K_ ] _[n]_, with [ _K_ ] = _{_ 1 _, . . ., K}_, is the set of allocation variables\nand Cat( _**w**_ ) denotes the Categorical distribution with probability weights _**w**_ . Given a\nsample _Y_ = ( _Y_ 1 _, . . ., Y_ _n_ ), the joint posterior distribution of ( _c,_ _**θ**_ _,_ _**w**_ ) then reads\n\n\n\n�\n\n\n\nDir( _**w**_ _|_ _**α**_ )\n\n\n\n_K_\n� _p_ 0 ( _θ_ _k_ )\n\n\n_k_ =1\n\n\n\n_n_\n�\n� _i_ =1\n\n\n\n_K_\n\n_w_ _[n]_ _[k]_ [(] _[c]_ [)+] _[α]_ _[k]_ _[−]_ [1]\n\n� _k_\n� _k_ =1\n\n\n\n_K_\n�\n� _k_ =1\n\n\n\n� _w_ _c_ _i_ _f_ _θ_ _ci_ ( _Y_ _i_ )\n\n\n_i_ =1\n\n\n\n�\n\n\n\n(3)\n\n\n\n_π_ ( _c,_ _**θ**_ _,_ _**w**_ ) _∝_\n\n\n_∝_\n\n\n\n_K_\n�\n� _k_ =1\n\n\n\n� _f_ _θ_ _k_ ( _Y_ _i_ ) _p_ 0 ( _θ_ _k_ ) _,_\n\n_i_ : _c_ _i_ = _k_\n\n\n\nwhere _n_ _k_ ( _c_ ) = [�] _[n]_ _i_ =1 [1] [(] _[c]_ _[i]_ [ =] _[ k]_ [) and] [ 1] [ denotes the indicator function. In particular, it is]\npossible to integrate out ( _**θ**_ _,_ _**w**_ ) from (3) to obtain the marginal posterior distribution\nof _c_ given by\n\n\n\n_K_\n�\n� _k_ =1\n\n\n\n� _f_ _θ_ _k_ ( _Y_ _i_ ) _p_ 0 ( _θ_ _k_ ) d _θ_ _k_ _,_\n\n_i_ : _c_ _i_ = _k_\n\n\n\n(4) _π_ ( _c_ ) _∝_\n\n\n\n_K_\n�\n� _k_ =1\n\n\n\n� Γ ( _α_ _k_ + _n_ _k_ ( _c_ ))\n\n\n_k_ =1\n\n\n\n_k_ =1\n\n\n\n� Θ\n\n\n\n�\n\n\n\n2\n\n\nfrom which we deduce the so-called full-conditional distribution of _c_ _i_\n\n\n(5) _π_ ( _c_ _i_ = _k | c_ _−i_ ) _∝_ [ _α_ _k_ + _n_ _k_ ( _c_ _−i_ )] _p_ ( _Y_ _i_ _| Y_ _−i_ _, c_ _−i_ _, c_ _i_ = _k_ ) _k ∈_ [ _K_ ]\n\n\nwhere _c_ _−i_ = ( _c_ 1 _, . . ., c_ _i−_ 1 _, c_ _i_ +1 _, . . ., c_ _n_ ), _Y_ _−i_ = ( _Y_ 1 _, . . ., Y_ _i−_ 1 _, Y_ _i_ +1 _, . . ., Y_ _n_ ) and\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n_p_ ( _Y_ _i_ _| Y_ _−i_ _, c_ _−i_ _, c_ _i_ = _k_ ) = _f_ _θ_ ( _Y_ _i_ )\n� Θ _̸_\n\n\n_̸_\n\n\n\n� _j_ = _̸_ _i_ : _c_ _j_ = _k_ _[f]_ _[θ]_ [(] _[Y]_ _[j]_ [)] _[p]_ [0] [(] _[θ]_ [)] [d] _[θ]_\n~~�~~ Θ ~~�~~ _j_ = _̸_ _i_ : _c_ _j_ = _k_ _[f]_ _[θ]_ _[′]_ [(] _[Y]_ _[j]_ [)] _[p]_ [0] [(] _[θ]_ _[′]_ [) d] _[θ]_ _[′]_\n\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\nis the predictive distribution of observation _Y_ _i_ given _Y_ _−i_ and the allocation variables.\nIf _p_ 0 is conjugate with respect to the density _f_ _θ_, then _p_ ( _Y_ _i_ _| Y_ _−i_ _, c_ _−i_ _, c_ _i_ = _k_ ) and thus\n_π_ ( _c_ _i_ = _k | c_ _−i_ ) are available in closed form. For example, if _f_ _θ_ ( _y_ ) = _N_ ( _y | θ,_ Σ) and\n_p_ 0 ( _θ_ ) = _N_ ( _θ | θ_ 0 _,_ Σ 0 ) it holds that _p_ ( _Y_ _i_ _| Y_ _−i_ _, c_ _−i_ _, c_ _i_ = _k_ ) = _N_ ( _Y_ _i_ _|_ ¯ _µ,_ Σ), where [¯]\n\n\n¯\n¯Σ = Σ+�Σ _[−]_ 0 [1] + _n_ _k_ ( _c_ _−i_ )Σ _[−]_ [1] [�] _[−]_ [1] _,_ _µ_ = �Σ _[−]_ 0 [1] + _n_ _k_ ( _c_ _−i_ )Σ _[−]_ [1] [�] _[−]_ [1] [ �] Σ _[−]_ 0 [1] _[θ]_ [0] [ +] _[ n]_ _[k]_ [(] _[c]_ _[−][i]_ [)Σ] _[−]_ [1] [ ¯] _[Y]_ _[k,][−][i]_ �\n\n\nand _Y_ [¯] _k,−i_ = _n_ _[−]_ _k_ [1] [(] _[c]_ _[−][i]_ [)][ �] _j_ = _̸_ _i_ : _c_ _j_ = _k_ _[Y]_ _[j]_ [. Analogous expressions are available for likelihoods]\nin the exponential family, see e.g. Robert (2007, Sec.3.3) for details.\n\n#### **1.2 The Marginal Gibbs (MG) sampler**\n\n\nMost popular algorithms for finite mixture models are based on the augmentation\nin (2), see e.g. Diebolt and Robert (1994). Here we consider the random-scan [1] Gibbs\nsampler which iterates updates from _π_ ( _c_ _i_ _| c_ _−i_ ) for randomly chosen _i ∈_ [ _n_ ]. Its Markov\nkernel _P_ MG is defined as\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n_P_ MG ( _c, c_ _[′]_ ) = [1]\n\n_n_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n_n_\n� _P_ MG _,i_ ( _c, c_ _[′]_ ) _,_ _c, c_ _[′]_ _∈_ [ _K_ _[n]_ ]\n\n\n_i_ =1\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\nwith _P_ MG _,i_ ( _c, c_ _[′]_ ) = _δ_ _c_ _−i_ ( _c_ _[′]_ _−i_ [)] _[π]_ [(] _[c]_ _[i]_ _[ |][ c]_ _[−][i]_ [). The associated pseudocode is given in Algo-]\nrithm 1. We refer to _P_ MG as _marginal_ sampler, since it targets the marginal posterior\ndistribution of _c_ defined in (4). Once a sample from _π_ ( _c_ ) is available, draws from\n_π_ ( _c,_ _**θ**_ _,_ _**w**_ ) can be obtained by sampling from _π_ ( _**θ**_ _,_ _**w**_ _| c_ ), so that Algorithm 1 can be\nused to perform full posterior inference on _π_ ( _c,_ _**θ**_ _,_ _**w**_ ).\nBeing an irreducible and aperiodic Markov kernel on a finite space, _P_ MG is uniformly\nergodic for every fixed _n_, see e.g. Levin and Peres (2017, Theorem 4.9) and Roberts\nand Rosenthal (2004, Sec.3.3) for discussion about uniform ergodicity. However, as we\nwill see later on, its rate of convergence tends to deteriorate quickly as _n_ increases.\n\n\n1 Here we consider the random-scan strategy since it simplifies some of the proofs and comparisons below.\nWe expect the behaviour of _P_ MG _[n]_ [, where] _[ P]_ _[ k]_ [ =] _[ P . . . P]_ [ denotes the] _[ k]_ [-th power of a Markov kernel] _[ P]_ [, to be]\nroughly comparable to the one of the deterministic-scan version (which updates _c_ _i_ for _i_ = 1 _, . . ., n_ sequentially\nat each iteration) in most cases of interest, and thus stick to the random-scan for simplicity.\n\n\n3\n\n\n**Algorithm 1** (Marginal sampler _P_ MG )\n\nInitialize _c_ [(0)] _∈_ [ _K_ ] _[n]_ .\n**for** _t ≥_ 1 **do**\n\nSample _i ∼_ Unif ( _{_ 1 _, . . ., n}_ ), where Unif denotes the uniform distribution.\nSample _c_ [(] _i_ _[t]_ [)] _∼_ _π_ ( _c_ _i_ _| c_ [(] _−_ _[t][−]_ _i_ [1)] ), with _π_ ( _c_ _i_ = _k | c_ _−i_ ) as in (5), and set _c_ [(] _−_ _[t]_ [)] _i_ [=] _[ c]_ _−_ [(] _[t][−]_ _i_ [1)] .\n**end for**\n\n\nA popular alternative to the marginal sampler is the so-called _conditional_ sampler\nintroduced in Diebolt and Robert (1994), which directly targets _π_ ( _c,_ _**θ**_ _,_ _**w**_ ) defined in\n(3) alternating updates of ( _**θ**_ _,_ _**w**_ ) _| c_ and _c |_ ( _**θ**_ _,_ _**w**_ ). We postpone the discussion of\nthis algorithm to Section 3.2, since the latter is always dominated by _P_ MG in terms of\nmixing speed (see e.g. Proposition 3.3).\n\n#### **1.3 Illustrative example**\n\n\nIt is well-known that _P_ MG can be slow to converge when _n_ is large (Celeux et al.,\n2000; Lee et al., 2009). As a first illustrative example, we take model (1) with _K_ = 2,\n_f_ _θ_ ( _y_ ) = _N_ ( _y | θ,_ 1), _p_ 0 ( _θ_ ) = _N_ ( _θ |_ 0 _,_ 1), _**α**_ = (0 _._ 5 _,_ 0 _._ 5), and we consider the posterior\ndistribution given ( _Y_ 1 _, . . ., Y_ _n_ ) generated as\n\n\ni.i.d.\n(6) _Y_ _i_ _∼_ 0 _._ 9 _N_ (0 _._ 9 _,_ 1) + 0 _._ 1 _N_ ( _−_ 0 _._ 9 _,_ 1) _,_ _i_ = 1 _, . . ., n_\n\n\nwith _n_ = 2000. This is a relatively simple one-dimensional problem, with data generated from two components with a reasonable degree of separation between them (the\ntwo means are almost two standard deviations away from each other).\nWe use _P_ MG to sample from the resulting posterior _π_ ( _c_ ), leading to a Markov chain\n_{c_ [(] _[t]_ [)] _}_ _t_ =0 _,_ 1 _,_ 2 _,..._ on [ _K_ ] _[n]_ . The left panel of Figure 1 displays 100 independent traceplots of\nthe size of the largest cluster in _{c_ [(] _[t]_ [)] _}_ _t_, with all chains independently initialized by sampling _c_ [(0)] uniformly from [ _K_ ] _[n]_ . Most runs are still far from 0 _._ 9 (value around which we\nexpect the posterior to concentrate) after 150 _×n_ iterations. Indeed trajectories exhibit\na typical random-walk behaviour, with slow convergence to stationarity. The center\npanel instead shows the traceplots generated by the same number of runs and iterations\nof _P_ NR, the non-reversible scheme we propose in Section 2.2 (see Algorithm 6 therein\nfor pseudocode and full details). The chain appears to reach the high-probability region and forget the starting configuration much faster. This is also clear from the right\npanel, which displays empirical estimates of the marginal distributions of the Markov\nchains induced by _P_ MG and _P_ NR over time.\n\n\n4\n\n\n**Marginal**\n\n\n0 50 100 150\n\n\n\n**Non reversible**\n\n\n0 50 100 150\n\n\n\n**Marginal distribution of the chains**\n\n\n1 20 40 60 80 100 120 140\n\n\n\nFigure 1: Left and center: traceplots of 100 independent runs of the size of the largest cluster for\n150 iterations (after a thinning of size _n_, i.e. after 150 _× n_ total updates) of _P_ MG (left)\nand _P_ NR (center) in Algorithm 6. Right: empirical estimates of the marginal distribution\nat every 10 _× n_ iterations for _P_ MG (gray) and _P_ NR (black). The model is the one in (1)\nwith _K_ = 2, _f_ _θ_ ( _y_ ) = _N_ ( _y | θ,_ 1), _p_ 0 ( _θ_ ) = _N_ ( _θ |_ 0 _,_ 1) and _**α**_ = (0 _._ 5 _,_ 0 _._ 5), while the data\nare generated as in (6). Initial configurations _c_ [(0)] are sampled uniformly from [ _K_ ] _[n]_ .\n\n#### **1.4 Lifted samplers for discrete spaces**\n\n\nOur proposed sampler is inspired by classical non-reversible MCMC constructions (Diaconis et al., 2000; Fearnhead et al., 2018), which loosely speaking force the algorithm\nto persistently move in one direction as much as possible. To illustrate the idea, consider an arbitrary probability distribution _π_ on a countable space _C_ and an augmented\ndistribution\n\n\n\n_π_ ˜( _c, v_ ) = _[π]_ [(] _[c]_ [)] _c ∈C, v ∈{−_ 1 _,_ +1 _},_\n\n2 _[,]_\n\n\n\non the space _X_ = _C × {−_ 1 _,_ +1 _}_, so that _π_ is the marginal distribution of ˜ _π_ over _C_ .\nGiven two Markov kernels _{q_ +1 ( _c, ·_ ) _}_ _c∈C_ and _{q_ _−_ 1 ( _c, ·_ ) _}_ _c∈C_ on _C_, let _P_ lift be the nonreversible ˜ _π_ -invariant Markov kernel defined in Algorithm 2. The kernels are usually\n\n\n**Algorithm 2** Generating a sample ( _c_ _[′]_ _, v_ _[′]_ ) _∼_ _P_ lift (( _c, v_ ) _, ·_ )\n\nGenerate ˜ _c ∼_ _q_ _v_ ( _c, ·_ ).\nSet ( _c_ _[′]_ _, v_ _[′]_ ) = (˜ _c, v_ ) with probability\n\n\n\nmin 1 _,_ _[π]_ [(] _[c]_ [˜][)] _[q]_ _[−][v]_ [(] _[c]_ [˜] _[,][ c]_ [)]\n� _π_ ( _c_ ) _q_ _v_ ( _c,_ ˜ _c_ )\n\n\n\n_._\n�\n\n\n\nOtherwise set ( _c_ _[′]_ _, v_ _[′]_ ) = ( _c, −v_ ).\n\n\nchosen so that _q_ _v_ ( _c, ·_ ) and _q_ _−v_ ( _c, ·_ ) have disjoint support and the variable _v ∈{−_ 1 _,_ +1 _}_\nencodes a direction (or velocity) along which the chain is exploring the space: such\ndirection is reversed only when a proposal is rejected (see Algorithm 2). As a simple\n\n\n5\n\n\nexample, take _C_ = N and _q_ _v_ ( _c, ·_ ) = _δ_ _c_ + _v_ ( _·_ ), so that _v_ = +1 implies that the chain is\nmoving towards increasing values and viceversa with _v_ = _−_ 1. Within this perspective\n_v_ can be seen as a “memory bank” which keeps track of the past history of the chain\nand introduces momentum. The kernel _P_ lift is often referred to as a _lifted_ version of\nthe (reversible) Metropolis-Hastings kernel with proposal distribution _q_ = 0 _._ 5 _q_ +1 +\n0 _._ 5 _q_ _−_ 1 and target distribution _π_ . Lifted kernels can mix significantly faster than their\nreversible counterparts (Diaconis et al., 2000) and are in general at least as efficient as\nthe original method under mild assumptions (Bierkens, 2016; Andrieu and Livingstone,\n2021; Gagnon and Maire, 2024b). However, whether or not lifting techniques achieve\na notable speed-up depends on the features of _π_ and the choice of _q_ _v_ . For example, if\nproposed moves are often rejected, then the direction _v_ will be reversed frequently and\nthe chain will exhibit an almost reversible behaviour; while if the sampler manages to\nmake long ‘excursions’ (i.e. consecutive moves without flipping direction) one expects\nto observe significant gains obtained by lifting.\n\n\n**Non-reversible samplers for mixture models** General techniques to construct non-reversible samplers for discrete spaces have been proposed in the literature,\nsee e.g. Gagnon and Maire (2024a, Sec.3) for constructions on partially-ordered discrete\nspaces and Power and Goldman (2019) for discrete spaces with algebraic structures.\nWe are, however, not aware of successful applications of these methodologies to mixture models. Part of the reason could be that, in order to build a lifted counter-part of\n_P_ MG for mixture models, one would need to define some notion of direction or partial\nordering on [ _K_ ] _[n]_, or more generally on the space of partitions and it is not obvious\nhow to do so in a way that is computationally efficient and results in long excursions\nwith persistence in direction (thus leading to substantial speed-ups). For example, one\ncould directly rely on the cartesian product structure of [ _K_ ] _[n]_ and attach a velocity\ncomponent to each coordinate, applying for example the discrete Hamiltonian Monte\nCarlo algorithm of Nishimura et al. (2020): this however would not pair well with the\ngeometry of posterior distributions _π_ ( _c_ ) arising in mixture models and likely result in\nshort excursions and little speed-up.\nTo tackle this issue, we take a different perspective on [ _K_ ] _[n]_, moving from the kernel\n_P_ MG, which is a mixture over data-points (i.e. over _i ∈_ [ _n_ ]), to a kernel _P_ R which is a\nmixture over pairs of clusters (see Section 2.1 for definition). This allows us to derive\nan effective non-reversible sampler _P_ NR targeting _π_ ( _c_ ), built as a mixture of lifted\nsamplers associated to pairs of clusters (see Section 2.2 for definition). Note that,\nwhile we designed our sampler to be effective for posterior distributions of mixture\nmodels, the proposed scheme can in principle be used with any distribution _π_ on [ _K_ ] _[n]_ .\n\n#### **1.5 Related literature**\n\n\n**Bayesian mixture models** Bayesian finite mixture models are a classical topic\nwhich has received a lot of attention in the last decades, see Marin et al. (2005);\n\n\n6\n\n\nFr¨uhwirth-Schnatter (2006) for some reviews. The challenges related to sampling from\nthe resulting posterior distribution have been also discussed extensively, see e.g. early\nexamples in (Diebolt and Robert, 1994; Celeux et al., 2000; Stephens, 2000; Lee et al.,\n2009; Hobert et al., 2011), and the marginal and conditional samplers we compare\nwith are arguably the most popular schemes that are routinely used to accomplish\nsuch tasks (Marin et al., 2005, Section 1.4).\n\n\n**Lifted MCMC for statistical models with discrete parameters** Nonreversible MCMC samplers have been previously designed for and applied to Bayesian\nstatistical models with discrete parameters, such as variable selection, permutationbased and graphical models; see e.g. Power and Goldman (2019); Gagnon and Maire\n(2024a); Schauer and Wien¨obst (2024) and references therein. However, posterior\ndistributions arising from such models are usually strongly concentrated and highly\nnon-smooth, limiting the length of excursions and speed-ups obtained with lifted chains.\nAs a result, one often ends up observing large gains (e.g. hundred-fold) when targeting\nuniform or prior distributions (which are usually quite flat) and more modest gains\n(e.g. two-fold) when targeting actual posterior distributions used in practice; see e.g.\nSchauer and Wien¨obst (2024, Figures 1 and 3), Power and Goldman (2019, Table 1) or\nGagnon and Maire (2024a, Figure 1) [2] . Instead, a key feature of our proposed sampler\nis that, in many cases of interest, the speed-up relative to its reversible counter-part\nremains large even in the presence of observed data (i.e. for the actual posterior). We\nargue that this is due to statistical features of mixture models that make them wellsuited to appropriately designed non-reversible samplers (such as _P_ NR ); see Section\n2.2.1 for more details.\n\n#### **1.6 Structure of the paper**\n\n\nIn Section 2 we introduce our proposed non-reversible Markov kernel _P_ NR, which targets _π_ ( _c_ ) in (4). In Section 3 we first show that, after accounting for computational\ncost, _P_ NR cannot perform worse than _P_ MG, in terms of asymptotic variance, by more\nthan a factor of four. This is done by combining some variations of classical results on\nasymptotic variances of lifted samplers with a Peskun comparison between _P_ MG and an\nauxiliary reversible kernel _P_ R . We then provide analogous results for the conditional\nsampler by showing that it is dominated by the marginal one (Section 3.2). In Section\n4 we continue the comparison between _P_ MG and _P_ NR, showing that in the prior case\nthe latter improves on the former by an order of magnitude, i.e. reducing the convergence time from _O_ ( _n_ [2] ) to _O_ ( _n_ ). This is done through a scaling limit analysis, which\n\n\n2 This is in contrast with applications of lifting techniques to discrete models arising in Statistical Physics\n(see e.g. Vucelja, 2016), which often feature a higher degree of symmetry and smoothness, thus making\nnon-reversible MCMC methods more effective; see e.g. Power and Goldman (2019, Table 1) for numerical\nexamples and Faulkner and Livingstone (2024) for a recent review.\n\n\n7\n\n\nproves that, after rescaling time by a factor of _n_ [2], the evolution of the frequencies\n_n_ _k_ ( _c_ ) evolving according to _P_ MG converges to a Wright-Fisher process (Ethier, 1976),\nwhich is a diffusion on the probability simplex. In contrast, when the chain evolves\naccording to _P_ NR, we obtain convergence to a non-singular piecewise deterministic\nMarkov process (Davis, 1984) after rescaling time by only a factor of _n_ . Section 5 discusses a variant of _P_ NR and, finally, Section 6 provides various numerical simulations,\nwhere _P_ NR is shown to significantly outperform _P_ MG in sampling from mixture models\nposterior distributions, both in low and high-dimensional cases. The Supplementary\nMaterial contains additional simulation studies, together with the proofs of all the\ntheoretical results. R code to replicate all the numerical experiments can be found at\n`[https://github.com/gzanella/NonReversible_FiniteMixtures](https://github.com/gzanella/NonReversible_FiniteMixtures)` .\n\n### **2 A non-reversible marginal sampler**\n\n#### **2.1 A reversible sampler that operates over pairs of clus-** **ters**\n\n\nLet _π_ ( _c_ ) be an arbitrary probability distribution on [ _K_ ] _[n]_, such as (4) in the context of\nfinite mixtures, and denote the set of ordered pairs in [ _K_ ], with cardinality _K_ ( _K_ _−_ 1) _/_ 2,\n\nas\n\n\n(7) _K_ = �( _k, k_ _[′]_ ) _∈_ [ _K_ ] [2] : _k < k_ _[′]_ [�] _._\n\n\nAs an intermediate step towards defining _P_ NR, we first consider a _π_ -reversible Markov\nkernel on [ _K_ ] _[n]_ defined as\n\n\n(8) _P_ R ( _c, c_ _[′]_ ) = � _p_ _c_ ( _k, k_ _[′]_ ) _P_ _k,k_ _′_ ( _c, c_ _[′]_ ) _c, c_ _[′]_ _∈_ [ _K_ ] _[n]_ _,_\n( _k,k_ _[′]_ ) _∈K_\n\n\nwhere\n\n\n(9) _p_ _c_ ( _k, k_ _[′]_ ) = _[n]_ _[k]_ [(] _[c]_ [)][ +] _[ n]_ _[k]_ _[′]_ [(] _[c]_ [)] _,_ ( _k, k_ _[′]_ ) _∈K_\n( _K −_ 1) _n_\n\n\nis a probability distribution on _K_ for each _c ∈_ [ _K_ ] _[n]_, i.e. [�] ( _k,k_ _[′]_ ) _∈K_ _[p]_ _[c]_ [(] _[k, k]_ _[′]_ [) = 1, and]\n_P_ _k,k_ _′_ is the _π_ -reversible Metropolis-Hastings (MH) kernel that proposes to move a\nuniformly drawn point from cluster _k_ to cluster _k_ _[′]_ or viceversa with probability 1 _/_ 2.\nThe resulting kernel _P_ R is defined in Algorithm 3 where, for ease of notation, for every\n_c ∈_ [ _K_ ] _[n]_, _i ∈_ [ _n_ ] and _k ∈_ [ _K_ ] we write ( _c_ _−i_ _, k_ ) _∈_ [ _K_ ] _[n]_ for the vector _c_ with the _i_ -th\nentry _c_ _i_ replaced by _k_ .\nDespite the fact that _P_ R is a mixture with weights _p_ _c_ depending on the current\nstate _c_, invariance with respect to _π_ is preserved, as proven in the next lemma. The\n\n\n8\n\n\n**Algorithm 3** Generating a sample _c_ _[′]_ _∼_ _P_ R ( _c, ·_ )\n\nSample ( _k, k_ _[′]_ ) _∼_ _p_ _c_ as in Algorithm 4.\nSet ( _k_ _−_ _, k_ + ) = ( _k, k_ _[′]_ ) with probability 1 _/_ 2 and ( _k_ _−_ _, k_ + ) = ( _k_ _[′]_ _, k_ ) otherwise\nIf _n_ _k_ _−_ ( _c_ ) = 0 set _c_ _[′]_ = _c_ .\nIf _n_ _k_ _−_ ( _c_ ) _>_ 0 sample _i ∼_ Unif ( _{i_ _[′]_ _∈_ [ _n_ ] : _c_ _i_ _′_ = _k_ _−_ _}_ ) and set _c_ _[′]_ = ( _c_ _−i_ _, k_ + ) with probability\nmin _{_ 1 _, r_ ( _c, i, k_ _−_ _, k_ + ) _}_ and _c_ _[′]_ = _c_ otherwise, where\n\n\n\n_n_ _k_ _−_ ( _c_ )\n(10) _r_ ( _c, i, k_ _−_ _, k_ + ) =\n� _n_ _k_ + ( _c_ ) + 1\n\n\n\n_π_ ( _c_ _i_ = _k_ + _| c_ _−i_ )\n� _π_ ( _c_ _i_ = _k_ _−_ _| c_ _−i_ ) _[,]_\n\n\n\nkey point is that _p_ _c_ ( _k, k_ _[′]_ ) only depends on _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ ) and _P_ _k,k_ _′_ leaves the latter\nquantity unchanged.\n\n\n**Lemma 2.1.** _The Markov kernel P_ R _defined in Algorithm 3 is π-reversible. Moreover,_\n_if π_ ( _c_ ) _>_ 0 _for every c ∈_ [ _K_ ] _[n]_ _it is also irreducible, aperiodic and uniformly ergodic._\n\n\nSampling from _p_ _c_ can be performed efficiently using Algorithm 4, where one cluster\nis selected with probability proportional to its size and the other uniformly at random\nfrom the remaining ones. Validity is proved in the next lemma.\n\n\n**Lemma 2.2.** _For each c ∈_ [ _K_ ] _[n]_ _, Algorithm 4 produces a sample_ ( _k, k_ _[′]_ ) _from the_\n_probability distribution p_ _c_ _defined in_ (9) _._\n\n\n**Algorithm 4** Sampling ( _k, k_ _[′]_ ) _∼_ _p_ _c_ defined in (9)\n\nSample _k_ 1 from _{_ 1 _, . . ., K}_ with probabilities ( _n_ 1 ( _c_ ) _/n, . . ., n_ _K_ ( _c_ ) _/n_ )\nSample _k_ 2 uniformly at random from _{_ 1 _, . . ., K}\\{k_ 1 _}_\nSet _k_ = min _{k_ 1 _, k_ 2 _}_ and _k_ _[′]_ = max _{k_ 1 _, k_ 2 _}_\n\n\n**Comparison between** _P_ MG **and** _P_ R Both _P_ MG and _P_ R can be interpreted as\nreversible Metropolis-Hastings schemes that propose single-point moves. Specifically,\n_P_ MG and _P_ R propose moving datapoint _i_ to cluster _k_ with, respectively, probabilities\n\n\n\n\n_[|][ c]_ _[−][i]_ [)]\n_a_ MG ( _i, k_ ) = _[π]_ [(] _[c]_ _[i]_ [ =] _[ k]_\n\n\n\n\n_[ k]_ _[|][ c]_ _[−][i]_ [)] and _a_ R ( _i, k_ ) = _[n]_ _[c]_ _[i]_ [(] _[c]_ [)][ +] _[ n]_ _[k]_ [(] _[c]_ [)]\n\n_n_ _n_ _c_\n\n\n\n\n[)][ +] _[ n]_ _[k]_ [(] _[c]_ [)] 1 ( _c_ _i_ _̸_ = _k_ )\n\n_n_ _c_ _i_ ( _c_ ) 2( _K −_ 1) _n_ _[,]_\n\n\n\nfor ( _i, k_ ) _∈_ [ _n_ ] _×_ [ _K_ ]. For _P_ MG the Metropolis-Hastings acceptance probability is always\none, while for _P_ R it is not. It is interesting to note that\n\n\n1 1\n_a_ R ( _i, k_ ) _≥_\n2( _K −_ 1) _n_ _[≥]_ 2( _K −_ 1) _[a]_ [MG] [(] _[i, k]_ [)] _[,]_\n\n\n9\n\n\nmeaning that the proposal probabilities of _P_ R can be at most 2( _K −_ 1) times smaller\nthan the ones of _P_ MG . This connection will help providing formal comparison results\nbetween _P_ R and _P_ MG in Section 3 (see Theorem 3.1 and Remark 3.2 for more details).\nWe postpone details on these comparisons to Section 3 and now focus on how to leverage\nthe mixture representation of _P_ R in (8) to build effective non-reversible algorithms\ntargeting _π_ ( _c_ ).\n\n\n**Cost per iteration of** _P_ MG **and** _P_ R For both _P_ MG and _P_ R the cost per iteration\nis usually dominated by the computation of the conditional distribution _π_ ( _c_ _i_ = _k | c_ _−i_ )\nin (5), which will depend on the specific combination of kernel _f_ _θ_ and prior _p_ 0 . Indeed,\nAlgorithm 1 requires in addition only to sample from a uniform distribution on a\ndiscrete set (which has a fixed cost). Similar considerations hold for Algorithm 3,\nsince sampling from _p_ _c_ with Algorithm 4 entails again only sampling from two uniform\ndistributions. Thus, we measure cost per iteration of these samplers in terms of the\nnumber of conditional distribution evaluations, which is _K_ for _P_ MG and 2 for _P_ R :\ntherefore the ratio of costs of _P_ MG versus _P_ R is _K/_ 2. The same will hold for _P_ NR in\nAlgorithm 6 below, which requires essentially the same computations of Algorithm 3.\n\n#### **2.2 The proposed non-reversible sampler**\n\n\nConsider the extended target distribution\n\n\n\n_K_ ( _K−_ 1) _/_ 2\n\n˜ 1\n\n(11) _π_ ( _c, v_ ) := _π_ ( _c_ ) _c ∈_ [ _K_ ] _[n]_ _, v_ = ( _v_ _k,k_ _′_ ) ( _k,k_ _′_ ) _∈K_ _∈{−_ 1 _,_ +1 _}_ _[K]_ [(] _[K][−]_ [1)] _[/]_ [2]\n\n� 2 �\n\n\n\n_π_ ˜( _c, v_ ) := _π_ ( _c_ ) 1\n� 2\n\n\n\n2\n\n\n\nand the ˜ _π_ -invariant Markov kernel _P_ NR defined as\n\n\n(12) _P_ NR (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = � _p_ _c_ ( _k, k_ _[′]_ ) _P_ [˜] _k,k_ _′_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) _,_\n( _k,k_ _[′]_ ) _∈K_\n\n\nwith _p_ _c_ defined as in (9) and _P_ [˜] _k,k_ _′_ being the ˜ _π_ -invariant kernel defined in Algorithm 5.\nThe kernel _P_ [˜] _k,k_ _′_ operates on the _k_ -th and _k_ _[′]_ -th clusters, and it is a lifted counter-part\nof _P_ _k,k_ _′_, with associated velocity component _v_ _k,k_ _′_ . In this construction, the velocity\nvector _v_ is _K_ ( _K −_ 1) _/_ 2 dimensional and only one of its component is actively used to\nmove _c_ at each iteration. The pseudo-code associated to _P_ NR is given in Algorithm 6.\nThe algorithm depends on a parameter _ξ ≥_ 0, which can be interpreted as the\nrefresh rate at which directions are flipped. While useful to take _ξ >_ 0 for technical\nreasons (i.e. to ensure aperiodicity), we do not expect the value of _ξ_ to have significant\nimpacts in practice provided it is set to a small value, and in the simulations we always\nset _ξ_ = 1 _/_ 2.\nThe next lemma shows that _P_ NR is a valid ˜ _π_ -invariant kernel.\n\n\n10\n\n\n**Algorithm 5** Generating a sample ( _c_ _[′]_ _, v_ _[′]_ ) _∼_ _P_ [˜] _k,k_ _′_ (( _c, v_ ) _, ·_ )\n\nWith probability _ξ/n_ flip _v_ _k,k_ _′_ to _−v_ _k,k_ _′_\nSet ( _k_ _−_ _, k_ + ) = ( _k, k_ _[′]_ ) if _v_ _k,k_ _′_ = +1, and ( _k_ _−_ _, k_ + ) = ( _k_ _[′]_ _, k_ ) if _v_ _k,k_ _′_ = _−_ 1\nIf _n_ _k_ _−_ ( _c_ ) = 0, set ( _c_ _[′]_ _, v_ _[′]_ ) = ( _c, v_ [(] _[flip]_ [)] ), with _v_ [(] _[flip]_ [)] = ( _v_ _−_ ( _k,k_ _′_ ) _, −v_ _k,k_ _′_ )\nIf _n_ _k_ _−_ ( _c_ ) _>_ 0, sample _i ∼_ Unif ( _{i_ _[′]_ _∈_ [ _n_ ] _| c_ _i_ _′_ = _k_ _−_ _}_ ) and set ( _c_ _[′]_ _, v_ _[′]_ ) = (( _c_ _−i_ _, k_ + ) _, v_ ) with\nprobability min _{_ 1 _, r_ ( _c, i, k_ _−_ _, k_ + ) _}_ and otherwise ( _c_ _[′]_ _, v_ _[′]_ ) = ( _c, v_ [(] _[flip]_ [)] ), with _r_ ( _c, i, k_ _−_ _, k_ + ) _}_\ndefined in (10)\nWith probability _ξ/n_ flip _v_ _k,k_ _[′]_ _[′]_ [ to] _[ −][v]_ _k,k_ _[′]_ _[′]_\n\n\n**Algorithm 6** One step of the non-reversible kernel ( _c_ _[′]_ _, v_ _[′]_ ) _∼_ _P_ NR (( _c, v_ ) _, ·_ )\n\nSample ( _k, k_ _[′]_ ) _∼_ _p_ _c_ as in Algorithm 4.\nSample ( _c_ _[′]_ _, v_ _[′]_ ) _∼_ _P_ [˜] _k,k_ _′_ (( _c, v_ ) _, ·_ ) as in Algorithm 5.\n\n\n**Lemma 2.3.** _For any probability distribution π on_ [ _K_ ] _[n]_ _, the Markov kernel P_ NR _defined_\n_in Algorithm 6 is_ ˜ _π-invariant, with_ ˜ _π as in_ (11) _. Moreover, if ξ >_ 0 _and π_ ( _c_ ) _>_ 0 _for_\n_every c ∈_ [ _K_ ] _[n]_ _, then P_ NR _is irreducible, aperiodic and uniformly ergodic._\n\n\n**2.2.1** **Specificities of mixture models that make** _P_ NR **work well**\n\n\nWe now discuss at an informal level some of the specificities of the posterior distribution\n_π_ ( _c_ ) arising from mixture models that make _P_ NR work well.\n\n\n**Lack of identifiability and concentration** An important statistical feature of\nmixture models is that cluster labels are in general not identifiable as _n →∞_, meaning\nthat even when _n_ is large there is non-vanishing uncertainty on the value of _c_ _i_ . In other\nwords, while the posterior distribution of _**w**_ and _**θ**_ concentrates as _n →∞_, the one of _c_\ndoes not (not even at the level of marginals, meaning that, for every fixed _i_, _π_ ( _c_ _i_ ) does\nnot converge to a point mass as _n →∞_ ); see e.g. Nguyen (2013); Guha et al. (2021)\nand references therein for asymptotic results for mixture model posteriors. Intuitively,\nlack of concentration occurs because the information about each individual _c_ _i_ does not\ngrow with _n_ (since each _c_ _i_ is associated to a single datapoint). This also tends to\nmake posteriors flatter, i.e. moving one observation from one cluster to another usually\nleads to a small change in the target distribution. By contrast, many models with\ndiscrete parameters lead to posteriors that become increasingly more concentrated and\nrough as _n →∞_, which has a major impacts on the convergence properties of MCMC\nalgorithms targeting them, including making standard MCMC converge faster (see e.g.\nYang et al., 2016; Zhou et al., 2022; Zhou and Chang, 2023) and lifting techniques less\neffective (as already discussed in Section 1.5).\n\n\n11\n\n\n**Cancellations in the acceptance ratio** For _π_ ( _c_ ) as in (4), the MH ratio _r_ ( _c, i, k_ _−_ _, k_ + )\nreads\n\n\n\n_α_ _k_ + + _n_ _k_ + ( _c_ )\n(13) _r_ ( _c, i, k_ _−_ _, k_ + ) =\n� _n_ _k_ + ( _c_ ) + 1\n\n\n\n_n_ _k_ _−_ ( _c_ )\n�� _α_ _k_ _−_ + _n_ _k_ _−_ ( _c_ ) _−_ 1\n\n\n\n_p_ ( _Y_ _i_ _| Y_ _−i_ _, c_ _−i_ _, c_ _i_ = _k_ + )\n� _p_ ( _Y_ _i_ _| Y_ _−i_ _, c_ _−i_ _, c_ _i_ = _k_ _−_ ) _[.]_\n\n\n\nInterestingly, the proposal probability almost matches the term _α_ _k_ + _n_ _k_ ( _c_ ) arising from\nthe prior. In particular, by writing\n\n\n\n_α_ _k_ + + _n_ _k_ + ( _c_ )\n� _n_ _k_ + ( _c_ ) + 1\n\n\n\n_n_ _k_ _−_ ( _c_ )\n�� _α_ _k_ _−_ + _n_ _k_ _−_ ( _c_ ) _−_ 1\n\n\n\n= 1 + _α_ _k_ + _−_ 1\n� � _n_ _k_ + ( _c_ ) + 1\n\n\n\n1 + _[α]_ _[k]_ _[−]_ _[−]_ [1]\n�� _n_ _k_ _−_ ( _c_ )\n\n\n\n_−_ 1\n\n_,_\n�\n\n\n\nwe see that the first part of (13) goes to 1 as _n_ _k_ + ( _c_ ) and _n_ _k_ _−_ ( _c_ ) increase, for every\nfixed value of _**α**_ . Notice that with _α_ _k_ = 1 for every _k_ this ratio is always equal to 1.\nThis cancellation contributes to make (13) closer to 1 and thus to make excursions of\n_P_ NR longer.\n\n\n**Flatness in the tails and behavior out of stationarity** Interestingly, also the\nratio of predictive distributions in (13) tends to get close to 1 for partitions that do not\ncorrespond to well-identified and separate clusters, meaning that mixture model posteriors _π_ ( _c_ ) become increasingly flatter in the tails. To illustrate this, consider the com\ni.i.d.\nmon situation when labels are initialized uniformly at random, i.e. _c_ [(0)] _i_ _∼_ Unif([ _K_ ]).\nIn this situations, by construction, clusters are similar to each other under _c_ [(0)], resulting in ratios of predictive distributions that are close to 1 (and converge to 1 as\n_n →∞_ ). As a consequence, the non-reversible chain will proceed almost deterministically without flipping directions until clusters start to differentiate significantly. This is\nindeed the behavior observed in the middle panel of Figure 1, as well as in Section 6.2\nand B of the Supplementary Material with different values of _K_ and likelihood kernels.\nMore generally, in mixture model contexts, non-reversibility is particularly helpful during the transient phase, where the algorithm has not yet reached the high-probability\nregion under _π_ and has to explore large flat regions of the state space [3] .\n\n\n**Overlapping components and the overfitted case** Another situation that\nmakes ratios of predictive distributions close to 1 is when the actual clusters in the data\nhave a considerable overlap. An extreme case of this situation is when the true number\nof components _K_ _[∗]_ (assuming data were actually generated by a well-specified mixture\nmodel) is strictly smaller than _K_, which amounts to assuming that a plausible upper\nbound on _K_ _[∗]_ is known and _K_ is set to such value (instead of the less plausible scenario\nwhere _K_ _[∗]_ itself is known). This is often called the overfitted case, see e.g. Rousseau\n\n\n3 This is, again, in contrast with typical Bayesian discrete models that lead to posteriors with large\n“discrete gradients” in the tails providing strong enough signal for reversible schemes to converge fast in the\nfirst part of the transient phase (Yang et al., 2016; Zhou et al., 2022; Zhou and Chang, 2023).\n\n\n12\n\n\nand Mengersen (2011) for a theoretical exploration of its asymptotic properties, and it\nis a common situation since in many context (e.g. density estimation) it is preferable\nto overshoot rather than undershoot the value of _K_ and thus practitioners often set _K_\nto some conservative, moderately large value. See Section 6.4 for more discussion on\nthe overfitted case and empirical evidence that in this setting the improvement of _P_ NR\nover the latter is particularly apparent.\n\n### **3 Asymptotic variance comparison results**\n\n\nIn this section we compare the various kernels discussed above in terms of asymptotic\nvariances. Among other results we show that, after accounting for computational cost,\n_P_ NR cannot be worse than _P_ MG by more than a factor of 4. Given a Markov chain _{X_ _t_ _}_ _t_\nwith a _π_ -invariant Markov kernel _P_ started in stationarity, the asymptotic variance of\nthe associated MCMC estimator is given by\n\n\n\n�\n\n\n\n= Var _π_ ( _g_ ) + 2\n\n\n\n_∞_\n� Cov ( _g_ ( _X_ 0 ) _, g_ ( _X_ _t_ )) _,_\n\n\n_t_ =1\n\n\n\nVar( _g, P_ ) = lim\n_T_ _→∞_ _[T]_ [Var]\n\n\n\n1\n\n_T_\n�\n\n\n\n_T_\n�\n\n\n\n� _g_ ( _X_ _t_ )\n\n\n_t_ =1\n\n\n\nfor every function _g_ such that Var _π_ ( _g_ ) is well-defined.\n\n#### **3.1 Ordering of reversible and non-reversible schemes**\n\n\nThe next theorem provides ordering results for the asymptotic variances of _P_ MG, _P_ R\nand _P_ NR . Technically speaking these kernels are not directly comparable, since _P_ MG\nand _P_ R are defined on [ _K_ ] _[n]_ while _P_ NR is defined on [ _K_ ] _[n]_ _×{−_ 1 _,_ +1 _}_ _[K]_ [(] _[K][−]_ [1)] _[/]_ [2] . Nonetheless, we are only interested in estimating expectations of test functions that depend\non _c_ alone, so that we can restrict attention to those, as usually done in non-reversible\nMCMC literature (Andrieu and Livingstone, 2021; Gagnon and Maire, 2024b). Given a\ntest function _g_ : [ _K_ ] _[n]_ _→_ R, with a slight abuse of notation, we also use _g_ in Var( _g, P_ NR )\nto denote the function defined as _g_ ( _c, v_ ) = _g_ ( _c_ ) for all ( _c, v_ ) _∈_ [ _K_ ] _[n]_ _×{−_ 1 _,_ +1 _}_ _[K]_ [(] _[K][−]_ [1)] _[/]_ [2] .\n\n\n**Theorem 3.1.** _Let π be a probability distribution on_ [ _K_ ] _[n]_ _and g_ : [ _K_ ] _[n]_ _→_ R _. Then_\n\n\n(14) _Var_ ( _g, P_ NR ) _≤_ _Var_ ( _g, P_ R ) _≤_ _c_ ( _K_ ) _Var_ ( _g, P_ MG ) + [ _c_ ( _K_ ) _−_ 1] _Var_ _π_ ( _g_ ) _,_\n\n\n_where c_ ( _K_ ) = 2( _K −_ 1) _and Var_ _π_ ( _g_ ) _denotes Var_ ( _g_ ( _X_ 0 )) _for X_ 0 _∼_ _π._\n\n\nSince in most realistic applications Var( _g, P_ MG ) is much larger than Var _π_ ( _g_ ), the\ninequality in (14) implies that _P_ NR can be worse than _P_ MG, in terms of variance of\nthe associated estimators, only by a factor of 2( _K −_ 1). Moreover, since the cost per\niteration of _P_ MG is _K/_ 2 times the one of _P_ NR (see Section 2.1) the overall worsening\nis at most by a factor of 4.\n\n\n13\n\n\n**Remark 3.2.** The proof of the second inequality in (14) relies on showing that\n_P_ R ( _c, c_ _[′]_ ) _≥_ _c_ _[−]_ [1] ( _K_ ) _P_ MG ( _c, c_ _[′]_ ) for every _c ̸_ = _c_ _[′]_, which means that the probability of\nchanging state according to _P_ R is not too low compared to the one of _P_ MG . Interestingly, the converse is not true, in the sense that there is no _d >_ 0 independent from _n_\nsuch that _P_ MG ( _c, c_ _[′]_ ) _≥_ _dP_ R ( _c, c_ _[′]_ ). Indeed, let _π_ be as in (4) with _K_ = 3, _**α**_ = (1 _,_ 1 _,_ 1)\nand _f_ _θ_ = _f_ . Then if _c_ = (1 _, . . .,_ 1 _,_ 2 _,_ 3) and _c_ _[′]_ = (1 _, . . .,_ 1 _,_ 2 _,_ 2) it is easy to see that\n\n\n2\n_P_ MG ( _c, c_ _[′]_ ) = and _P_ R ( _c, c_ _[′]_ ) = [1]\n_n_ (3 + _n −_ 1) 6 _n_ _[.]_\n\n\nThe first inequality in (14) instead relies on extending classical asymptotic variance\ncomparison results for lifted kernels to the case of state-dependent mixtures such as in\n_P_ NR, as shown in Section C.1.1 of the supplement.\n\n                        \n\nWe stress that the results of Theorem 3.1 hold uniformly, in the sense that no assumptions on _π_ are needed. Thus using _P_ NR is guaranteed to provide performances\nwhich never get significantly worse than the ones of _P_ MG in terms of asymptotic variances. In the next sections, we will see that on the contrary _P_ NR can lead to significant\nimprovements (e.g. by a factor of _n_ ) relative to _P_ MG .\n\n#### **3.2 Comparison with conditional sampler**\n\n\nWe now define the _conditional_ sampler targeting _π_ ( _c,_ _**θ**_ _,_ _**w**_ ) mentioned in Section 1.2.\nThe pseudocode is given in Algorithm 7 and we denote with _P_ CD the associated Markov\nkernel on [ _K_ ] _[n]_ _×_ Θ _[K]_ _×_ ∆ _K−_ 1 . Also here we consider the random-scan case, which\nallows for an easier comparison with _P_ MG and _P_ NR . We expect the main take-away\nmessages to remain valid for the arguably more popular deterministic-scan scheme,\neven if theoretical results there are less neat (see e.g. Roberts and Rosenthal (2015);\nHe et al. (2016); Gaitonde and Mossel (2024); Ascolani et al. (2024) and references\ntherein).\nThe next proposition, whose proof is inspired by the one of (Liu, 1994, Thm.1),\nshows that _P_ MG always yields a smaller asymptotic variance than _P_ CD . Again with an\nabuse of notation we use _g_ to denote both _g_ : [ _K_ ] _[n]_ _→_ R and _g_ : [ _K_ ] _[n]_ _×_ Θ _[K]_ _×_ ∆ _K−_ 1 _→_ R\nfunction of the first argument alone.\n\n\n**Proposition 3.3.** _Let π be as in_ (3) _and g_ : [ _K_ ] _[n]_ _→_ R _. Then for every f_ _θ_ _, n, Y we_\n_have that Var_ ( _g, P_ MG ) _≤_ _Var_ ( _g, P_ CD ) _._\n\n\nCombined with Theorem 3.1, the above result implies that Var( _g, P_ NR ) _≤_ _c_ ( _K_ )Var( _g, P_ CD )+\n\n[ _c_ ( _K_ ) _−_ 1] Var _π_ ( _g_ ), so that if _P_ NR outperforms _P_ MG then it should also be preferred to\n_P_ CD . Thus in the following we restrict to the comparison between _P_ MG and _P_ NR .\n\n\n14\n\n\n**Algorithm 7** (Conditional sampler _P_ CD )\n\nInitialize ( _c_ [(0)] _,_ _**θ**_ [(0)] _,_ _**w**_ [(0)] ) _∈_ [ _K_ ] _[n]_ _×_ Θ _[K]_ _×_ ∆ _K−_ 1\n**for** _t ≥_ 1 **do**\n\nSample _i ∼_ Unif ( _{_ 1 _, . . ., n_ + 1 _}_ ).\n**if** _i ≤_ _n_ **then**\n\nSample _c_ [(] _i_ _[t]_ [)] _∼_ _π_ ( _c_ _i_ _|_ _**θ**_ [(] _[t][−]_ [1)] _,_ _**w**_ [(] _[t][−]_ [1)] ) with\n\n_π_ ( _c_ _i_ = _k |_ _**θ**_ _,_ _**w**_ ) = ~~�~~ _Kk_ _[′]_ _w_ =1 _k_ _f_ _[w]_ _θ_ _[k]_ _k_ _[′]_ ( _[f]_ _Y_ _[θ]_ _I_ _k_ ) _[′]_ [(] _[Y]_ _[i]_ [)] _,_ _k_ = 1 _, . . ., K._\n\n\n**end if**\n\n**if** _i_ = _n_ + 1 **then**\n\nSample _**w**_ [(] _[t]_ [)] _∼_ Dir � _α_ 1 + _n_ 1 ( _c_ [(] _[t][−]_ [1)] ) _, . . ., α_ _K_ + _n_ _K_ ( _c_ [(] _[t][−]_ [1)] )�.\nSample _θ_ _k_ [(] _[t]_ [)] _[∼]_ _[π]_ [(] _[θ]_ _[k]_ _[ |][ c]_ [(] _[t][−]_ [1)] [)] _[ ∝]_ [�] _j_ : _c_ [(] _j_ _[t][−]_ [1)] = _k_ _[f]_ _[θ]_ _[k]_ [(] _[Y]_ _[j]_ [)] _[p]_ [0] [(] _[θ]_ _[k]_ [) for] _[ k]_ [ = 1] _[, . . ., K]_ [.]\n\n**end if**\n\n**end for**\n\n### **4 Scaling limit analysis**\n\n\nIn this section we derive scaling limit results for _P_ MG and _P_ NR as _n →∞_ . In general, given a sequence of discrete-time Markov chains _{X_ _t_ [(] _[n]_ [)] _}_ _t∈_ N, scaling limit results\n(Gelman et al., 1997; Roberts and Rosenthal, 2001b) consist in showing that a timechanged process _{Z_ _t_ [(] _[n]_ [)] _}_ _t∈_ R defined as _Z_ _t_ [(] _[n]_ [)] = _X_ _⌈_ [(] _h_ _[n]_ ( [)] _n_ ) _t⌉_ [, with] _[ h]_ [(] _[n]_ [)] _[ →∞]_ [and] _[ ⌈·⌉]_ [denoting]\nthe ceiling function, converges in a suitable sense to a non-degenerate process _{Z_ _t_ _}_ _t∈_ R +\nas _n →∞_ . Provided the limiting process is non-singular and ergodic, this is usually\ninterpreted as suggesting that _O_ ( _h_ ( _n_ )) iterations of the discrete-time Markov chain are\nneeded to mix. In other words, the time rescaling required to obtain a non-trivial limit\nis a measure of how the process speed scales as _n_ grows.\nIn order to derive such results we restrict to the prior case, where the likelihood\nis uninformative and the posterior distribution of _c_ coincides with the prior (2). This\ncan be formally described by setting _f_ _θ_ ( _y_ ) = _f_ ( _y_ ), with _f_ probability density on _Y_, so\nthat the data do not provide any information on the labels. The joint distribution and\nfull conditionals become\n\n\n\n_K_\n� _k_ =1 [Γ][(] _[α]_ _[k]_ [ +] _[ n]_ _[k]_ [(] _[c]_ [))]\n(15) _π_ ( _c_ ) =\n\n\n\n_|_ _**α**_ _|_ + _n −_ 1 _[,]_\n\n\n\n1 [Γ][(] _[α]_ _[k]_ [ +] _[ n]_ _[k]_ [(] _[c]_ [))]\n\n_,_ _π_ ( _c_ _i_ = _k | c_ _−i_ ) = _[α]_ _[k]_ [ +] _[ n]_ _[k]_ [(] _[c]_ _[−][i]_ [)]\nΓ( _|_ _**α**_ _|_ + _n_ ) _|_ _**α**_ _|_ + _n −_ 1\n\n\n\nwith _|_ _**α**_ _|_ = [�] _[K]_ _k_ =1 _[α]_ _[k]_ [. This is clearly a simplified setting, which allows an explicit math-]\nematical treatment and it can be considered as an extreme case of un-identifiability\nand overlapping components (which are indeed all the same). Extending the analysis\n\n\n15\n\n\nto the more realistic case of informative likelihood is an interesting direction for future\nresearch, see Section 7 for more details.\n\n#### **4.1 Marginal sampler**\n\n\nConsider a Markov chain _{c_ [(] _[t]_ [)] _}_ _t∈_ N with kernel _P_ MG and invariant distribution (15),\nwhere we suppress the dependence on _n_ for simplicity. Let\n\n\n\n_X_ _k_ ( _c_ ) = _[n]_ _[k]_ [(] _[c]_ [)]\n\n\n\n_n_\n\n\n\n\n[(] _[c]_ [)]\n\n= [1]\n\n_n_ _n_\n\n\n\n_n_\n� 1 ( _c_ _i_ = _k_ ) _,_ _c ∈_ [ _K_ ] _[n]_ _,_\n\n\n_i_ =1\n\n\n\nbe the multiplicity of component _k_ and\n\n\n(16) _**X**_ _t_ = ( _X_ _t,_ 1 _, . . ., X_ _t,K_ ) = _X_ 1 _c_ [(] _[t]_ [)] [�] _, . . ., X_ _K_ _c_ [(] _[t]_ [)] [��] _._\n� � �\n\n\nCrucially, since _π_ ( _c_ _i_ = _k | c_ _i_ ) defined in (15) only depends on the multiplicities, i.e.\n( _c_ 1 _, . . ., c_ _n_ ) are exchangeable a priori, it follows that ( _**X**_ _t_ ) _t_ =0 _,_ 1 _,_ 2 _,..._ is itself a Markov\nchain. Moreover, _{_ _**X**_ _t_ _}_ _t∈_ N is de-initializing for _{c_ [(] _[t]_ [)] _}_ _t∈_ N in the sense of Roberts and\nRosenthal (2001a), so that the convergence properties of the former are equivalent to\nthe one of the latter (by e.g. Corollary 2 therein). With an abuse of notation, we\ndenote the kernel of _{_ _**X**_ _t_ _}_ _t∈_ N also as _P_ MG .\nIn the proof of Theorem 4.1 we show that\n\n\n\nE [ _X_ _t_ +1 _,k_ _−_ _x_ _k_ _|_ _**X**_ _t_ = _**x**_ ] = [2]\n\n_n_ [2]\n\n\n\n� _α_ 2 _k_\n\n\n\n2 _[k]_ [+] _[ o]_ [(1)] �\n\n\n\n_α_ _k_\n\n2 _[−|]_ _**[α]**_ _[|]_ _[x]_ 2 _[k]_\n\n\n\nand\n\n\n\nE �( _X_ _t_ +1 _,k_ _−_ _x_ _k_ ) [2] _|_ _**X**_ _t_ = _**x**_ � = _n_ [2] [2] [[] _[x]_ _[k]_ [(1] _[ −]_ _[x]_ _[k]_ [) +] _[ o]_ [(1)]] _[,]_\n\n\n\nas _n →∞_ . The above suggests that a rescaling of order _O_ ( _n_ [2] ) is needed to have a\nnon-trivial limit, as we will formally show below. In particular, let _{_ _**Z**_ _t_ _}_ _t∈_ R + be the\ncontinuous-time process with generator\n\n\n\n\n_[∂]_ _g_ ( _**x**_ ) + [1]\n\n_∂x_ _k_ 2\n\n\n\n2\n\n\n\n_∂x_ _k_ _∂x_ _k_ _′_ _[g]_ [(] _**[x]**_ [)] _[,]_\n\n\n\n(17) _Ag_ ( _**x**_ ) = [1]\n\n2\n\n\n\n_K_\n�\n\n\n\n� ( _α_ _k_ _−|_ _**α**_ _|x_ _k_ ) _∂x_ _[∂]_\n\n_k_ =1\n\n\n\n_∂_ [2]\n\n� _x_ _k_ ( _δ_ _kk_ _′_ _−_ _x_ _k_ _′_ ) _∂x_ _k_\n\n_k,k_ _[′]_ =1\n\n\n\n_K_\n�\n\n\n\nfor every _g_ : ∆ _K−_ 1 _→_ R twice differentiable and where _**x**_ = ( _x_ 1 _, . . ., x_ _K_ ). Such process\nexists (Ethier, 1976) and is called Wright-Fisher with mutation rates given by _**α**_ . In\nparticular, _{_ _**Z**_ _t_ _}_ _t∈_ R + is a diffusion taking values in ∆ _K−_ 1 whose stationary density is\nexactly _π_ ( _**x**_ ) = Dir( _**x**_ _|_ _**α**_ ). The next theorem shows that, choosing _h_ ( _n_ ) = _n_ [2] _/_ 2, the\ncontinuous-time rescaling of _{_ _**X**_ _t_ _}_ _t∈_ N converges to _{_ _**Z**_ _t_ _}_ _t∈_ R + .\n\n\n16\n\n\n**Theorem 4.1.** _Let {_ _**Z**_ _t_ [(] _[n]_ [)] _}_ _t∈_ R + _such that_ _**Z**_ _t_ [(] _[n]_ [)] = _**X**_ _⌈_ _n_ 22 _[t][⌉]_ _[, where][ {]_ _**[X]**_ _[t]_ _[}]_ _[t][∈]_ [N] _[ is the Markov]_\n\n_chain in_ (16) _with kernel P_ MG _and invariant distribution π as in_ (15) _. Let {_ _**Z**_ _t_ _}_ _t∈_ R +\n_be a diffusion with generator as in_ (17) _. Then if_ _**Z**_ 0 [(] _[n]_ [)] _→_ _**Z**_ 0 _weakly as n →∞, we have_\n_that {_ _**Z**_ _t_ [(] _[n]_ [)] _}_ _t∈_ R + _→{_ _**Z**_ _t_ _}_ _t∈_ R + _weakly as n →∞, according to the Skorokhod topology._\n\n\n**Remark 4.2.** The proof relies on convergence of generators, which is a standard technique when dealing with sequences of stochastic processes: we refer to (Ethier and\nKurtz, 1986, Chapter 4) for details. While this approach is common in the MCMC\nliterature (see e.g. Gelman et al. (1997); Roberts and Rosenthal (2001b) and related\nworks), we are not aware of applications of it to mixture model contexts. On the\ncontrary, the Wright-Fisher process often arises as the scaling limit of models for populations subjected to genetic drift and mutation (Ethier and Kurtz, 1986; Etheridge,\n2011). Connections between sampling schemes and diffusions in population genetics\nhave been also explored in other context, especially for sequential Monte Carlo techniques (Koskela et al., 2020; Brown et al., 2021). \n\n**Remark 4.3.** Theorem 4.1 suggests that _O_ ( _n_ [2] ) iterations are needed for _P_ MG to\nconverge. This is coherent with Khare and Zhou (2009, Prop.14.10.1) where, albeit\nmotivated by a different problem, the authors show that, when targeting the prior\ndistribution _π_ ( _c_ ) in (15), the second largest eigenvalue of _P_ MG is\n\n\n_|_ _**α**_ _|_\n1 _−_\n_n_ ( _n_ + _|_ _**α**_ _| −_ 1) _[.]_\n\n\nThis implies that the so-called relaxation time of _P_ MG scales as _O_ ( _n_ [2] ) as _n →∞_,\nwhich means that _O_ ( _n_ [2] ) iterations are required to mix; see e.g. Levin and Peres (2017,\nThm.12.5) for more details on relaxation times. \n\nIn order to see why an _O_ ( _n_ [2] ) convergence is slower than desired, consider for\nexample the case _K_ = 2. Then _{X_ _t,_ 1 _}_ _t∈_ N is a Markov chain on _{_ 0 _,_ 1 _/n, . . .,_ 1 _}_ and\nthus _P_ MG requires _n_ [2] iterations to sample from a distribution on a state space with\ncardinality _n_ . Moreover, _{X_ _t,_ 1 _}_ _t∈_ N can be seen as a random walk with transition\nprobabilities\n\n\n\nP _X_ _t_ +1 _,_ 1 = _x_ 1 + [1]\n� _n_ _[|][ X]_ _[t,]_ [1] [ =] _[ x]_ [1]\n\n\n\n_α_ 1 + _nx_ 1\n= (1 _−_ _x_ 1 )\n� _α_ 1 + _α_ 2 + _n −_ 1 _[≈]_ _[x]_ [1] [(1] _[ −]_ _[x]_ [1] [)]\n\n\n\nand\n\n\n\nP _X_ _t_ +1 _,_ 1 = _x_ 1 _−_ [1]\n� _n_ _[|][ X]_ _[t,]_ [1] [ =] _[ x]_ [1]\n\n\n\n_α_ 2 + _n_ (1 _−_ _x_ 1 )\n= _x_ 1\n� _α_ 1 + _α_ 2 + _n −_ 1 _[≈]_ _[x]_ [1] [(1] _[ −]_ _[x]_ [1] [)] _[,]_\n\n\n\nwhen _n_ is large. Thus the probability of going up and down is almost the same, leading\nto the observed random-walk behaviour. This is reminiscent of classical examples\nstudied in the non-reversible MCMC literature (Diaconis et al., 2000), where a faster\nalgorithm is devised by considering a lifted version of the standard random walk.\n\n\n17\n\n\n#### 4.2 Non-reversible sampler P NR\n\nConsider now a Markov chain _{c_ [(] _[t]_ [)] _, v_ [(] _[t]_ [)] _}_ _t∈_ N with kernel _P_ NR and invariant distribution\n(15). Define _**X**_ _t_ as in (16) and _**V**_ _t_ = � _V_ _t,k,k_ _′_ [�] ( _k,k_ _[′]_ ) _∈_ [ _K_ ] [2] [ as]\n\n\n_̸_\n\n\n_̸_\n\n\n\n0 if _k_ = _k_ _[′]_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_V_ _′_ =\n_t,k,k_\n\n\n_̸_\n\n\n_̸_\n\n\n\n\n\n\n\n\n\n\n\n_̸_\n\n\n_̸_\n\n\n\n_v_ [(] _[t]_ [)] if _k < k_ _[′]_\n_k,k_ _[′]_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_−v_ [(] _[t]_ [)] if _k > k_ _[′]_ _._\n_k_ _[′]_ _,k_\n\n\n_̸_\n\n\n_̸_\n\n\n\nThis means that _V_ _t,k_ _′_ _,k_ = +1 implies that we are proposing from cluster _k_ _[′]_ to _k_, for\nevery pair ( _k, k_ _[′]_ ). This allows for a simpler statement in the theorem to follow.\nBy exchangeability arguments as above, it is simple to see that _{_ ( _**X**_ _t_ _,_ _**V**_ _t_ ) _}_ _t∈_ N is\nde-initializing for _{c_ [(] _[t]_ [)] _, v_ [(] _[t]_ [)] _}_ _t∈_ N and thus it has the same convergence properties. In\nthe proof of Theorem 4.4 we show that\n\n\n_̸_\n\n\n_̸_\n\n\n\n_K −_ 1 [+] _[ o]_ [(1)]\n\n\n_̸_\n\n\n_̸_\n\n\n\nE [ _X_ _t_ +1 _,k_ _−_ _x_ _k_ _|_ _**X**_ _t_ = _**x**_ _,_ _**V**_ _t_ = _v_ ] = [1]\n\n_n_\n\n\n_̸_\n\n\n_̸_\n\n\n\n\n\n\n_̸_\n\n\n_̸_\n\n\n\n�\n _k_ _[′]_ : _v_ _k′,k_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_x_ _k_ + _x_ _k_ _′_\n\n\n_̸_\n\n\n_̸_\n\n\n\n\n\n\n_̸_\n\n\n_̸_\n\n\n\n _,_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_k_ _[′]_ : _v_ _k′,k_ =+1\n\n\n_̸_\n\n\n_̸_\n\n\n\n_xK_ _k_ + _− x_ 1 _k_ _′_ _[−]_ �\n\n_k_ _[′]_ : _v_ _k′,k_ = _−_ 1\n\n\n_̸_\n\n\n_̸_\n\n\n\nwhich suggest that rescaling time by _n_ is sufficient for a non-trivial limit. A technical\nissue is that, when _X_ _t,k_ = 0 for some _k_ then one of the velocities jumps deterministically\nto _V_ _t,k_ _′_ _,k_ = +1 with _k_ _[′]_ = _̸_ _k_ . To avoid complications related to such boundary effects,\nwe study the scaling of the process in the set\n\n\n_E_ _M_ _× V_ = _**x**_ _∈_ ∆ _K−_ 1 _| x_ _k_ _>_ [1] _× {−_ 1 _,_ 0 _,_ +1 _}_ [[] _[K]_ []] [2] _,_\n� _M_ [for every] _[ k]_ �\n\n\nwith _M >_ 0 arbitrarily large but fixed.\n\n\n_̸_\n\n\n\n_̸_\n\n\nLet _**Z**_ _t_ [(] _[M]_ [)]\n� �\n\n\n_̸_\n\n\n\n_̸_\n\n\n_t∈_ R + [be a piecewise deterministic Markov process]\n\n\n_̸_\n\n\n\n_̸_\n\n\n_**Z**_ [(] _[M]_ [)]\n_t∈_ R + [=] � 1 _,t_ _[,]_ _**[ Z]**_ 2 [(] _[M]_ _,t_ [)] �\n\n\n_̸_\n\n\n\n_̸_\n\n\n(Davis, 1984) on _E_ _M_ _×V_ defined as follows. Consider a inhomogeneous Poisson process\nΛ _t_ with rate\n\n\n_̸_\n\n\n\n_̸_\n\n\n1\n(18) _λ_ _**Z**_ _t_ [(] _[M]_ [)] =\n� � 2( _K −_ 1)\n\n_̸_\n\n\nwhere\n\n\n\n_̸_\n\n\n�\n\n\n_k_ = _̸_ _k_ _[′]_\n\n\n\n_̸_\n\n\n� _Z_ 1 [(] _[M]_ _,t,k_ [)] [+] _[ Z]_ 1 [(] _[M]_ _,t,k_ [)] _[′]_ � _β_ � _Z_ 1 [(] _[M]_ _,t,k_ [)] _[, Z]_ 1 [(] _[M]_ _,t,k_ [)] _[′]_ _[, Z]_ 2 [(] _[M]_ _,t,k,k_ [)] _[′]_ � + 2 _ξ,_\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_β_ ( _x_ _k_ _, x_ _k_ _′_ _, v_ _k_ _′_ _,k_ ) = max 0 _,_ _[α]_ _[k]_ _[−]_ _[−]_ [1]\n� _x_ _k_ _−_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_x_ _k_ +\n\n\n\n_̸_\n\n\n_̸_\n\n\n_[−]_ _[−]_ [1]\n\n+ [1] _[ −]_ _[α]_ _[k]_ [+]\n_x_ _k_ _−_ _x_ _k_\n\n\n\n_̸_\n\n\n_̸_\n\n\n�\n\n\n\n_̸_\n\n\n_̸_\n\n\nwith _k_ _−_ = _k_ _[′]_ and _k_ + = _k_ if _v_ _k_ _′_ _,k_ = +1 and viceversa if _v_ _k_ _′_ _,k_ = _−_ 1. In between events,\n\n\n18\n\n\n_{_ _**Z**_ _t_ [(] _[M]_ [)] _}_ _t∈_ R + evolves deterministically as\n(19)\nd _Z_ [(] _[M]_ [)]\n1 _,t,k_ = Φ _k_ _**Z**_ _t_ [(] _[M]_ [)]\nd _t_ � �\n\n\n_̸_\n\n\n\n1\n\n=\n_K −_ 1\n\n\n_̸_\n\n\n\n\n\n\n_̸_\n\n\n\n�\n\n_k_ _[′]_ : _Z_ [(] _[M]_ [)]\n\n 2 _[′]_\n\n\n_̸_\n\n\n\n\n_Z_ [(] _[M]_ [)]\n� 1 _,t,k_ [+] _[ Z]_ 1 [(] _[M]_ _,t,k_ [)] _[′]_ �\n\n\n\n\n_̸_\n\n\n\n_k_ _[′]_ : _Z_ [(] _[M]_ [)]\n2 _,t,k_ _[′]_ _,k_ [=+1]\n\n\n_̸_\n\n\n\n_Z_ [(] _[M]_ [)] _−_\n� 1 _,t,k_ [+] _[ Z]_ 1 [(] _[M]_ _,t,k_ [)] _[′]_ � �\n\n_k_ _[′]_ : _Z_ [(] _[M]_ [)]\n2 _,t,k_ _[′]_ _,k_ [=] _[−]_ [1]\n\n\n_̸_\n\n\n\nand\nd _Z_ [(] _[M]_ [)]\n2 d _,t_ _t_ _,k_ _[′]_ _,k_ = 0 _,_\n\nwith ( _k_ _[′]_ _, k_ ) _∈_ [ _K_ ] [2] . The system of differential equations in (19) admits a unique\nsolution by linearity in its arguments. Instead, at each event of Λ _t_, say at _τ >_ 0, a pair\n( _k, k_ _[′]_ ) _∈_ [ _K_ ] [2] is selected with probability\n\n\n_̸_\n\n\n\n_Z_ [(] _[M]_ [)]\n_q_ ( _k, k_ _[′]_ ) _∝_ 1 _,t,k_ [+] _[ Z]_ 1 [(] _[M]_ _,t,k_ [)] _[′]_\n2( _K −_ 1)\n\n\n_̸_\n\n\n\n� _β_ � _Z_ 1 [(] _[M]_ _,τ_ _−_ [)] _,k_ _[, Z]_ 1 [(] _[M]_ _,τ_ _−_ [)] _,k_ _[′]_ _[, Z]_ 2 [(] _[M]_ _,τ_ _−_ [)] _,k_ _[′]_ _,k_ � + 2 _ξ_ � 1 ( _k ̸_ = _k_ _[′]_ )\n\n\n_̸_\n\n\n\nand then the process jumps as follows:\n\n(20) _Z_ 2 [(] _[M]_ _,τ,k_ [)] _[′]_ _,k_ [=] _[ −][Z]_ 2 [(] _[M]_ _,τ_ _−_ [)] _,k_ _[′]_ _,k_ and _Z_ 2 [(] _[M]_ _,τ,k,k_ [)] _[′]_ [ =] _[ −][Z]_ 2 [(] _[M]_ _,τ_ _−_ [)] _,k,k_ _[′]_ _[,]_\n\n\nwhere _τ_ _−_ denotes the the left-limit at _τ_ . It follows that _**Z**_ _t_ [(] _[M]_ [)]\n� � _t∈_ R + [is a continuous-]\n\ntime process with generator\n(21)\n\n\n_̸_\n\n\n\n\n\n� _q_ ( _k, k_ _[′]_ ) � _g_ ( _**z**_ **(** _**k,k**_ _**′**_ **)** ) _−_ _g_ ( _**z**_ )�\n\n_k_ = _̸_ _k_ _[′]_ \n\n\n\n_̸_  _[,]_\n\n\n\n_B_ [(] _[M]_ [)] _g_ ( _**z**_ ) = 1 ( _**z**_ 1 _∈_ _E_ _M_ )\n\n\n_̸_\n\n\n\n\n\n\n _̸_\n\n\n\n_K_\n�\n\n_̸_\n\n\n\n_∂_\n\n� _k_ =1 Φ _k_ ( _**z**_ ) _∂z_ 1 _,k_ _g_ ( _**z**_ ) + _λ_ ( _**z**_ ) _k_ � = _̸_ _k_\n\n\n\n_̸_\n\n\nfor every _g_ : _E_ _M_ _× V →_ R twice continuously differentiable in the first argument,\nwhere _**z**_ [(] _[k,k]_ _[′]_ [)] _∈_ _E_ _M_ _× V_ is equal to _**z**_ except for\n\n\n_**z**_ [(] _[k,k]_ _[′]_ [)]\n2 _,k,k_ _[′]_ [ =] _[ −]_ _**[z]**_ 2 [(] _[k,k]_ _,k_ _[′]_ _,k_ _[′]_ [)] [=] _[ −]_ _**[z]**_ [2] _[,k,k]_ _[′]_ _[.]_\n\n\nSuch a process exists for every _M >_ 0 since the rates _λ_ ( _**z**_ ) are bounded (Davis, 1984).\nWe can think of _**Z**_ _t_ [(] _[M]_ [)]\n� � _t∈_ R + [as a process with an absorbing boundary, which remains]\n\nconstant as soon as _Z_ [(] _[M]_ [)]\n1 _,t,k_ _[≤]_ [1] _[/M]_ [ for some] _[ k]_ [.]\n\nAnalogously, define _**X**_ _t_ [(] _[M]_ [)] _,_ _**V**_ _t_ [(] _[M]_ [)]\n� � _t∈_ N [as a modification of] _[ {]_ _**[X]**_ _[t]_ _[,]_ _**[ V]**_ _[t]_ _[}]_ _[t][∈]_ [N] [, which re-]\n\nmains constant as soon as _X_ _t,k_ [(] _[M]_ [)] _≤_ 1 _/M_ for some _k_ . The next theorem shows that,\n\n\n\n_̸_\n\n\nchoosing _h_ ( _n_ ) = _n_, the continuous-time rescaling of _**X**_ _t_ [(] _[M]_ [)] _,_ _**V**_ _t_ [(] _[M]_ [)]\n� �\n\n\n\n_̸_\n\n\nchoosing _h_ ( _n_ ) = _n_, the continuous-time rescaling of _**X**_ _t_ _,_ _**V**_ _t_\n\n_t∈_ N [converges to]\n_**Z**_ _t_ [(] _[M]_ [)]\n� � _t_ R [.]\n\n\n\n_̸_\n\n\n_t∈_ R + [.]\n\n\n\n_̸_\n\n\n19\n\n\n**Theorem 4.4.** _Fix M >_ 0 _and let_ � _**Z**_ _t_ [(] _[M,n]_ [)] � _t∈_ R + _[such that]_ _**[ Z]**_ _t_ [(] _[M,n]_ [)] = � _**X**_ _⌈_ [(] _nt_ _[M]_ _⌉_ [)] _[,]_ _**[ V]**_ _⌈_ [ (] _nt_ _[M]_ _⌉_ [)] � _,_\n\n_where {_ _**X**_ _t_ _,_ _**V**_ _t_ _}_ _t∈_ N _is a Markov chain with operator P_ NR _and invariant distribution_\n_as in_ (11) _, with π in_ (15) _._ _Let_ _**Z**_ _t_ [(] _[M]_ [)]\n� � _t∈_ R + _[be a piecewise deterministic Markov]_\n\n_process with generator_ (21) _. Then if_ _**Z**_ 0 [(] _[M,n]_ [)] _→_ _**Z**_ 0 [(] _[M]_ [)] _weakly as n →∞, we have that_\n_**Z**_ _t_ [(] _[M,n]_ [)] _**Z**_ _t_ [(] _[M]_ [)]\n� � _t∈_ R + _[→]_ � � _t∈_ R + _[weakly as][ n][ →∞][, according to the Skorokhod topology.]_\n\n\n**Remark 4.5.** Looking at the process only in the interior of the simplex is inspired by\nother works on diffusion approximations, see e.g. Barton et al. (2004) where they use\na similar technique to deal with explosive behaviour in the boundary. If _α_ _k_ _>_ 1 for\nevery _K_, we could proceed as in Theorem 4 _._ 2 therein to show that the boundary is\nnever reached and thus the limit can be extended to the whole space. \n\nTheorem 4.4 suggests that the overall computational cost of Algorithm 6 is _O_ ( _n_ )\nand, combined with Theorem 4.1, this suggest an _O_ ( _n_ ) speed-up relative to _P_ MG in\nthe prior case. In Section 6 we will show empirically that large improvements are also\npresent in more realistic and interesting settings where the likelihood is informative.\n\n### 5 A variant of P NR\n\n\nThe kernels _P_ R and _P_ NR sample a new pair ( _k, k_ _[′]_ ) at every iteration. While this is\nnatural and allows for direct theoretical comparisons with _P_ MG (see Theorem 3.1), an\nalternative in the non-reversible case is to keep the same value of ( _k, k_ _[′]_ ) for multiple\niterations. We thus define the following, non-reversible and ˜ _π_ -invariant kernel\n\n\n\n_**Z**_ _t_ [(] _[M]_ [)]\n_t∈_ R + _[→]_ � �\n\n\n\n_t∈_ R + _[weakly as][ n][ →∞][, according to the Skorokhod topology.]_\n\n\n\n_∞_\n� _q_ _m_ _c_ ( _k,k_ _′_ ) ( _t_ ) _P_ [˜] _k,k_ _[t]_ _[′]_ _[,]_\n\n\n_t_ =1\n\n\n\n(22) _Q_ NR = �\n\n( _k,k_ _[′]_ ) _∈K_\n\n\n\n2\n\n_K_ ( _K −_ 1)\n\n\n\nwith _m_ _c_ ( _k, k_ _[′]_ ) = ( _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ )) _/s_ for some fixed _s ∈_ (0 _,_ 1) and _q_ _m_ ( _t_ ) being the\nprobability mass function of a geometric random variable with parameter 1 _/m_ . The\nalgorithm picks a couple ( _k, k_ _[′]_ ) uniformly at random and then takes a random number\nof steps of the lifted kernel _P_ [˜] _kk_ _′_, with average number of steps proportional to the total\nnumber of points in the two clusters, i.e. _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ ). The associated pseudo-code\nis presented in Algorithm 8. Reasoning as in Lemma 2.3 it is easy to see that _Q_ NR is\n\n\n**Algorithm 8** Modified non-reversible sampler ( _c_ _[′]_ _, v_ _[′]_ ) _∼_ _Q_ NR (( _c, v_ ) _, ·_ )\n\nSample ( _k, k_ _[′]_ ) _∼_ Unif( _K_ )\nSample _t ∼_ Geom ( _s/_ ( _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ ))) for some fixed _s ∈_ (0 _,_ 1)\nSample ( _c_ _[′]_ _, v_ _[′]_ ) _∼_ _P_ [˜] _k,k_ _[t]_ _[′]_ [((] _[c, v]_ [)] _[,][ ·]_ [)]\n\n\n_π_ ˜-invariant and uniformly ergodic, as stated in the next lemma.\n\n\n20\n\n\n**Lemma 5.1.** _For any probability distribution π on_ [ _K_ ] _[n]_ _, the Markov kernel Q_ NR _de-_\n_fined in Algorithm 8 is_ ˜ _π-invariant, with_ ˜ _π as in_ (11) _. Moreover, if π_ ( _c_ ) _>_ 0 _for every_\n_c ∈_ [ _K_ ] _[n]_ _, then Q_ NR _is irreducible, aperiodic and uniformly ergodic._\n\n\nThe distinction with the main algorithm is that _P_ NR resamples the pair ( _k, k_ _[′]_ ) at\neach iteration with probability proportional to _n_ _k_ ( _c_ )+ _n_ _k_ _′_ ( _c_ ), while _Q_ NR keeps the same\n( _k, k_ _[′]_ ) for _O_ ( _n_ _k_ ( _c_ )+ _n_ _k_ _′_ ( _c_ )) iterations and then resamples the pair ( _k, k_ _[′]_ ) uniformly from\n_K_ . Indeed we expect _P_ NR and _Q_ NR to perform similarly for fixed values of _K_, but we\nempirically observe that _Q_ NR tends to yield slower mixing as _K_ increases: see Section\nA in the Supplementary Material for a simulative comparison in the prior case. This\nmotivated us to focus on _P_ NR as the main scheme of interest in this paper.\n\n\n**Remark 5.2.** In the prior case of Section 4, where the invariant distribution is given\nby (15), it is possible to find a corresponding scaling limit for _Q_ NR . The proof is\nanalogous to the case of _P_ NR and we omit it for brevity, just limiting ourselves to\nidentifying the candidate limit and discussing its implications. Consider a Markov\nchain _{_ ( _**X**_ _t_ _,_ _**V**_ _t_ ) _}_ _t∈_ N with kernel _Q_ NR . With similar calculations as in Theorem 4.4, the\nprocess � _**Z**_ _t_ [(] _[M,n]_ [)] � _t∈_ R + [defined as] _**[ Z]**_ _t_ [(] _[M,n]_ [)] = � _**X**_ _⌈_ [(] _nt_ _[M]_ _⌉_ [)] _[,]_ _**[ V]**_ _⌈_ [ (] _nt_ _[M]_ _⌉_ [)] � can be shown to converge\n\nto _{_ _**Z**_ _t_ _}_ _t∈_ R + with generator\n\n\n_∂_ _∂_\n_C_ [(] _[M]_ [)] _g_ ( _**z**_ ) = 1 ( _**z**_ 1 _∈_ _E_ _M_ ) _g_ ( _**z**_ ) _−_ _g_ ( _**z**_ )\n� _∂z_ 1 _,k_ + _∂z_ 1 _,k_ _−_\n\n\n_̸_\n\n\n\n+ max 0 _,_ _[α]_ _[k]_ _[−]_ _[−]_ [1]\n� _z_ 1 _,k_ _−_\n\n\n_̸_\n\n\n\n_z_ 1 _,k_ +\n\n\n_̸_\n\n\n\n\n_[k]_ _[−]_ _[−]_ [1] + [1] _[ −]_ _[α]_ _[k]_ [+]\n\n_z_ 1 _,k_ _−_ _z_ 1 _,k_ +\n\n\n_̸_\n\n\n\n\n[ _g_ ( _**z**_ 1 _, −_ _**z**_ 2 ) _−_ _g_ ( _**z**_ )]\n�\n\n\n_̸_\n\n\n\n_g_ _**z**_ 1 _,_ _**z**_ 2 [(] _[k,k]_ _[′]_ [)] _−_ _g_ ( _**z**_ ) _,_\n� � � � [�]\n\n_̸_\n\n\n\n_s_\n+\n_z_ 1 _,k_ _−_ + _z_ 2 _,k_ + _̸_\n\n\n\n�\n\n\n_k_ = _̸_ _k_ _[′]_\n\n\n\n_z_ 1 _,k_ + _z_ 1 _,k_ _′_\n\n2( _K −_ 1)\n\n_̸_\n\n\n\n_̸_\n\n\nwith _k_ _−_ = _k_ _[′]_ and _k_ + = _k_ if _z_ 2 _,k,k_ _′_ = +1 and viceversa if _z_ 2 _,k,k_ _′_ = _−_ 1. Moreover\n_**z**_ 2 [(] _[k,k]_ _[′]_ [)] is the vector with _z_ 2 _,k,k_ _′_ = _−z_ 2 _,k_ _′_ _,k_ = +1 and zero otherwise. Interestingly, _C_ [(] _[M]_ [)]\n\ncoincides with the generator of the so-called Coordinate Sampler, introduced in Wu\nand Robert (2020), with target distribution Dir( _**α**_ ). \n#### **5.1 The random projection sampler being approximated**\n\n\nThe main feature of _Q_ NR is that, after sampling a pair ( _k, k_ _[′]_ ) _∈K_, the operator _P_ [˜] _k,k_ _′_ is\napplied for a random number of iterations. If _s →_ 0 the latter diverges almost surely,\nmeaning that after selecting the pair the sampler will behave as _P_ [˜] _k,k_ _[t]_ _[′]_ [ with] _[ t][ →∞]_ [. By]\ndefinition of _P_ [˜] _k,k_ _[t]_ _[′]_ [ and ergodicity, this converges to the kernel Π] _[k,k]_ _[′]_ [ that updates the]\nsub-partition of points in clusters _k_ and _k_ _[′]_ conditional on the rest, i.e.\n\n\n\n_̸_\n\n\nlim\n_t→∞_ _[P]_ _k,k_ _[ t]_ _[′]_ [(] _[c, c]_ _[′]_ [) = ˜Π] _[k,k]_ _[′]_ [(] _[c, c]_ _[′]_ [)] _[ ∝]_\n\n\n\n_̸_\n\n\n\n\n\n\n_̸_\n\n\n�\n _i_ : _c_ _i_ _/∈{_\n\n\n\n_̸_\n\n\n� 1 � _c_ _i_ = _c_ _[′]_ _i_ �\n\n_i_ : _c_ _i_ _/∈{k,k_ _[′]_ _}_ \n\n\n\n_̸_\n\n\n(23) _t_ lim _→∞_ _[P]_ _k,k_ _[ t]_ _[′]_ [(] _[c, c]_ _[′]_ [) = ˜Π] _[k,k]_ _[′]_ [(] _[c, c]_ _[′]_ [)] _[ ∝]_ � 1 � _c_ _i_ = _c_ _[′]_ _i_ � _π_ ( _c_ _[′]_ ) _c, c_ _[′]_ _∈_ [ _K_ ] _[n]_ _._\n\n _i_ : _c_ _i_ _/∈{k,k_ _[′]_ _}_ \n\n\n\n_̸_\n\n\n21\n\n\nNote that Π _k,k_ _′_ is a projection kernel, i.e. Π [2] _k,k_ _[′]_ [ = Π] _[k,k]_ _[′]_ [. Analogously, again as] _[ s][ →]_ [0,]\n_Q_ NR converges to the random projection kernel defined as\n\n\n\n2\n_P_ RP ( _c, c_ _[′]_ ) =\n_K_ ( _K −_ 1)\n\n\n\n�\n\n\n\n(24) _P_ RP ( _c, c_ _[′]_ ) = _K_ ( _K −_ 1) � Π _k,k_ _′_ ( _c, c_ _[′]_ ) _c, c_ _[′]_ _∈_ [ _K_ ] _[n]_ _,_\n( _k,k_ _[′]_ ) _∈K_\n\n\n\nwhose structure resembles the one of a random-scan Gibbs Sampler that updates the\nsub-partition of two randomly chosen pairs of clusters given the configuration of the\nother clusters. In this perspective, _Q_ NR can be interpreted as a Metropolis-withinGibbs sampler approximating _P_ RP .\n\n\n**Remark 5.3.** In the prior case, as _n →∞_, we expect _P_ RP in turn to approximate a\nGibbs sampler on the ( _K −_ 1)-dimensional simplex, which at every iteration updates\ntwo coordinates chosen at random. In the special case of _**α**_ = (1 _, . . .,_ 1), the latter has\nbeen studied in Smith (2014) and shown to require _O_ ( _K_ log( _K_ )) iterations for mixing.\n\n\n### **6 Simulations**\n\n#### **6.1 Prior case**\n\n\nFirst of all we consider the prior case, where _f_ _θ_ = _f_ and the target distribution is given\nby (15). We let _K_ = 3, _n_ = 1000 and we run Algorithms 1 and 6 for 300 independent\nruns, first with _**α**_ = (1 _,_ 1 _,_ 1) and then with _**α**_ = (0 _._ 1 _,_ 0 _._ 1 _,_ 0 _._ 1). Initial configurations are\n\ni.i.d.\nindependently generated, so that _c_ [(0)] _i_ _∼_ Unif ([ _K_ ]). For each run we store the value of\nthe chains after _T_ = 100 _× n_ iterations and plot the corresponding proportion of labels\nof the first two components, i.e. ( _n_ 1 ( _c_ [(] _[T]_ [)] ) _/n, n_ 2 ( _c_ [(] _[T]_ [)] ) _/n_ ) in Figure 2. If the chains had\nreached convergence by then, these should be 300 independent samples approximately\nfollowing a Dirichlet-Multinomial distribution with parameters _**α**_ (since _n_ is large, this\nis visually close to drawing samples directly from a Dir( _**α**_ ) distribution).\nFrom the results in Figure 2, it is clear that the non-reversible scheme (second\ncolumn) leads to faster convergence: this is particularly manifest in the second row\n(corresponding to _**α**_ = (0 _._ 1 _,_ 0 _._ 1 _,_ 0 _._ 1)), where the mass should be concentrated around\nthe borders of the simplex. Indeed, both chains associated to _P_ MG remain stuck close\nto the initial configuration, where the proportion within each group is close to 1 _/_ 3.\nThis is also clear from the last column of Figure 2, which shows that the marginal\ndistribution of _P_ NR (in black) converges to the stationary one after fewer iterations.\n\n\n22\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n\nFigure 2: Left and center column: plot of the proportions of the first two components in the last\nof 100 iterations (after a thinning of size _n_ ) over 300 independent runs for _P_ MG (left) and\n_P_ NR (center). Right column: plot of the marginal distribution of the proportion of the\nfirst component at every 10 iterations (after thinning) for _P_ MG (gray) and _P_ NR (black).\nThe first and second rows refer to _**α**_ = (1 _,_ 1 _,_ 1) and _**α**_ = (0 _._ 1 _,_ 0 _._ 1 _,_ 0 _._ 1), respectively. The\ntarget distribution is given in (15).\n\n#### **6.2 Posterior case**\n\n\nWe now consider model (1) with _Y_ = Θ = R, _K_ = 3,\n\n\n(25) _f_ _θ_ ( _y_ ) = _N_ ( _y | θ, σ_ [2] ) _,_ _p_ 0 ( _θ_ ) = _N_ ( _θ | µ_ 0 _, σ_ 0 [2] [)] _[.]_\n\n\nand hyperparameters set to _µ_ 0 = 0 and _σ_ [2] = _σ_ 0 [2] [= 1. We then generate 300 indepen-]\ndent data sets of size _n_ = 1000, each generated from the model as follows:\n\n\ni.i.d.\n1. Sample _**w**_ _∼_ Dirichlet( _**α**_ ) and _θ_ _k_ _∼_ _p_ 0 for _k_ = 1 _, . . ., K_ .\n\n\ni.i.d.\n2. Sample _Y_ _i_ _∼_ [�] _[K]_ _k_ =1 _[w]_ _[k]_ _[f]_ _[θ]_ _k_ [(] _[y]_ [) for] _[ i]_ [ = 1] _[, . . ., n]_ [.]\n\n\nFor each dataset we target the associated posterior using _P_ MG and _P_ NR . As before\ni.i.d.\nwe initialize each chain uniformly, i.e. _c_ [(0)] _i_ _∼_ Unif ([ _K_ ]), and store its value after\n_T_ = 100 _× n_ iterations. Since the data are generated from the (Bayesian) model,\nthe resulting distribution of the proportions within each component should be close\nto the prior one, i.e. again a Dirichlet-multinomial with parameter _**α**_ . This test for\nconvergence, discussed for example in Geweke (2004), relies on the fact that sampling\nfrom the _prior_ distribution is equivalent to sampling from the _posterior_, given data\ngenerated according to the marginal distribution induced by the model.\n\n\n23\n\n\nThe resulting samples are displayed in Figure 3, with the same structure as in\nFigure 2. Again the non-reversible scheme is much closer to the correct distribution,\nwhile _P_ MG remains close to the initial configuration. Indeed, the results are remarkably\nclose to the ones presented in Section 6.1: this suggests that the behaviour observed in\nthe prior case is informative also of the actual behaviour observed in the posterior case,\nat least in this setting. In Section B of the Supplementary Material similar results are\nshown for the Poisson kernel.\n\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n\nFigure 3: Left and center column: plot of the proportions of the first two components in the last\nof 100 iterations (after a thinning of size _n_ ) over 300 independent runs for _P_ MG (left)\nand _P_ NR (center). Right column: plot of the marginal distribution of the proportion\nof the first component at every 10 iterations (after thinning) for _P_ MG (gray) and _P_ NR\n(black). The rows refer to _α_ = 1 and _α_ = 0 _._ 1 and the target distribution is given by the\nposterior of model (1), with _f_ _θ_ ( _y_ ) as in (25), _µ_ 0 = 0 and _σ_ [2] = _σ_ 0 [2] [= 1.]\n\n#### **6.3 A high dimensional example**\n\n\nWe now consider a higher dimensional version of the previous setting, where\n\n\n(26) _f_ _θ_ ( _y_ ) = _N_ ( _y | θ, σ_ _p_ [2] _[I]_ _[p]_ [)] _[,]_ _p_ 0 ( _θ_ ) = _N_ ( _θ | µ_ 0 _, σ_ 0 [2] _[I]_ _[p]_ [)] _[,]_\n\n\n\nwhere now _y ∈_ R _[p]_ and _θ ∈_ R _[p]_ with _p ≥_ 1. We rescale the likelihood variance as _σ_ _p_ [2] [=] _[ cp]_\nwhich guarantees that\n\n_p_\n\n1\n_σ_ [2] � ( _θ_ 1 _j_ _−_ _θ_ 2 _j_ ) [2] = _O_ (1) _._\n_p_\n\n\n\n_p_\n�\n\n\n\n� ( _θ_ 1 _j_ _−_ _θ_ 2 _j_ ) [2] = _O_ (1) _._\n\n_j_ =1\n\n\n\n24\n\n\nIn other words, we ask that the distance across components, rescaled by the variance,\ndoes not diverge as _p_ grows: this implies that some overlap between components is\nretained and that the problem is statistically non-trivial (see e.g. Chandra et al. (2023)\nfor more discussion of Bayesian mixture models with high-dimensional data).\nWe generate 500 independent samples of size _n_ = 1000 from model (26) with\n_p_ = 18, _K_ = 5, _µ_ 0 = 0, _σ_ 0 [2] [= 0] _[.]_ [5,] _[ c]_ [ = 2 and] _**[ α]**_ [ = (4] _[,]_ [ 1] _[, . . .,]_ [ 1). The data are generated]\nas explained in the previous section and we run both _P_ MG and _P_ NR, retaining only the\nlast iteration for every chain: the initialization is again uniform at random.\nIn Figure 4 we plot the histograms of the last iteration for the proportion associated\nto the first component of _P_ MG and _P_ NR for 500 independent runs. Comparing the\nlatter with the prior density, given by a Dirichlet-Multinomial with parameters (4 _,_ 4)\n(approximately Beta(4 _,_ 4)), it is evident that the non-reversible scheme is able to forget\nthe initialization while the reversible is not. Indeed, as also clear from the right plot\nof Figure 4, the marginal distribution of _P_ MG significantly underestimates the size of\nthe first cluster after _T_ = 100 _× n_ iterations.\n\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n\nFigure 4: Left and center column: histogram of the proportion of the first component in the last\nof 100 iterations (after a thinning of size _n_ ) over 500 independent runs for _P_ MG (left) and\n_P_ NR (center). The gray line corresponds to the density of a Beta(4 _,_ 4). Right column:\nplot of the marginal distribution of the chains at every 10 iterations (after thinning) for\n_P_ MG (gray) and _P_ NR (black). The target distribution is given by the posterior of model\n(1), with _f_ _θ_ ( _y_ ) as in (26), _p_ = 18, _K_ = 5, _µ_ 0 = 0, _σ_ 0 [2] [= 0] _[.]_ [5,] _[ c]_ [ = 2 and] _**[ α]**_ [ = (4] _[,]_ [ 1] _[, . . .,]_ [ 1).]\n\n#### **6.4 Overfitted setting**\n\n\nFinally, we consider an overfitted case, previously discussed in Section 2.2.1. We take\na one-dimensional Gaussian kernel as in (25) and take _α_ _k_ = _α_ for all _k ∈{_ 1 _, . . ., K}_ .\nIn this setting, using the notation of Section 2.2.1, Rousseau and Mengersen (2011,\nThm.1) implies that\n\n\n1. if _α >_ 1 _/_ 2, then more than _K_ _[∗]_ atoms have non-negligible mass, i.e. multiple\natoms are associated to the same “true” component,\n\n\n25\n\n\n2. if _α ≤_ 1 _/_ 2, then the posterior concentrates on configurations with exactly _K_ _[∗]_\n\ncomponents, up to _n_ _[−]_ [1] _[/]_ [2] posterior mass.\n\n\ni.i.d.\nWe take _K_ = 2 and _K_ _[∗]_ = 1, with _Y_ _i_ _∼_ _N_ ( _y |_ 2 _,_ 1) and _n_ = 1000. The first two\ncolumns of Figure 5 plot the histogram of the proportion of the first component after\n_T_ = 100 _×n_ iterations (and thinning of size _n_ ) for _α_ = 1 (top row) and _α_ = 0 _._ 1 (bottom\nrow). The two algorithms are initialized according to the “incorrect” scenario, i.e. all\nthe observations in the first component in the first row and uniformly at random\nin the bottom row. The figure illustrates that only _P_ NR is able to reach the high\nprobability region: this means that, despite its locality, the persistence of _P_ NR allows\nfor significantly faster traveling across the space. On the contrary, _P_ MG remain stuck\nin the initial configuration (which yields a similar likelihood) for both the scenarios.\nThis is also confirmed by the right column, which depicts the marginal distribution of\nthe chains: after few iterations, the distribution associated to _P_ NR stabilizes and yields\nthe correct behaviour.\n\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n\nFigure 5: Left and center column: histogram of the proportion of the first component in the last\nof 100 iterations (after a thinning of size _n_ ) over 300 independent runs for _P_ MG (left) and\n_P_ NR (center). Right column: plot of the marginal distribution of the proportion of the\nfirst component at every 10 iterations (after thinning) for _P_ MG (gray) and _P_ NR (black).\nFirst row: _α_ = 3 _/_ 2 and initialization uniformly at random. Second row: _α_ = 0 _._ 1 and\n_c_ [(0)] _i_ = 1 for every _i_ . The target distribution is given by the posterior of model (1), with\n\ni.i.d.\n_Y_ _i_ _∼_ _N_ ( _y |_ 2 _,_ 1) and _f_ _θ_ ( _y_ ) as in (25), _µ_ 0 = 0 and _σ_ [2] = _σ_ 0 [2] [= 1.]\n\n\n26\n\n\n### **7 Discussion**\n\nIn this work we introduced a novel, simple and effective non-reversible MCMC sampler\nfor mixture models, which enjoys three favourable features: (i) it is a simple modification of the original marginal scheme of Algorithm 1, (ii) its performance cannot be\nworse than the reversible chain by more than a factor of four (Theorem 3.1), (iii) it is\nshown to drastically speed-up convergence in various scenarios of interest.\n\nBoth the theory and methodology presented in this work could be extended in many\ninteresting directions, and we now discuss some of those, starting from algorithmic and\nmethodological ones. First, in the current formulation of Algorithm 6, the pair of\nclusters to update and the observation to move are selected with probabilities that\ndo not depend on the actual observations within the clusters (except for their sizes).\nA natural extension would be to consider informed proposal distributions, as in e.g.\nZanella (2020); Power and Goldman (2019); Gagnon and Maire (2024b): we expect this\nto lead to a potentially large decrease of the number of iterations needed for mixing, but\nwith an additional cost per iteration. We leave the discussion and exploration of this\ntradeoff to future work. Second, one could also consider schemes that adaptively modify\nthe probabilities _p_ _c_ ( _k, k_ _[′]_ ) in (9) in order to propose more often clusters with higher\noverlap (or higher acceptance rates of proposed swaps), thus reducing computational\nwaste associated to frequently proposing swaps across clusters with little overlap.\n\nFrom the theoretical point of view, it would be highly valuable to extend the scaling limit analysis to the posterior case. While interesting, we expect this to require\nworking with measure-valued processes and, more crucially, to require significant work\nin combining the MCMC analysis part with currently available results about posterior\nasymptotic behaviour of mixture models (Nguyen, 2013; Guha et al., 2021).\nIn this paper we stick to the case of a fixed number of components. A natural\ngeneralization regards the case of _K_ random or infinite (e.g. Dirichlet process mixtures,\nsee Ferguson (1973); Lo (1984)). This presents additional technical difficulties that we\nleave to future work: for example, since no upper bound is available on the number\nof components, it would be more natural to define a Markov chain over the full space\nof partitions of [ _n_ ]. Finally, mixture models are an instance of the broader framework\nof latent class models (Goodman, 1974) and it would be interesting to explore the\neffectiveness of the methodology developed here in such broader settings.\n\n### **References**\n\n\nAndrieu, C. and S. Livingstone (2021). Peskun–Tierney ordering for Markovian Monte\nCarlo: beyond the reversible scenario. _The Annals of Statistics 49_ (4), 1958–1981.\nAscolani, F., H. Lavenant, and G. Zanella (2024). Entropy contraction of the Gibbs\nsampler under log-concavity. _arXiv preprint arXiv:2410.00858_ .\n\n\n27\n\n\nBarton, N. H., A. M. Etheridge, and A. K. Sturm (2004). Coalescence in a random\nbackground. _Annals of Applied Probability 14_ (3), 754 – 785.\nBierkens, J. (2016). Non-reversible Metropolis-Hastings. _Statistics and Comput-_\n_ing 26_ (6), 1213–1228.\nBrown, S., P. A. Jenkins, A. M. Johansen, and J. Koskela (2021). Simple conditions\nfor convergence of sequential Monte Carlo genealogies with applications. _Electronic_\n_Journla of Probability 26_, 1 – 22.\nCeleux, G., M. Hurn, and C. P. Robert (2000). Computational and inferential difficulties with mixture posterior distributions. _Journal of the American Statistical_\n_Association 95_ (451), 957–970.\nChandra, N. K., A. Canale, and D. B. Dunson (2023). Escaping the curse of dimensionality in Bayesian model-based clustering. _Journal of Machine Learning Re-_\n_search 24_ (144), 1–42.\nChen, T.-L. and C.-R. Hwang (2013). Accelerating reversible Markov chains. _Statistics_\n_& Probability Letters 83_ (9), 1956–1962.\nDavis, M. H. (1984). Piecewise-deterministic Markov processes: A general class of\nnon-diffusion stochastic models. _Journal of the Royal Statistical Society: Series B_\n_(Methodological) 46_ (3), 353–376.\nDempster, A. P., N. M. Laird, and D. B. Rubin (1977). Maximum likelihood from\nincomplete data via the EM algorithm. _Journal of the royal statistical society: series_\n_B (methodological) 39_ (1), 1–22.\nDiaconis, P., S. Holmes, and R. M. Neal (2000). Analysis of a nonreversible Markov\nchain sampler. _Annals of Applied Probability_, 726–752.\nDiebolt, J. and C. P. Robert (1994). Estimation of finite mixture distributions through\nBayesian sampling. _Journal of the Royal Statistical Society: Series B (Methodologi-_\n_cal) 56_ (2), 363–375.\nEtheridge, A. (2011). _Some Mathematical Models from Population Genetics: Ecole_ _[´]_\n_D’Et´e de Probabilit´es de Saint-Flour XXXIX-2009_ _[´]_ . Springer.\nEthier, S. N. (1976). A class of degenerate diffusion processes occurring in population\ngenetics. _Communications on Pure and Applied Mathematics 29_ (5), 483–493.\nEthier, S. N. and T. G. Kurtz (1986). _Markov processes: characterization and conver-_\n_gence_ . John Wiley & Sons.\nFaulkner, M. F. and S. Livingstone (2024). Sampling algorithms in statistical physics:\na guide for statistics and machine learning. _Statistical Science 39_ (1), 137–164.\nFearnhead, P., J. Bierkens, M. Pollock, and G. O. Roberts (2018). Piecewise deterministic Markov processes for continuous-time Monte Carlo. _Statistical Science 33_ (3),\n386–412.\nFerguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. _Ann._\n_Stat. 1_ (2), 209–230.\nFr¨uhwirth-Schnatter, S. (2006). _Finite mixture and Markov switching models_, Volume\n425. Springer.\n\n\n28\n\n\nGagnon, P. and F. Maire (2024a). An asymptotic Peskun ordering and its application\nto lifted samplers. _Bernoulli 30_ (3), 2301–2325.\nGagnon, P. and F. Maire (2024b). Theoretical guarantees for lifted samplers. _arXiv_\n_preprint arXiv:2405.15952_ .\nGaitonde, J. and E. Mossel (2024). Comparison Theorems for the Mixing Times of\nSystematic and Random Scan Dynamics. _arXiv preprint arXiv:2410.11136_ .\nGelman, A., W. R. Gilks, and G. O. Roberts (1997). Weak convergence and optimal scaling of random walk Metropolis algorithms. _The Annals of applied probabil-_\n_ity 7_ (1), 110–120.\nGeweke, J. (2004). Getting it right: Joint distribution tests of posterior simulators.\n_Journal of the American Statistical Association 99_ (467), 799–804.\nGoodman, L. A. (1974). Exploratory latent structure analysis using both identifiable\nand unidentifiable models. _Biometrika 61_ (2), 215–231.\nGuha, A., N. Ho, and X. Nguyen (2021). On posterior contraction of parameters and\ninterpretability in Bayesian mixture modeling. _Bernoulli 27_ (4), 2159–2188.\nHe, B. D., C. M. De Sa, I. Mitliagkas, and C. R´e (2016). Scan order in Gibbs sampling:\nModels in which it matters and bounds on how much. _Advances in neural information_\n_processing systems 29_ .\nHobert, J. P., V. Roy, and C. P. Robert (2011). Improving the Convergence Properties of the Data Augmentation Algorithm with an Application to Bayesian Mixture\nModeling. _Statistical Science 26_ (3), 332–351.\nKhare, K. and H. Zhou (2009). Rates of convergence of some multivariate Markov\nchains with polynomial eigenfunctions. _Ann. App. Probab. 19_, 737–777.\nKoskela, J., P. A. Jenkins, A. M. Johansen, and D. Span´o (2020). Asymptotic genealogies of interacting particle systems with an application to sequential Monte Carlo.\n_The Annals of Statistics 1_, 560 – 583.\nLee, K., J.-M. Marin, K. Mengersen, and C. Robert (2009). Bayesian inference on finite\nmixtures of distributions. In _Perspectives in mathematical sciences I: Probability and_\n_statistics_, pp. 165–202. World Scientific.\nLevin, D. A. and Y. Peres (2017). _Markov chains and mixing times_, Volume 107.\nAmerican Mathematical Soc.\nLiu, J. S. (1994). The collapsed gibbs sampler in bayesian computations with applications to a gene regulation problem. _Journal of the American Statistical Associa-_\n_tion 89_ (427), 958–966.\nLo, A. Y. (1984). On a class of Bayesian nonparametric estimates: I. density estimates.\n_Ann. Stat. 12_ (1), 351–357.\nMarin, J.-M., K. Mengersen, and C. P. Robert (2005). Bayesian modelling and inference\non mixtures of distributions. _Handbook of statistics 25_, 459–507.\nMcLachlan, G. J., S. X. Lee, and S. I. Rathnayake (2019). Finite mixture models.\n_Annual review of statistics and its application 6_ (1), 355–378.\nNguyen, X. (2013). Convergence of latent mixing measures in finite and infinite mixture\n\n\n29\n\n\nmodels. _Annals of Statistics 41_ (1), 370–400.\nNishimura, A., D. B. Dunson, and J. Lu (2020). Discontinuous Hamiltonian Monte\nCarlo for discrete parameters and discontinuous likelihoods. _Biometrika 107_ (2),\n365–380.\nPower, S. and J. V. Goldman (2019). Accelerated sampling on discrete spaces with\nnon-reversible Markov processes. _arXiv preprint arXiv:1912.04681_ .\nRobert, C. P. (2007). _The Bayesian choice: from decision-theoretic foundations to_\n_computational implementation_, Volume 2. Springer.\nRoberts, G. O. and J. S. Rosenthal (2001a). Markov chains and de-initializing processes. _Scandinavian Journal of Statistics 28_ (3), 489–504.\nRoberts, G. O. and J. S. Rosenthal (2001b). Optimal scaling for various MetropolisHastings algorithms. _Statistical Science 16_ (4), 351–367.\nRoberts, G. O. and J. S. Rosenthal (2004). General state space Markov chains and\nMCMC algorithms. _Probability Surveys 1_, 20–71.\nRoberts, G. O. and J. S. Rosenthal (2015). Surprising convergence properties of some\nsimple Gibbs samplers under various scans. _International Journal of Statistics and_\n_Probability 5_ (1), 51–60.\nRousseau, J. and K. Mengersen (2011). Asymptotic behaviour of the posterior distribution in overfitted mixture models. _Journal of the Royal Statistical Society Series_\n_B: Statistical Methodology 73_ (5), 689–710.\nSchauer, M. and M. Wien¨obst (2024). Causal structure learning with momentum:\nSampling distributions over Markov Equivalence Classes. _Proceedings of Machine_\n_Learning Research 246_, 382–400.\nSmith, A. (2014). A Gibbs sampler on the n-simplex. _Annals of Applied Probabil-_\n_ity 24_ (1), 114–130.\nStephens, M. (2000). Dealing with label switching in mixture models. _Journal of the_\n_Royal Statistical Society: Series B (Statistical Methodology) 62_ (4), 795–809.\nTierney, L. (1998). A note on Metropolis-Hastings kernels for general state spaces.\n_Annals of Applied probability_, 1–9.\nVucelja, M. (2016). Lifting—a nonreversible Markov chain Monte Carlo algorithm.\n_American Journal of Physics 84_ (12), 958–968.\nWu, C. and C. P. Robert (2020). Coordinate sampler: a non-reversible Gibbs-like\nMCMC sampler. _Statistics and Computing 30_ (3), 721–730.\nYang, Y., M. J. Wainwright, and M. I. Jordan (2016). On the computational complexity\nof high-dimensional Bayesian variable selection. _The Annals of Statistics 44_ (6),\n2497–2532.\nZanella, G. (2020). Informed proposals for local MCMC in discrete spaces. _Journal of_\n_the American Statistical Association 115_ (530), 852–865.\nZhou, Q. and H. Chang (2023). Complexity analysis of Bayesian learning of highdimensional DAG models and their equivalence classes. _The Annals of Statis-_\n_tics 51_ (3), 1058–1085.\n\n\n30\n\n\nZhou, Q., J. Yang, D. Vats, G. O. Roberts, and J. S. Rosenthal (2022). Dimensionfree mixing for high-dimensional Bayesian variable selection. _Journal of the Royal_\n_Statistical Society: Series B (Statistical Methodology) 84_ (5), 1751–1784.\n\n### A Comparison between P NR and Q NR\n\n\nIn this section we consider the same setting of Section 6.1, where the target distribution\nis given in (15). We run both _P_ NR and _Q_ NR (with _s_ = 1) for 300 independent trials\nwith initialization uniformly at random. We consider _n_ = 1000, _K_ = 3 _,_ 10 _,_ 20 _,_ 50 and\n_**α**_ = (1 _,_ 1 _/_ ( _K −_ 1) _, . . .,_ 1 _/_ ( _K −_ 1), so that the marginal distribution on the proportion\nof the first component is a Dirichlet-Multinomial with parameters (1 _,_ 1) and thus close\nto a uniform distribution on (0 _,_ 1).\nFigure 6 plots the corresponding empirical marginal distribution obtained by the\nchains (black corresponds to _P_ NR and gray to _Q_ NR ). Even if both schemes correctly\nreach stationarity, it seems that _Q_ NR yields slower mixing as _K_ increases: this is particularly evident in the case _K_ = 50, where _Q_ NR remains close to the initial configuration.\n\n### **B Simulations for the Poisson kernel**\n\n\nHere we consider model (1) with _K_ = 3 and\n\n\n(27) _f_ _θ_ ( _y_ ) = Po( _y | θ_ ) _,_ _p_ 0 ( _θ_ ) = Gamma( _θ | β_ 1 _, β_ 2 ) _._\n\n\nIt is easy to show that the predictive distribution reads\n\n\n_p_ ( _Y_ _n_ +1 = _y | Y_ ) = Γ(Γ _β_ ( 1 _β_ + 1 + ~~[�]~~ [�] ~~_[n]_~~ _i_ =1 _[n]_ _i_ = _[Y]_ 1 _[i]_ [)Γ(] _[Y]_ _[i]_ [ +] _[y]_ [ + 1)] _[y]_ [)] ( _n_ +( _n β_ + 2 + 1) _β_ 2 ) _[β][β]_ [1][1] [+][+][�] ~~[�]~~ _i_ _[n]_ =1 _i_ ~~_[n]_~~ =1 _[Y][Y]_ _[i]_ _[i]_ [+] _[y]_ _[.]_\n\n\nWe consider _β_ 1 = _β_ 2 = 1 and we draw 300 independent samples from the model above\nwith _n_ = 1000, following the same procedure illustrated in Section 6.2. For each dataset\nwe run Algorithms 1 and 6, initialized uniformly at random, and we retain only the\nlast iteration.\n\nThe results of the simulations are similar to the ones of Section 6.2, as shown in\nFigure 7: again the non-reversible scheme is much closer to the prior distribution, while\n_P_ MG remains close to the initial configuration.\n\n\n31\n\n\n|Col1|Marginal distribution of the chains|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\n1 10 15 20 25 30 35 40 45 50\n\n|Col1|Marginal distribution of the chains|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\n\n1 10 15 20 25 30 35 40 45 50\n\n\n\n|Col1|Marginal distribution of the chains|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\n1 10 15 20 25 30 35 40 45 50\n\n|Col1|Marginal distribution of the chains|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\n\n1 10 15 20 25 30 35 40 45 50\n\n\n\nFigure 6: Plots of the marginal distribution of the proportion of the first component for the\nchains associated to _P_ NR (black) and _Q_ NR (gray) at every. From top left to bottom\nright, the plots refer to _K_ = 3 _,_ 10 _,_ 20 _,_ 50, where the target distribution is as in (15) with\n_**α**_ = (1 _,_ 1 _/_ ( _K −_ 1) _, . . .,_ 1 _/_ ( _K −_ 1) and _n_ = 1000.\n\n### **C Proofs**\n\n#### **C.1 General results about lifting and mixtures**\n\n\nIn order to prove results below, especially Theorem 3.1, we first need to generalize\nsome classical results about lifting of Markov chains (see e.g. Chen and Hwang, 2013;\nBierkens, 2016; Andrieu and Livingstone, 2021) to our mixture case, which can be\nseen as a way to construct ‘multi-dimensional’ lifted chains. We will make use of the\nfollowing classical lemma, which for example follows by results in Chen and Hwang\n(2013) as detailed below.\n\n\n**Lemma C.1.** _Let µ be a probability distribution on a finite space X_ _, P a µ-invariant_\n_and irreducible Markov transition matrix, P_ _[∗]_ _the µ-adjoint of P and K_ = ( _P_ + _P_ _[∗]_ ) _/_ 2 _._\n\n\n32\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Marginal**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n**Non reversible**\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n**Marginal distribution of the chains**\n\n\n1 10 30 50 70 90\n\n\n\nFigure 7: Left and center column: plot of the proportions of the first two components in the last\nof 100 iterations (after a thinning of size _n_ ) over 300 independent runs for _P_ MG (left)\nand _P_ NR (center). Right column: plot of the marginal distribution of the proportion\nof the first component at every 10 iterations (after thinning) for _P_ MG (gray) and _P_ NR\n(black). The rows refer to _α_ = 1 and _α_ = 0 _._ 1 and the target distribution is given by the\nposterior of model (1), with _f_ _θ_ ( _y_ ) as in (27) and _β_ 1 = _β_ 2 = 1.\n\n\n_Then Var_ ( _g, P_ ) _≤_ _Var_ ( _g, K_ ) _for all g_ : _X →_ R _._\n\n\n\n_Proof._ Consider the decomposition _P_ = _K_ + _Q_, _Q_ = [1]\n\n\n\n\n[1]\n\n2 _[P][ −]_ [1] 2\n\n\n\n_Proof._ Consider the decomposition _P_ = _K_ + _Q_, _Q_ = 2 _[P][ −]_ 2 _[P]_ _[ ∗]_ [. By construction,] _[ K]_\n\nis a _µ_ -reversible transition matrix. Moreover, by definition of adjoint we have that _Q_\nis antisymmetric with respect to _µ_, which means that _µ_ ( _x_ ) _Q_ ( _x, y_ ) = _−µ_ ( _y_ ) _Q_ ( _y, x_ ) for\nall _x, y ∈X_ . Finally, for every _x ∈X_ we have that\n\n\n\n� _Q_ ( _x, y_ ) = [1] 2\n\n_y∈X_\n\n\n\n2\n\n\n\n�\n\n\n\n2\n\n\n\n� _P_ ( _x, y_ ) _−_ [1] 2\n\n_y∈X_\n\n\n\n�\n\n\n\n� _P_ _[∗]_ ( _x, y_ ) = 0\n\n_y∈X_\n\n\n\nand thus each row of _Q_ sums up to zero. Therefore by Lemma 2 in Chen and Hwang\n(2013) we have that Var( _g, P_ ) = Var( _g, K_ + _Q_ ) _≤_ Var( _g, K_ ) for all _g_ : _X →_ R.\n\n\n**C.1.1** **Result with general notation**\n\n\nLet _π_ be a probability distribution on a finite space _C_ . Let _D ∈_ N, and ( _K_ _d,_ +1 ) _d∈{_ 1 _,...,D}_\nand ( _K_ _d,−_ 1 ) _d∈{_ 1 _,...,D}_ be Markov transition kernels on _C_ such that\n\n\n(28) _π_ ( _c_ ) _K_ _d,_ + ( _c, c_ _[′]_ ) = _π_ ( _c_ _[′]_ ) _K_ _d,−_ ( _c_ _[′]_ _, c_ ) for all _c ̸_ = _c_ _[′]_ and all _d_ = 1 _, . . ., D ._\n\n\n33\n\n\nDefine the Markov transition kernel on _C_\n\n\n\n(29) _K_ _R_ ( _c, c_ _[′]_ ) =\n\n\n\n_D_\n� _p_ _c_ ( _d_ ) _K_ _d_ � _c, c_ _[′]_ [�] _,_\n\n\n_d_ =1\n\n\n\nwhere _K_ _d_ = ( _K_ _d,_ + + _K_ _d,−_ ) _/_ 2 and _p_ _c_ are weights such that [�] _[D]_ _d_ =1 _[p]_ _[c]_ [(] _[d]_ [) = 1 for all] _[ c][ ∈C]_\nand\n\n\n(30) _p_ _c_ ( _d_ ) = _p_ _c_ _′_ ( _d_ ) if _K_ _d_ ( _c, c_ _[′]_ ) _>_ 0 _._\n\n\nDefine the Markov transition kernel on _C × {−_ 1 _,_ 1 _}_ _[D]_\n\n\n\n(31) _K_ _NR_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) =\n\n\n\n_D_\n� _p_ _c_ ( _d_ ) ( _F_ _d_ _K_ lift _,d_ _F_ _d_ ) (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) _,_\n\n\n_d_ =1\n\n\n\nwhere _F_ _d_ is the flipping operator defined as\n\n\n_F_ _d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = 1 ( _c_ = _c_ _[′]_ ) (1 _−_ _α_ ) 1 ( _v_ = _v_ _[′]_ ) + _α_ 1 ( _v_ _−d_ = _v_ _−_ _[′]_ _d_ _[, v]_ _d_ _[′]_ [=] _[ −][v]_ _[d]_ [)]\n� �\n\n\nfor some fixed _α ∈_ [0 _,_ 1] and\n\n\n(32) _K_ lift _,d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = _K_ _d,v_ _d_ � _c, c_ _[′]_ [�] _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ )\n\n\nwith\n\n\n(33) _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ ) = 1 ( _v_ _−d_ = _v_ _−_ _[′]_ _d_ [)] 1 ( _c ̸_ = _c_ _[′]_ ) 1 ( _v_ _d_ = _v_ _d_ _[′]_ [) +] [ 1] [(] _[c]_ [ =] _[ c]_ _[′]_ [)] [1] [(] _[v]_ _[d]_ [=] _[ −][v]_ _d_ _[′]_ [)] _._\n� �\n\n\nHere _α_ plays the role of a refresh rate. One could also think at the case _α_ = 0 for\nsimplicity, where _F_ _d_ becomes the identity operator and can thus be ignored.\n\n\n**Lemma C.2.** _Under_ (28) _-_ (33) _, we have that_\n\n\n_(a) K_ _R_ _is π-reversible._\n\n\n_(b) K_ _NR_ _is_ ˜ _π-invariant, with_ ˜ _π_ ( _c, v_ ) = _π_ ( _c_ )2 _[−][D]_ _._\n\n\n_(c) Var_ (˜ _g, K_ _NR_ ) _≤_ _Var_ ( _g, K_ _R_ ) _for all g_ : _C × {−_ 1 _,_ +1 _}_ _[D]_ _→_ R _and_ ˜ _g_ : _C →_ R _such_\n_that g_ ( _c, v_ ) = ˜ _g_ ( _c_ ) _for all_ ( _c, v_ ) _∈C × {−_ 1 _,_ +1 _}_ _[D]_ _._\n\n\n_Proof._ Consider first part (a). By (28), for every _c ̸_ = _c_ _[′]_ we have\n\n\n2 _π_ ( _c_ ) _K_ _d_ ( _c, c_ _[′]_ ) = _π_ ( _c_ ) _K_ _d,_ + ( _c, c_ _[′]_ ) + _π_ ( _c_ ) _K_ _d,−_ ( _c, c_ _[′]_ )\n\n= _π_ ( _c_ _[′]_ ) _K_ _d,−_ ( _c_ _[′]_ _, c_ ) + _π_ ( _c_ _[′]_ ) _K_ _d,_ + ( _c_ _[′]_ _, c_ ) = 2 _π_ ( _c_ _[′]_ ) _K_ _d_ ( _c_ _[′]_ _, c_ ) _._\n\n\n34\n\n\nand thus by (29) and (30) we have\n\n\n_̸_\n\n\n_̸_\n\n\n\n_π_ ( _c_ ) _K_ _R_ ( _c, c_ _[′]_ ) =\n\n\n=\n\n\n_̸_\n\n\n_̸_\n\n\n\n_D_\n� _p_ _c_ ( _d_ ) _π_ ( _c_ ) _K_ _d_ ( _c, c_ _[′]_ )\n\n\n_d_ =1\n\n\n_D_\n� _p_ _c_ _′_ ( _d_ ) _π_ ( _c_ _[′]_ ) _K_ _d_ ( _c_ _[′]_ _, c_ ) = _π_ ( _c_ _[′]_ ) _K_ _d_ ( _c_ _[′]_ _, c_ ) _,_\n\n\n_d_ =1\n\n\n_̸_\n\n\n_̸_\n\n\n\nmeaning that _K_ _R_ is _π_ -reversible.\nConsider now point (b). Let ( _c_ _[′]_ _, v_ _[′]_ ) _∈C × {−_ 1 _,_ +1 _}_ _[D]_ . If _v_ = _v_ _[′]_, by (32),\n_Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ ) = 1 ( _c ̸_ = _c_ _[′]_ ) and (28) we have\n\n\n_̸_\n\n\n_̸_\n\n\n\n� _π_ ˜( _c, v_ ) _K_ lift _,d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = �\n\n_c∈C_ _c∈C_\n\n\n_̸_\n\n\n_̸_\n\n\n\n�\n\n\n_̸_\n\n\n_̸_\n\n\n\n� _π_ ( _c_ ) _K_ _d,v_ _d_ ( _c, c_ _[′]_ ) _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ )2 _[−][D]_\n\n_c∈C_\n\n\n_̸_\n\n\n_̸_\n\n\n\n= � _π_ ( _c_ ) _K_ _d,v_ _d′_ [(] _[c, c]_ _[′]_ [)2] _[−][D]_\n\n_c_ = _̸_ _c_ _[′]_\n\n\n_̸_\n\n\n\n_̸_\n\n= � _π_ ( _c_ _[′]_ ) _K_ _d,−v_ _d′_ [(] _[c]_ _[′]_ _[, c]_ [)2] _[−][D]_ [ = ˜] _[π]_ [(] _[c]_ _[′]_ _[, v]_ _[′]_ [)] �1 _−_ _K_ _d,−v_ _d′_ [(] _[c]_ _[′]_ _[, c]_ _[′]_ [)] � _._\n\n_c_ = _̸_ _c_ _[′]_\n\n\n\n_̸_\n\n\n_̸_\n\n\nSimilarly, if _v_ _−d_ = _v_ _−_ _[′]_ _d_ [and] _[ v]_ _[d]_ [ =] _[ −][v]_ _d_ _[′]_ [, by] _[ Q]_ _[d,c,c]_ _[′]_ [(] _[v, v]_ _[′]_ [) =] [ 1] [(] _[c]_ [ =] _[ c]_ _[′]_ [) we have that]\n\n\n\n_̸_\n\n\n_̸_\n\n\n� _π_ ˜( _c, v_ ) _K_ lift _,d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = �\n\n_c∈C_ _c∈C_\n\n\n\n_̸_\n\n\n_̸_\n\n\n�\n\n\n\n_̸_\n\n\n_̸_\n\n\n� _π_ ( _c_ ) _K_ _d,v_ _d_ ( _c, c_ _[′]_ ) _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ )2 _[−][D]_\n\n_c∈C_\n\n\n\n_̸_\n\n\n_̸_\n\n\n= ˜ _π_ ( _c_ _[′]_ ) _K_ _d,−v_ _d′_ [(] _[c]_ _[′]_ _[, c]_ _[′]_ [)2] _[−][D]_ [ = ˜] _[π]_ [(] _[c]_ _[′]_ _[, v]_ _[′]_ [)] _[K]_ _[d,][−][v]_ _d_ _[′]_ [(] _[c]_ _[′]_ _[, c]_ _[′]_ [)] _[.]_\n\n\nSumming the two expressions above, and using the fact that _K_ lift _,d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = 0\nif _v_ _−d_ _̸_ = _v_ _−_ _[′]_ _d_ [, we have]\n\n\n� _π_ ˜( _c, v_ ) _K_ lift _,d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = ˜ _π_ ( _c_ _[′]_ _, v_ _[′]_ ) _,_\n\n\n_c,v_\n\n\nwhich implies that _K_ lift _,d_ is ˜ _π_ -invariant. Since _F_ _d_ is also trivially ˜ _π_ -invariant and composition of invariant kernels remains invariant, then _F_ _d_ _K_ lift _,d_ _F_ _d_ is ˜ _π_ -invariant. Finally,\nusing (30), we have\n\n\n\n_̸_\n\n\n_̸_\n\n\n_π_ ˜( _c, v_ ) ( _F_ _d_ _K_ lift _,d_ _F_ _d_ ) (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ ))\n\n_c,v_\n\n\n\n_̸_\n\n\n_̸_\n\n\n� _π_ ˜( _c, v_ ) _K_ _NR_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) =\n\n\n_c,v_\n\n\n=\n\n\n=\n\n\n\n_̸_\n\n\n_̸_\n\n\n_D_\n�\n\n\n_d_ =1\n\n\n_D_\n�\n\n\n\n_̸_\n\n\n_̸_\n\n\n_D_\n�\n\n\n\n_̸_\n\n\n_̸_\n\n\n� _p_ _c_ _′_ ( _d_ )˜ _π_ ( _c_ _[′]_ _, v_ _[′]_ ) = ˜ _π_ ( _c_ _[′]_ _, v_ _[′]_ ) _,_\n\n\n_d_ =1\n\n\n\n_̸_\n\n\n_̸_\n\n\n� _p_ _c_ ( _d_ )˜ _π_ ( _c, v_ ) ( _F_ _d_ _K_ lift _,d_ _F_ _d_ ) (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ ))\n\n\n_c,v_\n\n\n\n_̸_\n\n\n_̸_\n\n\n� _p_ _c_ _′_ ( _d_ ) �\n\n_d_ =1 _c,v_\n\n\n\n_̸_\n\n\n_̸_\n\n\n35\n\n\nand therefore _K_ _NR_ is ˜ _π_ -invariant.\nConsider now point (c). Let _K_ [¯] _R_ = ( _K_ _NR_ + _K_ _NR_ _[∗]_ [)] _[/]_ [2, where] _[ K]_ _NR_ _[∗]_ [is the ˜] _[π]_ [-adjoint]\nof _K_ _NR_ . Since _F_ _d_ _[∗]_ [=] _[ F]_ _[d]_ [, which is easy check by definition of] _[ F]_ _[d]_ [, we have that]\n( _F_ _d_ _K_ lift _,d_ _F_ _d_ ) _[∗]_ = _F_ _d_ _[∗]_ _[K]_ lift _[∗]_ _,d_ _[F]_ _[ ∗]_ _d_ [=] _[ F]_ _[d]_ _[K]_ lift _[∗]_ _,d_ _[F]_ _[d]_ [, which implies]\n\n\n\n_K_ _NR_ _[∗]_ [((] _[c, v]_ [)] _[,]_ [ (] _[c]_ _[′]_ _[, v]_ _[′]_ [)) =]\n\n\n\n_D_\n� _p_ _c_ ( _d_ ) � _F_ _d_ _K_ lift _[∗]_ _,d_ _[F]_ _[d]_ � (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ ))\n\n\n_d_ =1\n\n\n\nand thus\n\n\n_K_ ¯ _R_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = [1]\n\n2\n\n\n\n� _p_ _c_ ( _d_ ) ( _F_ _d_ _K_ lift _,d_ _F_ _d_ ) (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) + [1] 2\n\n_d_ =1\n\n\n\n_D_\n�\n\n\n\n2\n\n\n\n_D_\n� _p_ _c_ ( _d_ ) � _F_ _d_ _K_ lift _[∗]_ _,d_ _[F]_ _[d]_ � (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ ))\n\n\n_d_ =1\n\n\n\n=\n\n\n\n_D_\n� _p_ _c_ ( _d_ ) � _F_ _d_ _K_ [¯] _NR,d_ _F_ _d_ � (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ ))\n\n\n_d_ =1\n\n\n\nwith _K_ [¯] _NR,d_ := 2 [1]\n\n\n\n2 _[K]_ lift _[∗]_ _,d_ [. By (][28][) we have that for] _[ c]_ _[′]_ _[ ̸]_ [=] _[ c]_\n\n\n\n\n[1] [1]\n\n2 _[K]_ [lift] _[,d]_ [ +] 2\n\n\n\n_K_ lift _[∗]_ _,d_ [((] _[c, v]_ [)] _[,]_ [ (] _[c]_ _[′]_ _[, v]_ _[′]_ [)) =] _[π]_ [˜] _π_ ˜ [(] ( _[c]_ _c, v_ _[′]_ _[,][ v]_ _[′]_ ) [)] _[K]_ [lift] _[,d]_ [((] _[c]_ _[′]_ _[, v]_ _[′]_ [)] _[,]_ [ (] _[c, v]_ [))]\n\n\n\n= _[π]_ _π_ [(] ( _[c]_ _c_ _[′]_ ) [)] _[K]_ _[d,v]_ _d_ _[′]_ � _c_ _[′]_ _, c_ � _Q_ _d,c_ _′_ _,c_ ( _v_ _[′]_ _, v_ )\n\n\n\n= _K_ _d,−v_ _d′_ � _c, c_ _[′]_ [�] _Q_ _d,c_ _′_ _,c_ ( _v_ _[′]_ _, v_ ) = _K_ _d,−v_ _d_ � _c, c_ _[′]_ [�] _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ ) _,_\n\n\nwhere we used the definition of _Q_ _d,c_ _′_ _,c_ ( _v_ _[′]_ _, v_ ). For _c_ _[′]_ = _c_ we have that\n\n\n_K_ lift _[∗]_ _,d_ [((] _[c, v]_ [)] _[,]_ [ (] _[c, v]_ _[′]_ [)) =] _[ K]_ [lift] _[,d]_ [((] _[c, v]_ _[′]_ [)] _[,]_ [ (] _[c, v]_ [))]\n\n\n= _K_ _′_\n_d,v_ _d_ [(] _[c, c]_ [)] _[ Q]_ _[d,c,c]_ [(] _[v]_ _[′]_ _[, v]_ [) =] _[ K]_ _[d,][−][v]_ _[d]_ [ (] _[c, c]_ [)] _[ Q]_ _[d,c,c]_ _[′]_ [(] _[v, v]_ _[′]_ [)]\n\n\nwhere we used that _Q_ _d,c,c_ ( _v_ _[′]_ _, v_ ) _>_ 0 implies _v_ _d_ _[′]_ [=] _[ −][v]_ _[d]_ [. Thus]\n\n\n\n\n[1] � _c, c_ _[′]_ [�] _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ ) + [1]\n\n2 _[K]_ _[d,v]_ _[d]_ 2\n\n\n\n_K_ ¯ _NR,d_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) = [1]\n\n\n\n� _c, c_ _[′]_ [�] _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ )\n2 _[K]_ _[d,][−][v]_ _[d]_\n\n\n\n1\n\n=\n� 2\n\n\n\n\n[1] � _c, c_ _[′]_ [��] _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ )\n\n2 _[K]_ _[d,][−]_\n\n\n\n1\n\n� _c, c_ _[′]_ [�] + [1]\n2 _[K]_ _[d,]_ [+] 2\n\n\n\n= _K_ _d_ ( _c, c_ _[′]_ ) _Q_ _d,c,c_ _′_ ( _v, v_ _[′]_ ) _._\n\n\nLet now _g_ : _C × {−_ 1 _,_ +1 _}_ _[D]_ _→_ R and ˜ _g_ : _C →_ R such that _g_ ( _c, v_ ) = ˜ _g_ ( _c_ ). Then, since\n_F_ _d_ leaves the first coordinate invariate and _K_ _d_ does not depend on _v_, we have that\n\n� _F_ _d_ _K_ [¯] _NR,d_ _F_ _d_ _g_ � ( _c, v_ ) = � � _F_ _d_ _K_ [¯] _NR,d_ _F_ _d_ � (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ ))˜ _g_ ( _c_ _[′]_ )\n\n_c_ _[′]_ _,v_ _[′]_\n\n\n\n=\n�\n\n\n\n_K_ _d_ ( _c, c_ _[′]_ )˜ _g_ ( _c_ _[′]_ ) = _K_ _d_ _g_ ˜( _c_ ) _,_\n\n_c_ _[′]_\n\n\n\n36\n\n\nwhich implies\n\n\n\n_K_ ¯ _NR_ _g_ ( _c, v_ ) = � _K_ ¯ _NR_ (( _c, v_ ) _,_ ( _c_ _[′]_ _, v_ _[′]_ )) _g_ ( _c_ _[′]_ _, v_ _[′]_ ) = _K_ _R_ _g_ ˜( _c_ ) _._\n\n_c_ _[′]_ _,v_ _[′]_\n\n\n\nBy simple induction then _K_ [¯] _NR_ _[t]_ _[g]_ [(] _[c, v]_ [) =] _[ K]_ _R_ _[t]_ _[g]_ [˜][(] _[c]_ [) for every] _[ t]_ [.] It thus follows that\nVar( _g,_ _K_ [¯] _R_ ) = Var(˜ _g, K_ _R_ ). Point (c) then follows from Var( _g, K_ _NR_ ) _≤_ Var( _g,_ _K_ [¯] _R_ ),\nwhich is a consequence of Lemma C.1.\n\n#### **C.2 Proof of Lemma 2.1**\n\n\n_Proof._ Reversibility follows by Lemma C.2 (point (a)), with _K_ _R_ = _P_ R and ( _k, k_ _[′]_ ) _∈K_\nin place of _d ∈_ [ _D_ ]. The only delicate condition to verify is given by (30), which\nfollows since _P_ _k,k_ _′_ ( _c, c_ _[′]_ ) _>_ 0 implies that _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ ) = _n_ _k_ ( _c_ _[′]_ ) + _n_ _k_ _′_ ( _c_ _[′]_ ) and therefore\n_p_ _c_ ( _k, k_ _[′]_ ) = _p_ _c_ _′_ ( _k, k_ _[′]_ ).\nSince _π_ ( _c_ ) _>_ 0 for every _c ∈_ [ _K_ ] _[n]_, we have _π_ ( _c_ _i_ = _k | c_ _−i_ ) _>_ 0 for every _c_ _−i_ _∈_\n\n[ _K_ ] _[n][−]_ [1] . Combining this with the fact that _p_ _c_ ( _k, k_ _[′]_ ) _>_ 0 for every ( _k, k_ _[′]_ ) _∈K_ such\nthat _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ ) _>_ 0, we get that for every pair _c ̸_ = _c_ _[′]_ _∈_ [ _K_ ] _[n]_ there exists a _T ∈_ N\nand a sequence _c_ = _c_ [(0)] _, c_ [(1)] _, . . ., c_ [(] _[T]_ [)] = _c_ _[′]_ such that _P_ R � _c_ [(] _[t][−]_ [1)] _, c_ [(] _[t]_ [)] [�] _>_ 0 for every\n_t_ = 1 _, . . ., T −_ 1. Thus, _P_ NR is irreducible. It is also easy to see that _P_ R is aperiodic.\nUniform ergodicity then follows from Levin and Peres (2017, Theorem 4.9).\n\n#### **C.3 Proof of Lemma 2.2**\n\n\n_Proof._ Fix ( _k, k_ _[′]_ ) _∈K_ and let ( _k_ 1 _, k_ 2 ) be the pair sampled in the first two lines of\nAlgorithm 4. Then a draw from the latter will have ( _k, k_ _[′]_ ) as realization if and only if\n( _k_ 1 _, k_ 2 ) = ( _k, k_ _[′]_ ) or ( _k_ 1 _, k_ 2 ) = ( _k_ _[′]_ _, k_ ). By construction\n\n\n_n_ _k_ ( _c_ ) _n_ _k_ _′_ ( _c_ )\nP( _k_ 1 = _k, k_ 2 = _k_ _[′]_ ) = P( _k_ 1 = _k_ _[′]_ _, k_ 2 = _k_ ) =\n( _K −_ 1) _n_ _[,]_ ( _K −_ 1) _n_ _[,]_\n\n\nand thus _p_ _c_ ( _k, k_ _[′]_ ) = P( _k_ 1 = _k, k_ 2 = _k_ _[′]_ ) + P( _k_ 1 = _k_ _[′]_ _, k_ 2 = _k_ ), as desired.\n\n#### **C.4 Proof of Lemma 2.3**\n\n\n_Proof._ Invariance follows by Lemma C.2 (point (b)), with _K_ _NR_ = _P_ NR and ( _k, k_ _[′]_ ) _∈K_\nin place of _d ∈_ [ _D_ ]. The condition (30) is satisfied as shown in the proof of Lemma 2.1.\nConsider then irreducibility. For ease of notation, we use the notation _X_ = [ _K_ ] _[n]_ _× K_\nand _x_ = ( _c, v_ ) _∈X_ . If _π_ ( _c_ ) _>_ 0, this implies that _π_ ( _c_ _i_ = _k | c_ _−i_ ) _>_ 0 for every\n_c_ _−i_ _∈_ [ _K_ ] _[n][−]_ [1] . Combining this with the fact that _p_ _c_ ( _k, k_ _[′]_ ) _>_ 0 for every ( _k, k_ _[′]_ ) _∈K_\nsuch that _n_ _k_ ( _c_ ) + _n_ _k_ _′_ ( _c_ ) _>_ 0, we get that for every pair _x ̸_ = _x_ _[′]_ _∈X_ there exists a\n_T ∈_ N and a sequence _x_ = _x_ [(0)] _, x_ [(1)] _, . . ., x_ [(] _[T]_ [)] = _x_ _[′]_ such that _P_ NR � _x_ [(] _[t][−]_ [1)] _, x_ [(] _[t]_ [)] [�] _>_ 0 for\nevery _t_ = 1 _, . . ., T −_ 1. Thus, _P_ NR is irreducible. Moreover, if _ξ >_ 0 it is immediate to\n\n\n37\n\n\ndeduce that _P_ NR is aperiodic. Uniform ergodicity then follows from Levin and Peres\n(2017, Theorem 4.9).\n\n#### **C.5 Proof of Theorem 3.1**\n\n\n_Proof of Theorem 3.1._ The first inequality Var( _g, P_ NR ) _≤_ Var( _g, P_ R ) follows by point\n(c) of Lemma C.2, with _K_ _NR_ = _P_ NR and _K_ _R_ = _P_ R .\nIn order to prove the other inequality in (14) it suffices to show that\n\n\n1\n(34) _P_ R ( _c, c_ _[′]_ ) _≥_ _c ̸_ = _c_ _[′]_ _∈_ [ _K_ ] _[n]_ _,_\n_c_ ( _K_ ) _[P]_ [MG] [(] _[c, c]_ _[′]_ [)] _[,]_\n\n\nby, e.g., Theorem 2 in Zanella (2020).\nIn order to prove (34), fix _c_ and _c_ _[′]_ such that _c_ = ( _c_ _−i_ _, k_ ) and _c_ _[′]_ = ( _c_ _−i_ _, k_ _[′]_ ) with\n_i ∈_ [ _n_ ] and ( _k, k_ _[′]_ ) _∈K_ . Indeed for every other pair ( _c, c_ _[′]_ ) such that _c ̸_ = _c_ _[′]_ we have that\n_P_ R ( _c, c_ _[′]_ ) = _P_ MG ( _c, c_ _[′]_ ) = 0. By definition of _P_ MG and _P_ R we have\n\n\n_P_ MG ( _c, c_ _[′]_ ) = [1]\n\n_n_ _[π]_ [(] _[c]_ _[i]_ [ =] _[ k]_ _[′]_ _[ |][ c]_ _[−][i]_ [)] _[.]_\n\n\nand\n\n\n\n_,_\n�\n\n\n\n1\n_P_ R ( _c, c_ _[′]_ ) =\n2 _n_ _k_\n\n\n\n_n_ _k_ + _n_ _k_ _′_ _n_ _k_ _π_ ( _c_ _i_ = _k_ _[′]_ _| c_ _−i_ )\n1 _,_\n( _K −_ 1) _n_ [min] � _n_ _k_ _′_ + 1 _π_ ( _c_ _i_ = _k | c_ _−i_ )\n\n\n\nwhere _n_ _j_ = _n_ _j_ ( _c_ ) for every _j ∈_ [ _K_ ] and _n_ _k_ _≥_ 1 by definition of _c_ . Thus\n\n\n\n1 _n_ _k_ + _n_ _k_ _′_\n_P_ R ( _c, c_ _[′]_ ) =\n2( _K −_ 1) _n_ [min] � _n_ _k_\n\n\n\n\n_[k]_ [ +] _[ n]_ _[k]_ _[′]_ _π_ ( _c_ _i_ = _k_ _[′]_ _| c_ _−i_ )\n\n_n_ _k_ _′_ + 1 _π_ ( _c_ _i_ = _k | c_ _−i_ )\n\n\n\n_n_ _k_ _′_\n\n_,_ _[n]_ _[k]_ [ +] _[ n]_ _[k]_ _[′]_\n_n_ _k_ _n_ _k_ _′_ + 1\n\n\n\n_π_ ( _c_ _i_ = _k | c_ _−i_ )\n\n\n\n�\n\n\n\n1 _[|][ c]_ _[−][i]_ [)]\n_≥_ 1 _,_ _[π]_ [(] _[c]_ _[i]_ [ =] _[ k]_ _[′]_\n2( _K −_ 1) _n_ [min] � _π_ ( _c_ _i_ = _k | c_ _−i_ )\n\n\n\n�\n\n\n\n\n_[|][ c]_ _[−][i]_ [)] 1 1\n= _[π]_ [(] _[c]_ _[i]_ [ =] _[ k]_ _[′]_ min\n\n2( _K −_ 1) _n_ � _π_ ( _c_ _i_ = _k_ _[′]_ _| c_ _−i_ ) _[,]_ _π_ ( _c_ _i_ = _k | c_ _−i_ )\n\n\n\n\n_[c]_ _[i]_ [ =] _[ k]_ _[′]_ _[|][ c]_ _[−][i]_ [)] 1 1\n\nmin\n2( _K −_ 1) _n_ � _π_ ( _c_ _i_ = _k_ _[′]_ _| c_ _−i_ ) _[,]_ _π_ ( _c_ _i_ = _k | c_ _−i_ )\n\n\n\n�\n\n\n\n1 _π_ ( _c_ _i_ = _k_ _[′]_ _| c_ _−i_ ) 1\n_≥_ =\n2( _K −_ 1) _n_ 2( _K −_ 1) _[P]_ [MG] [(] _[c, c]_ _[′]_ [)] _[,]_\n\n\n\nwhich is exactly (34).\n\n#### **C.6 Proof of Proposition 3.3**\n\n\n_Proof._ Let _P_ [˜] MG be the _π_ ( _c,_ _**w**_ _,_ _**θ**_ )-reversible Markov kernel on [ _K_ ] _[n]_ _×_ Θ _[K]_ _×_ ∆ _K−_ 1 that,\ngiven ( _c_ [(] _[t]_ [)] _,_ _**w**_ [(] _[t]_ [)] _,_ _**θ**_ [(] _[t]_ [)] ) generates ( _c_ [(] _[t]_ [+1)] _,_ _**w**_ [(] _[t]_ [+1)] _,_ _**θ**_ [(] _[t]_ [+1)] ) by\n\n\n_c_ [(] _[t]_ [+1)] _∼_ _P_ MG _c_ [(] _[t]_ [)] _, ·_ _,_ ( _**w**_ [(] _[t]_ [+1)] _,_ _**θ**_ [(] _[t]_ [+1)] ) _∼_ _π_ _**w**_ _,_ _**θ**_ _| c_ = _c_ [(] _[t]_ [)] [�] _._\n� � �\n\n\n38\n\n\nBy construction, Var( _g, P_ MG ) = Var( _g,_ _P_ [˜] MG ) for any _g_ that is a function of _c_ alone,\nbecause the marginal process on [ _K_ ] _[n]_ induced by _P_ [˜] MG is a Markov chain with kernel\n_P_ MG .\nWe now compare _P_ [˜] MG and _P_ CD . Let\n\n\n_⟨f, g⟩_ _π_ = _f_ ( _x_ ) _g_ ( _x_ ) _π_ (d _x_ ) _,_ _X_ = [ _K_ ] _[n]_ _×_ Θ _[K]_ _×_ ∆ _K−_ 1 _,_\n� _X_\n\n\nbe the _L_ [2] ( _π_ ) inner product. Then for any _g ∈_ _L_ [2] ( _π_ ) and ( _c,_ _**w**_ _,_ _**θ**_ ) _∼_ _π_ we have\n\n\n_⟨_ ( _I −_ _P_ CD ) _g, g⟩_ _π_\n\n\n\n_n_ 1\n� E[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ _−i_ _,_ _**w**_ _,_ _**θ**_ )] + _n_ + 1 [E][[Var(] _[g]_ [(] _[c,]_ _**[ w]**_ _[,]_ _**[ θ]**_ [)] _[ |][ c]_ [)]]\n\n_i_ =1\n\n\n\n_n_ 1\n� E[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ _−i_ )] + _n_ + 1\n\n_i_ =1\n\n\n\n_n_\n\nE[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ _−i_ )]\n\n� _n_\n\n_i_ =1\n\n\n\n(35)\n\n\n\n1\n\n=\n_n_ + 1\n\n\n1\n_≤_\n_n_ + 1\n\n\n\n= [1]\n\n\n_n_\n\n\n\n_n_\n� E[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ _−i_ )] = _⟨_ ( _I −_ _P_ [˜] MG ) _g, g⟩_ _π_ _,_\n\n\n_i_ =1\n\n\n\nwhere the middle inequality follows from the fact that\n\n\nE[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ )] = E [E[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ ) _| c_ _−i_ ]] _≤_ E[Var( _g_ ( _c,_ _**w**_ _,_ _**θ**_ ) _| c_ _−i_ )] _,_\n\n\nfor every _i_ = 1 _, . . ., n_ by the law of total variance. We thus have _⟨_ ( _I −_ _P_ CD ) _g, g⟩_ _π_ _≤_\n_⟨_ ( _I −_ _P_ [˜] MG ) _g, g⟩_ _π_ for every _g ∈_ _L_ [2] ( _π_ ), which implies Var( _g,_ _P_ [˜] MG ) _≤_ Var( _g, P_ CD ) for\nall _g_ (see e.g. the proof of Tierney, 1998, Theorem 4). We thus have Var( _g, P_ MG ) =\nVar( _g,_ _P_ [˜] MG ) _≤_ Var( _g, P_ CD ) for all _g_ for functions _g_ that depend only on _c_ .\n\n#### **C.7 Proof of Theorem 4.1**\n\n\n_Proof._ By (15) for every _**x**_ _∈_ ∆ _K−_ 1 we have that, as _n →∞_,\n\n\n\nE [ _X_ _t_ +1 _,k_ _−_ _x_ _k_ _|_ _**X**_ _t_ = _**x**_ ] = [1] _[ −]_ _[x]_ _[k]_\n\n\n\n\n_[k]_ _|_ _**α**_ _| −_ _α_ _k_ + _n_ (1 _−_ _x_ _k_ )\n\n\n_n_ _**α**_ + _n −_ 1\n\n\n\n\n_[x]_ _[k]_ _α_ _k_ + _nx_ _k_\n\n_n_ _|_ _**α**_ _|_ + _n −_ 1 _[−]_ _[x]_ _n_ _[k]_\n\n\n\n_|_ _**α**_ _|_ + _n −_ 1\n\n\n\n_α_ _k_ _−_ _|_ _**α**_ _|x_ _k_\n= [2]\n_n_ ( _|_ _**α**_ _|_ + _n −_ 1) [=] _n_ [2]\n\n\n\n� _α_ 2 _k_\n\n\n\n2 _[k]_ [+] _[ o]_ [(1)] �\n\n\n\n_k_\n\n2 _[−|]_ _**[α]**_ _[|]_ _[x]_ 2 _[k]_\n\n\n\nand\n\n\n\nE ( _X_ _t_ +1 _,k_ _−_ _x_ _k_ ) [2] _|_ _**X**_ _t_ = _**x**_ = [1] _[ −]_ _[x]_ _[k]_\n� � _n_ [2]\n\n\n\n_|_ _**α**_ _| −_ _α_ _k_ + _n_ (1 _−_ _x_ _k_ )\n\n_[k]_\n\n_n_ [2] _**α**_ + _n −_ 1\n\n\n\n\n_[x]_ _[k]_ _α_ _k_ + _nx_ _k_\n\n_n_ [2] _|_ _**α**_ _|_ + _n −_ 1 [+] _[ x]_ _n_ _[k]_ [2]\n\n\n\n_|_ _**α**_ _|_ + _n −_ 1\n\n\n\n= [2]\n\n\n\n\n[[] _[x]_ _[k]_ [(1] _[ −]_ _[x]_ _[k]_ [) +] _[ o]_ [(1)]]\n_n_ [2]\n\n\n\n39\n\n\nand\n\n\n\n\n_[x]_ _[k]_ _α_ _k_ _′_ + _nx_ _k_ _′_ _[x]_ _[k]_ _[′]_\n\n_n_ [2] _|_ _**α**_ _|_ + _n −_ 1 [+] _n_ [2]\n\n\n\nE �( _X_ _t_ +1 _,k_ _−_ _x_ _k_ ) � _X_ _t_ +1 _,k_ _′_ _−_ _x_ _k_ _′_ [�] _|_ _**X**_ _t_ = _**x**_ � = _[−][x]_ _[k]_\n\n[2]\n\n\n\n\n_[k]_ _[′]_ _α_ _k_ + _nx_ _k_\n\n_n_ [2] _|_ _**α**_ _|_ + _n −_ 1\n\n\n\n= [2]\n\n[[] _[−][x]_ _[k]_ _[x]_ _[k]_ _[′]_ [ +] _[ o]_ [(1)]] _[,]_\n_n_ [2]\n\n\n\nand _n_ [2] E ( _X_ _t_ +1 _,k_ _−_ _x_ _k_ ) [3] _|_ _**X**_ _t_ = _**x**_ = _o_ (1) for _k ̸_ = _k_ _[′]_ _∈_ [ _K_ ]. By a second-order Taylor\n� �\nexpansion, this means that\n\n\nsup _|_ E [ _g_ ( _**X**_ _t−_ 1 ) _|_ _**X**_ _t_ = _**x**_ ] _−_ _g_ ( _**x**_ ) _−Ag_ ( _**x**_ ) _| →_ 0 _,_\n_**x**_ _∈_ ∆ _K_ +1\n\n\nas _n →∞_ for every _g_ twice differentiable real-valued function. The result then follows\nby Corollary 8 _._ 9 in (Ethier and Kurtz, 1986, Chapter 4).\n\n#### **C.8 Proof of Theorem 4.4**\n\n\n_Proof._ Fix _**z**_ = ( _**x**_ _,_ _**v**_ ) _∈_ _E_ _M_ _× V_ . Notice that\n\n\nE _X_ _t_ [(] +1 _[M]_ [)] _,k_ _[−]_ _[x]_ _[k]_ _[ |]_ _**[ X]**_ _t_ [(] _[M]_ [)] = _**x**_ _,_ _**V**_ _t_ [(] _[M]_ [)] = _**v**_\n� �\n\n\n\n_K −_ 1 _[α]_ [(] _**[x]**_ _[, k]_ _[′]_ _[, k]_ [)]\n\n\n\n\n\n\n\n�\n _k_ _[′]_ : _v_ _k′,k_\n\n\n\n_k_ _[′]_ : _v_ _k′,k_ =+1\n\n\n\n= 1 _−_ _[ξ]_\n� _n_\n\n\n\n1\n� _n_\n\n\n\n_x_ _k_ + _x_ _k_ _′_\n\n\n\n\n\n\n\n\n\n_xK_ _k_ + _− x_ 1 _k_ _′_ _[α]_ [(] _**[x]**_ _[, k, k]_ _[′]_ [)] _[ −]_ �\n\n_k_ _[′]_ : _v_ _k′,k_ = _−_ 1\n\n\n\n1\n\n_−_ _[ξ]_\n\n\n_n_ _n_\n\n\n\n\n\n\n\n�\n _k_ _[′]_ : _v_ _k′,k_\n\n\n\n_x_ _k_ + _x_ _k_ _′_\n\n\n\n_K −_ 1 _[α]_ [(] _**[x]**_ _[, k, k]_ _[′]_ [)]\n\n\n\n\n\n\n\n_xK_ _k_ + _− x_ 1 _k_ _′_ _[α]_ [(] _**[x]**_ _[, k]_ _[′]_ _[, k]_ [)] _[ −]_ �\n\n\n\n_k_ _[′]_ : _v_ _k′,k_ =+1\n\n\n\n _,_\n\n\n\nwhere\n\n\n\n_α_ _k_ + _nx_ _k_\n_α_ ( _**x**_ _, k, k_ _[′]_ ) = min 1 _,_\n� � _nx_ _k_ + 1\n\n\n\n_k_ _[′]_ : _v_ _k′,k_ = _−_ 1\n\n\n_nx_ _k_ _′_\n�� _α_ _k_ _′_ + _nx_ _k_ _′_ _−_ 1 ��\n\n\n\n= 1 _−_ _[β]_ [(] _[x]_ _[k]_ _[, x]_ _[k]_ _[′]_ _[, v]_ _[k]_ _[′]_ _[,][k]_ [)]\n\n\n\n\n_[k]_ _[′]_ _[, v]_ _[k]_ _[′]_ _[,][k]_ [)] 1\n\n+ _o_\n_n_ � _n_\n\n\n\n_n_\n\n\n\n_,_\n�\n\n\n\nfrom which we deduce that\n\n\n\n(36) E� _X_ _t_ [(] +1 _[M]_ [)] _,k_ _[−]_ _[x]_ _[k]_ _[ |]_ _**[ X]**_ _t_ [(] _[M]_ [)] = _**x**_ _,_ _**V**_ _t_ [(] _[M]_ [)] = _**v**_ � = [Φ] _[k]_ _n_ [(] _**[z]**_ [)]\n\n\n\n\n[(] _**[z]**_ [)] 1\n\n+ _o_\n_n_ � _n_\n\n\n\n_n_\n\n\n\n_._\n�\n\n\n\nSimilarly we get that\n\n\n1\n(37) E�� _X_ _t_ [(] +1 _[M]_ [)] _,k_ _[−]_ _[x]_ _[k]_ �� _X_ _t_ [(] +1 _[M]_ [)] _,k_ _[′]_ _[ −]_ _[x]_ _[k]_ _[′]_ � _|_ _**X**_ _t_ [(] _[M]_ [)] = _**x**_ _,_ _**V**_ _t_ [(] _[M]_ [)] = _**v**_ � = _o_ � _n_\n\n\n40\n\n\n\n_,_\n�\n\n\nfor every ( _k, k_ _[′]_ ) _∈_ [ _K_ ] [2] . Moreover\n(38)\n\nE _g_ _**x**_ _,_ _**V**_ _t_ [(] +1 _[M]_ [)] _−_ _g_ ( _**x**_ _,_ _**v**_ ) _|_ _**X**_ _t_ [(] _[M]_ [)] = _**x**_ _,_ _**V**_ _t_ [(] _[M]_ [)] = _**v**_\n� �\n� �\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n2 _ξ_\n+ _α_ ( _**x**_ _, k, k_ _[′]_ ) 1 _−_ _[ξ]_\n� � _n_ � _n_\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n2\n+ _α_ ( _**x**_ _, k, k_ _[′]_ ) 1 _−_ _[ξ]_\n� � _n_\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_ξ_\n� _n_\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n1 _−_ _α_ ( _**x**_ _, k, k_ _[′]_ )� [�] 1 _−_ _[ξ]_\n�� _n_\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n=\n�\n\n\n_k_ = _̸_ _k_ _[′]_\n\n\n=\n�\n\n_̸_\n\n\n_̸_\n\n\n\n_x_ _k_ + _x_ _k_ _′_\n� _g_ ( _**z**_ **(** _**k,k**_ _**′**_ **)** ) _−_ _g_ ( _**z**_ )�\n\n2( _K −_ 1)\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n\n_n_\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n\n_ξ_\n+ �1 _−_ _α_ ( _**x**_ _, k_ _[′]_ _, k_ )� [�]\n\n_n_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n2 + _α_ ( _**x**_ _, k_ _[′]_ _, k_ ) _[ξ]_\n� _n_\n\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n\n_g_ ( _**z**_ **(** _**k,k**_ _**′**_ **)** ) _−_ _g_ ( _**z**_ )\n\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n\n�\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n\n_k_ = _̸_ _k_ _[′]_\n\n= _[λ]_ [(] _**[z]**_ [)]\n\n\n_n_\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\n1\n_k_ � = _̸_ _k_ _[′]_ _q_ ( _k, k_ _[′]_ ) � _g_ ( _**z**_ **(** _**k,k**_ _**′**_ **)** ) _−_ _g_ ( _**z**_ )� + _o_ � _n_\n\n\n\n_̸_\n\n\n**)** ) _−_ _g_ ( _**z**_ ) _x_ _k_ + _x_ _k_ _′_ 1\n\n� _β_ ( _x_ _k_ _, x_ _k_ _′_ _, v_ _k,k_ _′_ ) + 2 _ξ_ � + _o_\n_n_ 2( _K −_ 1) � _n_\n\n_̸_\n\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\n�\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_n_\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_._\n�\n\n_̸_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\nBy a Taylor expansion we have that\n\n\nE _g_ ( _**Z**_ _t_ [(] +1 _[M]_ [)] _[ |]_ _**[ Z]**_ _t_ [(] _[M]_ [)] = _**z**_ _−_ _g_ ( _**z**_ ) = E _g_ ( _**x**_ _,_ _**V**_ _t_ [(] +1 _[M]_ [)] [)] _[ |]_ _**[ Z]**_ _t_ [(] _[M]_ [)] = _**z**_ _−_ _g_ ( _**z**_ )\n� � � �\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n_n_\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n+\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n_K_\n�\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\n_∂_ 1\n\n� _k_ =1 E �� _X_ _t_ [(] +1 _[M]_ [)] _,k_ _[−]_ _[x]_ _[k]_ � _|_ _**Z**_ _t_ [(] _[M]_ [)] = _**z**_ � _∂z_ 1 _,k_ _g_ ( _**z**_ ) + _o_ � _n_\n\n\n\n_̸_\n\n\n1 _−_ _[ξ]_\n� _n_ ��\n\n\n_̸_\n\n\n_̸_\n\n\n_,_\n�\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\nthat, combined with (36), (37) and (38), implies\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\nsup\n_**z**_ _∈E_ _M_ _×_ [ _K_ ] [2]\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\nE _g_ ( _**Z**_ _t_ [(] +1 _[M]_ [)] [)] _[ |]_ _**[ Z]**_ _[t]_ [ =] _**[ z]**_ _−_ _g_ ( _**z**_ ) _−Bg_ ( _**z**_ ) _→_ 0 _,_\n��� � � ���\n\n\n\n_̸_\n\n\n_̸_\n\n\n_̸_\n\n\nas _n →∞_ for every _g_ : ∆ _K−_ 1 _×_ [ _K_ ] [2] _→_ R twice continuously differentiable in the\nfirst argument. The result then follows by Corollary 8 _._ 9 in Ethier and Kurtz (1986,\nChapter 4).\n\n\n41\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03224v1",
      "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance\n  of Latent Ensembles",
      "authors": [
        "Dong Lao",
        "Yuxiang Zhang",
        "Haniyeh Ehsani Oskouie",
        "Yangchao Wu",
        "Alex Wong",
        "Stefano Soatto"
      ],
      "summary": "We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks.",
      "published": "2025-10-03T17:57:25Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03224v1"
    },
    "text": "Preprint version. Under review.\n\n## - T EST -T IME D EFENSE A GAINST A DVERSARIAL A T - TACKS VIA S TOCHASTIC R ESONANCE OF L ATENT E N\n\n#### SEMBLES\n\n\n**Dong Lao** **[1,2]** **Yuxiang Zhang** **[2]** **Haniyeh Ehsani Oskouie** **[2]** **Yangchao Wu** **[2]**\n\n**Alex Wong** **[3]** **Stefano Soatto** **[2]**\n\n1 LSU Vision Lab 2 UCLA Vision Lab 3 Yale Vision Lab\n1 dong.lao@lsu.edu\n2 _{_ zhangbrandon102,haniyehehsani,wuyangchao1997,soatto _}_ @ucla.edu\n3 alex.wong@yale.edu\n\n\nA BSTRACT\n\n\nWe propose a test-time defense mechanism against adversarial attacks: imperceptible image perturbations that significantly alter the predictions of a model. Unlike existing methods that rely on feature filtering or smoothing, which can lead to\ninformation loss, we propose to “combat noise with noise” by leveraging stochastic resonance to enhance robustness while minimizing information loss. Our approach introduces small translational perturbations to the input image, aligns the\ntransformed feature embeddings, and aggregates them before mapping back to the\noriginal reference image. This can be expressed in a closed-form formula, which\ncan be deployed on diverse existing network architectures without introducing additional network modules or fine-tuning for specific attack types. The resulting\nmethod is entirely training-free, architecture-agnostic, and attack-agnostic. Empirical results show state-of-the-art robustness on image classification and, for the\nfirst time, establish a generic test-time defense for dense prediction tasks, including stereo matching and optical flow, highlighting the method’s versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our method\nrecovers up to 68.1% of the accuracy loss on image classification, 71.9% on stereo\nmatching, and 29.2% on optical flow under various types of adversarial attacks.\n\n\n1 I NTRODUCTION\n\n\nMost deep neural networks in use today are deterministic maps from a fixed-size input to a fixedsize feature vector. In auto-regressive Transformer models, that vector encodes the next element\n(token) in the input sequence. Similarly, in convolutional architectures, that vector may encode the\ninput data. In either case, the output vector is often highly sensitive to perturbations of the input,\nand one can intentionally choose these imperceptible perturbations _adversarially_ so as to maximize\nthe change in the output Goodfellow et al. (2014). In some cases, the same perturbation can even\nbe disruptive for a large number of possible inputs Moosavi-Dezfooli et al. (2017), exploiting the\nconvoluted geometry of the decision boundary imposed by such trained models Tram`er et al. (2017).\nThis spurious sensitivity could be exploited adversarially to disrupt the operation of a model.\n\n\nFrom a classical perspective of signal processing, adversarial perturbations of images appear as\nsmall high-frequency “noise” resembling _aliasing_ artifacts. These are imperceptible since the human visual system easily discounts them on account of their poor fit to the ‘ecological statistics’ of\nnatural images Gibson (2014). Classical sampling theory prescribes anti-aliasing by low-pass filtering the data, removing information along with the artifacts. Low-pass filtering consists of spatial\naveraging of the perturbed data with respect to a chosen kernel, typically a Gaussian or a constant\n(“pillbox”). Alternatively, one could “denoise” the embeddings by averaging output vectors, also\na lossy operation. The choice of the kernel should match the statistics of the perturbations, which\nsets up a cat-and-mouse game where the adversary can easily modify the perturbations to bypass the\nanti-aliasing filter, namely _adaptive_ attacks Tramer et al. (2020), and the model needs to constantly\nbe fine-tuned to “anti-alias” new forms of adversarial perturbations. In the context of deep neural\n\n\n1\n\n\nPreprint version. Under review.\n\n\nnetworks, existing defense methods that manipulate feature representations Xie et al. (2019); Bai\net al. (2021); Yan et al. (2021); Kim et al. (2023) fundamentally adhere to this paradigm in principle. Despite substantial engineering efforts, these methods remain inherently vulnerable to adaptive\nattacks because they rely on pre-defined filtering strategies that are fixed at inference time.\n\n\n**Desiderata:** To break this cycle, we advocate a method to mitigate the effect of adversarial perturbation that (i) operates at test time, without the need to update the model weights, and (ii) does not\nentail information loss associated with direct or indirect spatial filtering. In addition, it would be\nideal if this method could (iii) be applied to existing network architectures without modifications,\nand (iv) be agnostic to the specific type of adversarial perturbation.\n\n\n**Stochastic resonance** is a technique that resolves a quantized signal below the quantization level,\nby quantizing and ensembling perturbed versions of the signal Benzi et al. (1981). It has been used\nextensively in cochlear implants, where power constraints limit the resolution of the digital circuitry\nStocks et al. (2002). It has also been used to ‘super-resolve’ Vision Transformer embeddings, where\nentire patches are encoded into a vector, which is computed at a coarsely subsampled grid Lao et al.\n(2024). In this paper, instead, we use Stochastic Resonance for the opposite purpose, _not_ to superresolve the quantized signal, but to perform latent ensembling to remove the effects of adversarial\nperturbations in the embedding.\n\n\nRather than averaging the data, or averaging their embedding as in classical denoising, _we average_\n_transformed embeddings_ in latent space. This averaging is performed over small transformations\nsampled at random or deterministically from the group of planar translations, by computing the\nencoding of the transformed image, and then mapping the encoding back through the push-forward\nof the inverse transformation. This process can be expressed as a single formula in equation 1. Since\nthe embedding is typically computed on a coarse grid, but the transformations are sampled on the\nnative lattice of the image, the resulting embedding is free of spatial averaging artifacts. As with\nother uses of Stochastic Resonance, the effect is seemingly paradoxical as _we combat noise with_\n_noise_ : We apply purposeful perturbations to eliminate the effect of adversarial perturbations.\n\n\nOur method can be thought of as marginalizing the translation group in latent space with respect to\na chosen prior, which is the only design choice in our method. We choose the simplest, which is the\nconstant prior. The purposeful perturbations alter the spatial sampling, and the implicit ensembling\nin latent space averages out the effect of sampling artifacts, thwarting the effect of adversarial perturbations. It is as if we were given multiple images with different ‘noise’, except that the noise in\nquestion is not the adversarial perturbation, but the splinters of adversarial perturbations obtained by\ndifferent spatially quantized versions of the perturbed image, due to the translational perturbations,\nwhich are then averaged out by the latent ensembling.\n\n\n**Outcomes:** Our method fulfills the desiderata (i)-(iv) laid out earlier: (i) It does not require training\nor fine-tuning; (ii) it minimizes information loss by latent ensembling of perturbed embeddings; (iii)\nit can be applied to different network architectures and tasks, _including_ networks already equipped\nwith different defense techniques like adversarial training, and (iv) is agnostic to the specific perturbation. To measure the effectiveness of our method in mitigating the effects of adversarial perturbations, we test it on three vastly different tasks, including image classification, and two other dense\nprediction tasks: stereo matching and optical flow. where we are the first to show a significant and\nconsistent improvement in robustness to various adversarial attacks.\n\n\nOne could argue that there is still a cat-and-mouse game in our setting, if the adversary knows\nour technique and tailors the adversarial perturbations to bypass it. To assess this risk, we conduct\n“worst-case” adaptive tests to measure the performance of our method under adaptive attacks when\nthe attacker knows the exact defense strategy, thus the adversarial perturbation is designed to maximally disrupt the result end-to-end, _including_ our stochastic resonance. Our results show that the\nmethod is resistant to breaking even when the adversary optimizes adaptively through it end-to-end.\n\n\n2 R ELATED W ORK\n\n\nThe literature on adversarial attack and defense is extensive. We highlight some of the advances.\n\n\n**Adversarial Training as Defense.** Adversarial training increases the robustness of the model by\ntraining it with adversarially augmented images. The popular attack methods used are Fast Gradient\n\n\n2\n\n\nPreprint version. Under review.\n\n\n\n**Noisy Features**\n\n\n\n**Attacked** **Image** _Encode_ 𝛷 **“** Boxer **”**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### **…** **…**\n\n\n\n**High Fidelity Features**\n\n\n\n“ Panda ”\n\n\n\n\n\n\n\n\n\n\n\n_& Inverse_\n\n|{𝑔−1}𝑛<br>𝑖∗ 𝑖=1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|_Upsample_<br>|_Upsample_<br>|_Upsample_<br>|_Upsample_<br>|_Upsample_<br>|_Upsample_<br>|\n\n\n\n_Transformations_\n\n\n|Attacked<br>Image|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|_Transformations_<br>{𝑔𝑖}𝑖=1<br>𝑛|\n|**+ **<br>**Stochastic**<br>**Resonance**<br>|||||||\n|**+ **<br>**Stochastic**<br>**Resonance**<br>|||||||\n\n\n\nFigure 1: **Defense against adversarial attacks via stochastic resonance.** _Neural networks are highly sensi-_\n_tive to small perturbations in the input space, which adversarial attacks exploit to manipulate network outputs._\n_Conventional defense strategies primarily focus on filtering out unreliable features or denoising either the in-_\n_put or the features. Instead of removing noise, we propose a novel defense by introducing noise. Based on_\n_stochastic resonance, controlled transformations are introduced to the input. Features are then aggregated after_\n_inverting these transformations. The resulting method can be applied exclusively at inference time, requires no_\n_training, and is compatible with diverse network architectures. Notably, it not only improves robustness against_\n_adversarial attacks but also increases the difficulty of crafting successful adversarial examples, even when the_\n_attacker is fully aware of whether and how stochastic resonance is being used (i.e. adaptive attacks)._\n\n\nSign Method (FGSM) Goodfellow et al. (2014), and Projected Gradient Descent (PGD) Madry et al.\n(2017). ALP Kannan et al. (2018) Minimizes the difference between the logits of pairs of clean and\nadversarially augmented images. TRADES Zhang et al. (2019) decomposes prediction error for adversarial examples into natural error and boundary error to improve adversarial robustness at the cost\nof accuracy. MART Wang et al. (2019) improves adversarial robustness by considering misclassified\nnatural examples during training. Subsequent work Cai et al. (2018); Zhang et al. (2020); Wang &\nWang (2022); Jin et al. (2022); Ghiasvand et al. (2024) improves adversarial robustness with curriculum learning Bengio et al. (2009), model ensembling, second order statistics, and gradient tracking.\nOn the other hand, some methods learn robust feature representation through a modified architecture\nor feature manipulation. Galloway et al. (2019); Benz et al. (2021); Wang et al. (2022) investigate\nthe effect of batch normalization on adversarial robustness. Dhillon et al. (2018); Madaan et al.\n(2020) prunes certain activations in the network that are susceptible to adversarial attacks. Xiao\net al. (2019) keep k-features with the largest magnitude and deactivate everything else. Zoran et al.\n(2020) uses an attention mask to highlight robust regions on the feature. Feature Denoising (FD) Xie\net al. (2019) uses classical denoising techniques to deactivate abnormal activations. Bai et al. (2021);\nYan et al. (2021) proposed Channel Activation Suppression (CAS) and Channel-wise Importancebased Feature Selection (CIFS) to deactivate feature channels that are vulnerable to attacks. Kim\net al. (2023) improves the robustness with Feature Separation and Recalibration (FSR). Our method\nalso operates in feature space, but purely during test time. While we choose some of these works as\nbaselines, our method works in conjunction with any aforementioned methods.\n\n\n**Adversarial Purification as Defense** Another line of work focuses on purifying or augmenting the\nimages before they are used as input. Tang & Zhang (2024); Yeh et al. (2024); Tsai et al. (2023)\ntrain an FGSM robust classifier, a diffusion model, or a mask auto-encoder, respectively, to purify\nadversarial examples. Wang et al. (2021) optimizes both the model and the input to minimize the\nentropy of model predictions to adapt to changing attacks. Cohen & Giryes (2024) trains a random\nforest predictor to ensemble outputs from test-time augmented images. These works involve training\na new model or updating the original model, while our method is purely test-time and does not\nrequire any training. P´erez et al. (2021) ensembles model output from different augmentations,\nwhich is a special case of our method, as the ensemble is performed solely on the output, while we\ncan ensemble at any layer, which both saves computational cost and achieves higher performance\n\n\nNotably, we recognize that above methods focus solely on classification as a task for adversarial\nattacks. Through extensive experiments, we demonstrate that our method can not only perform well\non classification, but also on dense prediction tasks like optical flow, and stereo matching.\n\n\n**Stochastic Resonance (SR)** was proposed by Benzi et al. (1981) and first applied in climate dynamics (Benzi et al., 1982) and later in signal processing (Wellens et al., 2003; Kosko & Mitaim,\n2001; Chen et al., 2007) and acoustics (Shu-Yao et al., 2016; Wang et al., 2014). Recently, Stochas\n\n3\n\n\nPreprint version. Under review.\n\n\ntic Resonance Transformer (SRT) Lao et al. (2024) uses SR to “super-resolve” Vision Transformer\n(ViT) embeddings. In this work, we instead use SR to mitigate adversarial perturbations. Since SR\nhas been developed specifically to address _quantization artifacts_, it has never before been used to\nmitigate classes of perturbations beyond aliasing. Our novel use of the technique leverages the fact\nthat group transformations and spatial quantization preserve the statistics of natural images, which\nare heavy-tailed, but do not preserve the statistics of adversarial perturbations.\n\n\n3 M ETHOD\n\n\n**Notation.** A digital image _x ∈_ [0 _, L −_ 1] _[W][ ×][H]_ can be described as a map from a discrete planar\nlattice Λ _⊂_ R [2] with _H_ rows and _W_ columns to _L_ discrete levels, _x_ : Λ _→_ [0 _, L −_ 1]; a ‘feature’\nor ‘embedding’ of an image _x_ is the output of an encoder _ϕ_ that maps it to a vector space with _K_\nchannels, typically through a parametric trained model:\n\n\n_ϕ_ : _x �→_ _ϕ_ ( _x_ ) _∈_ R _[K]_ _._\n\n\nWe represent a group transformation of the image through an operator _g_ : R [2] _→_ R [2], which can be\nrestricted to the lattice Λ through padding, sampling and quantization at the expense of invertibility:\n\n\n_g_ : _x �→_ _g_ ( _x_ ) _∈_ Λ _⊂_ R [2] _._\n\n\nFor example, a translation by an integer pixel can be represented by an upper diagonal matrix _G_,\n_g_ ( _x_ ) = _Gx_ with ones above the diagonal. The group _g_ operating on _x_ induces an operation on _ϕ_ via\n\n\n_g_ _∗_ _ϕ_ ( _x_ ) = _[.]_ _ϕ_ ( _g_ ( _x_ )) _._\n\n\nWe call the composition of _ϕ_ and _g_ the encoding of the transformed image\n\n\n_ψ_ ( _x_ ) = _[.]_ _ϕ_ ( _g_ ( _x_ )) = _ϕ ◦_ _g_ ( _x_ ) _._\n\n\nThe main object of interest in our method is:\n\n\n_g_ _∗_ _[−]_ [1] _[ψ]_ [(] _[x]_ [) =] _[ g]_ _∗_ _[−]_ [1] _◦_ _ϕ ◦_ _g_ ( _x_ ) _._\n\n\nThis is obtained, reading right-to-left, by first transforming the image, then passing it through an\nencoder, and then transforming back the feature map through the push-forward action _g_ _∗_ _[−]_ [1] [.]\n\n\n**Perturbations.** We consider two types of perturbations, extraneous and purposeful. The extraneous\none could be an additive perturbation to an image, ˜ _x_ = _x_ + _n_, designed to maximally change the\nvalue of the embedding (adversarial perturbation) _ϕ_ (˜ _x_ ):\n\n\n_x_ ˜ = _x_ + _n_ ( _x_ ) _|_ _n_ ( _x_ ) = arg max _d_ ( _ϕ_ ( _x_ ) _, ϕ_ (˜ _x_ )) _, |n| < ϵ_\n\n\nfor some small _ϵ_ designed so the perturbation is, ideally, imperceptible by humans.\n\n\nThe purposeful perturbations are small group actions _g_ _i_ _, i_ = 1 _, . . ., N_, which could be sampled\ndeterministically or at random according to some chosen distribution _g_ _i_ _∼_ _P_ _g_, either way yielding\na set _{g_ 1 ( _x_ ) _, . . ., g_ _N_ ( _x_ ) _}_ . Our goal is to use these purposeful perturbations to combat the effects of\nextraneous adversarial perturbations.\n\n\n**Averaging, smoothing, and stochastic resonance.** The resemblance between adversarial perturbations and aliasing artifacts has motivated the use of anti-aliasing, or smoothing, techniques to\nmitigate them. These consist of _spatial averaging_ of the data prior to computing the map _ϕ_ . If we\ncall _B_ _ij_ _[σ]_ [a neighborhood of size] _[ σ >]_ [ 0][ around][ (] _[i, j]_ [)] _[ ∈]_ [Λ][,]\n\n\n_B_ _ij_ _[σ]_ [=] _[ {]_ [(] _[i]_ _[′]_ _[, j]_ _[′]_ [)] _[ ∈]_ [Λ] _[ |][ d]_ [((] _[i, j]_ [)] _[,]_ [ (] _[i]_ _[′]_ _[, j]_ _[′]_ [))] _[ ≤]_ _[σ][}]_\n\n\nthen the simplest form of smoothing is simply averaging in a neighborhood,\n\n\n\n¯\n_x_ _i,j_ = [1]\n\n\n\n_x_ _i_ _′_ _j_ _′_\n( _i_ _[′]_ _,j_ _[′]_ ) _∈B_ _i,j_ _[σ]_\n\n\n\n_σ_ [2] �\n\n\n\nwhich we write in terms of translations _g_ ( _x_ _i,j_ ) = _x_ _i_ + _u,j_ + _v_ within the same neighborhood _B_ _[σ]_,\n\n\n\n_x_ ¯ = [1]\n\n_N_\n\n\n\n_N_\n� _g_ _i_ ( _x_ ) _._\n\n\n_i_ =1\n\n\n4\n\n\nPreprint version. Under review.\n\n\nFigure 2: **Results on CIFAR-10 under varying levels of stochastic resonance.** _Increasing the stochastic_\n_resonance level consistently enhances robustness across all settings, yielding clear gains over the baseline_\n_method (FSR). Notably, our approach achieves superior performance even under adaptive adversarial attacks_\n_(Ours-WorstCase), despite the baseline being evaluated only in the non-adaptive case._\n\n\nOne can then obtain an encoding by smoothing the embedding\n\n\n\n¯\n_ϕ_ ( _x_ ) = [1]\n\n_N_\n\n\n\n_N_\n� _ϕ_ ( _g_ _i_ ( _x_ )) _._\n\n\n_i_ =1\n\n\n\nThis can be interpreted as marginalizing the translation group with a prior _P_ _g_ when computing _ϕ_ .\nNotice that the average can be computed on a coarser lattice Λ [˜], but its value still depends on data\nin the finer grid Λ. Classical Sampling Theory teaches that smoothing mitigates the effect of highfrequency aliasing _n_ at the cost of information loss on _x_ .\n\n\nStochastic resonance also marginalizes the translation group, but by _averaging transformed data in_\n_latent space:_\n\n\n\nˆ\n_ϕ_ ( _x_ ) = [1]\n\n_N_\n\n\n\n_N_\n\n\n_−_ 1\n\n� _g_ _i∗_ _◦_ _ϕ ◦_ _g_ _i_ ( _x_ ) _._ (1)\n\n\n_i_ =1\n\n\n\nMore general groups, and more general averaging kernels, can be considered although we find that\nthe simplest case described here already suffices.\n\n\nStochastic resonance is _not_ smoothing, as it averages transformed versions of the image without\nblurring it, thanks to the inverse push-forward. It is also _not_ super-resolution, where fine-granularity\ndetails are hallucinated based on side information or priors, although it does allow resolving features computed on a coarse grid Λ [˜] within a finer grid Λ. Stochastic resonance uses the averaging\nof perturbations in latent space to ensemble populations of embeddings, rather than averaging or\ninterpolation of the same embedding.\n\n\n**Purposeful Perturbation.** The only design choice in the method is the set of purposeful perturbations. While that can be optimized for performance, we optimize for simplicity, restricting our\nattention to translation by integer pixels. We know that, for adversarial perturbations, _d_ ( _ϕ_ (˜ _x_ ) _, ϕ_ ( _x_ ))\nis large, where _d_ ( _·_ ) defines the distance between features. Ideally, for stochastic resonance, we want\n_d_ ( _ϕ_ [ˆ] (˜ _x_ ) _,_ _ϕ_ [ˆ] ( _x_ )) = 0 while keeping _ϕ_ [ˆ] as information-preserving as possible. The theory of Stochastic Resonance shows that, if we sub-sample a signal from its native granularity Λ to a coarser grid\n˜Λ _⊂_ Λ, and choose the purposeful perturbations to act on the finer grid Λ, under certain conditions\none can recover the original signal at the finer granularity Benzi et al. (1981). In this paper, we focus\non testing whether _ϕ_ [ˆ] is insensitive to adversarial perturbations. We do so empirically in Sect. 4.\n\n\n4 E XPERIMENTS\n\n\nWhile _g_ can be sampled from any invertible group transformation (e.g., rotation, scaling), we\nimplement stochastic resonance using integer-pixel translations, denoted as _{g_ _i_ _}_ = _{_ ( _x, y_ ) _|x ∈_\n\n[ _−d_ _x_ _, d_ _x_ ] _, j ∈_ [ _−d_ _y_ _, d_ _y_ ] _}_, following the approach in SRT Lao et al. (2024), which avoids interpolation artifacts. While the networks’ inherent sensitivity to pixel-level shifts is typically regarded\nas detrimental due to the “flickering problem” Azulay & Weiss (2019); Sundaramoorthi & Wang\n(2019), our approach, on the contrary, leverages it to defend against adversarial perturbations. Given\nthese perturbations _{g_ _i_ _}_, ensembling can be performed at any chosen layer of the network. Features\nare aggregated as described in Eq. 1 and then passed to downstream network components.\n\n\nWe first validate this approach on image classification (Sect. 4.1) following standard benchmarks,\nand also provide ablation studies on levels/layers of latent ensemble and rotation as augmentation.\n\n\n5\n\n\nPreprint version. Under review.\n\n\n\n\n\n\n\n\n|ResNet-18|85.02<br>85.14<br>85.78<br>79.87<br>81.46<br>85.25<br>84.93<br>84.93|56.21<br>56.81<br>55.57<br>56.53<br>58.07<br>59.20<br>61.02<br>58.81|48.22<br>48.54<br>50.42<br>49.80<br>52.47<br>53.00<br>56.08<br>53.58|46.37<br>46.70<br>49.91<br>48.17<br>51.02<br>51.65<br>55.17<br>52.39|47.38<br>47.72<br>53.47<br>49.89<br>49.44<br>52.45<br>55.53<br>52.73|\n|---|---|---|---|---|---|\n|WideResNet-34|84.92<br>**85.03**|60.87<br>**62.43**|56.13<br>**58.64**|55.16<br>**57.87**|54.02<br>**57.18**|\n\n\n\nTable 1: **Defense against adversarial attacks on classification task (CIFAR-10).** _Compared to baselines_\n_that filter or manipulate features, ours does not modify network architecture or weights. Instead, ours performs_\n_an ensemble in the feature space. On CIFAR-10, ours achieves state-of-the-art robustness without requiring any_\n_additional training. Moreover, even in a worst-case adaptive adversary setting where the attacker is fully aware_\n_of the defense and how stochastic resonance is applied, the effectiveness of adversarial attacks is still notably_\n_reduced, while the computational cost for executing such attacks is significantly increased._\n\n\n\n|Att. Strength (ϵ)|8/2|255|4/2|255|2/2|255|1/25|55|\n|---|---|---|---|---|---|---|---|---|\n|Metric|Top-1|Top-5|Top-1|Top-5|Top-1|Top-5|Top-1|Top-5|\n|No Defense<br>Ours (_d_ = 1)<br>Ours (_d_ = 2)<br>Ours (_d_ = 3)|4.51<br>11.66<br>18.78<br>**25.77**|19.55<br> 46.27<br> 58.58<br>** 65.88**|12.25<br> 27.85<br> 36.42<br>** 42.52**|42.15<br>65.80<br>72.86<br>**76.75**|29.79<br> 45.94<br> 51.58<br>** 54.94**|65.09<br> 77.63<br> 80.85<br>** 82.64**|48.09<br> 57.48<br> 60.41<br>** 62.08**|78.13<br>83.43<br>84.77<br>**85.50**|\n\n\nTable 2: **ImageNet with ViT-Small.** Increasing the\nlevel of stochastic resonance consistently improves\nboth Top-1 and Top-5 accuracy under adversarial attacks. Relative to the clean baseline (72.9 Top-1, 92.91\nTop-5), our method recovers up to 55.8% of the Top-1\naccuracy loss and 68.1% of the Top-5 accuracy loss.\n\n\n\n|Att. Strength (ϵ)|8/2|255|4/2|255|2/2|255|1/2|255|\n|---|---|---|---|---|---|---|---|---|\n|Metric|Top-1|Top-5|Top-1|Top-5|Top-1|Top-5|Top-1|Top-5|\n|No Defense<br>Initial Conv.<br>Res. Block 1<br>Res. Block 2<br>Res. Block 3|8.51<br>16.91<br>22.44<br>**24.88**<br>21.76|26.19<br> 34.66<br> 44.95<br>** 48.54**<br> 44.28|9.45<br> 17.36<br> 23.39<br>** 25.90**<br> 22.73|27.92<br> 35.28<br> 46.03<br>** 49.66**<br> 45.47|11.10<br> 18.12<br> 24.84<br>** 27.23**<br> 24.14|30.63<br> 36.30<br> 47.64<br>** 51.19**<br> 47.14|15.21<br>19.59<br>27.57<br>**30.11**<br>27.18|36.76<br> 38.25<br> 50.80<br>** 54.15**<br> 50.20|\n\n\nTable 3: **Layer-wise ablation on ResNet-50.** Adversarial perturbations resemble high-frequency noise.\nApplying stochastic resonance through shallow layers\nis sufficient to defend against adversarial attacks, substantially reducing the overall computational cost.\n\n\n\nSubsequently, we defend against adversarial attacks on dense prediction tasks, including stereo\nmatching (Sec. 4.2) and optical flow (Sec. 4.3). This is achieved given that our method is agnostic\nto attack mechanisms, network pre-training, and largely independent of architecture, requiring only\nlatent ensembling, resulting in a purely test-time, training-free approach with no auxiliary modules.\n\n\n4.1 I MAGE C LASSIFICATION\n\n\n**CIFAR-10.** We evaluate our method on CIFAR-10 Krizhevsky et al. (2009), building upon the standard and publicly available code base of FSR Kim et al. (2023) and accompanying evaluation protocol. We apply stochastic resonance to networks pre-trained with AT Madry et al. (2017), TRADES\nZhang et al. (2019), and MART Wang et al. (2019), using publicly released weights without any\nmodification. Our method operates with these methods purely at test time without any training. We\nconduct experiments on ResNet-18 He et al. (2016) and WideResNet-34 Zagoruyko & Komodakis\n(2016), depending on the availability of author-released pre-trained weights. In all cases, feature\nensembling is performed before the final linear layer. Furthermore, we consider a worst-case adaptive adversary setting, where the attacker has full knowledge of the model weights, and knows every\nstochastic resonance transformation by accessing _every single_ forward pass and its gradients.\n\n\nFig. 2 shows the results varying different levels of stochastic resonance. Increasing the resonance\nlevel consistently enhances robustness in all attack settings, leading to substantial improvements\nover the baseline method (FSR). Importantly, our approach surpasses the baseline even under adaptive adversarial attacks. Additionally, we compare our method against multiple baselines, including\nfeature-level manipulation methods (FD Xie et al. (2019), CAS Bai et al. (2021), CIFS Yan et al.\n(2021)) and ensemble-based approach TTE P´erez et al. (2021). The results, summarized in Tab. 1,\ndemonstrate that stochastic resonance consistently outperforms all baselines across different attacks.\nEven in the worst-case adaptive attack scenario, where the adversary accounts for all stochastic resonance forward passes, the model remains significantly more robust than the baseline methods. In\n\n\n6\n\n\nPreprint version. Under review.\n\n|Col1|Attack Strength (ϵ)|0.02|Col4|Col5|0.01|Col7|Col8|0.005|Col10|Col11|0.002|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||Metric|MAE|RMSE|D1-err|MAE|RMSE|D1-err|MAE|RMSE|D1-err|MAE|RMSE|D1-err|\n|FGSM|No Defense<br>Latent Smoothing<br>Ours (_d_ = 1)<br>Error Reduced (%)<br>Ours (_d_ = 2)<br>Error Reduced (%)|14.83<br>13.42<br>10.12<br> 31.76<br>**9.22**<br> 37.82|24.10<br> 22.09<br> 15.81<br> 34.40<br>**13.88**<br> 42.40|97.33<br>96.61<br>95.80<br>1.57<br>**94.61**<br>2.79|8.49<br>8.12<br>6.46<br>23.91<br>**6.13**<br>27.78|14.53<br>13.69<br>9.92<br> 31.73<br>**8.87**<br> 38.95|90.49<br>89.25<br>86.30<br>4.63<br>**84.49**<br>6.63|5.05<br>4.89<br>4.39<br>13.07<br>**4.19**<br>16.92|7.70<br>7.05<br>5.93<br>22.99<br>**5.43**<br>29.41|74.71<br>73.01<br>68.55<br>7.58<br>**66.74**<br>10.66|3.01<br>2.89<br>2.78<br>7.64<br>**2.73**<br>9.21|3.49<br>3.32<br>3.18<br>8.88<br>**3.12**<br>10.65|38.12<br>36.25<br>31.99<br>16.08<br>**31.18**<br>18.21|\n|PGD|No Defense<br>Latent Smoothing<br>Ours (_d_ = 1)<br>Error Reduced (%)<br>Ours (_d_ = 2)<br>Error Reduced (%)|161.70<br>161.79<br>107.86<br> 33.30<br>**77.59**<br> 52.01|162.61<br>162.69<br>125.14<br>23.04<br>**100.14**<br> 38.41|99.99<br> 99.99<br> 98.29<br>1.70<br>** 96.14**<br>3.85|131.66<br>131.46<br> 69.97<br>46.86<br>** 44.77**<br>66.02|140.55<br>140.41<br> 84.72<br> 39.72<br>** 66.23**<br> 52.87|98.64<br> 98.57<br>91.79<br>6.94<br>**89.24**<br>9.53|63.97<br>63.86<br>20.66<br>67.70<br>**17.98**<br>71.89|88.55<br>88.21<br>42.29<br> 52.24<br>**32.80**<br> 62.94|85.31<br>85.39<br>73.21<br> 14.18<br>**71.09**<br> 16.66|6.83<br>7.28<br>4.17<br> 38.95<br>**3.76**<br> 44.99|17.03<br>17.44<br>8.32<br> 51.15<br>**6.73**<br> 60.52|39.24<br>39.81<br>29.09<br> 25.87<br>**28.44**<br> 27.53|\n\n\n\nTable 5: **Stochastic Resonance Enhances Stereo Matching Robustness.** _Incorporating stochastic resonance_\n_significantly reduces prediction errors induced by adversarial attacks across all evaluation metrics, reducing_\n_error by up to 71.89% (MAE, when attacked by PGD with ϵ_ = 0 _._ 005 _). Notably, this defense mechanism_\n_operates entirely at test time without requiring any model re-training, which sets it apart from existing methods._\n\n\naddition, stochastic resonance increases attack complexity under the adaptive settings, making adversarial noise generation more challenging for the attacker. As a result, the computational cost for\ngenerating adaptive adversarial perturbations increases substantially. For example, with stochastic\nresonance, 8x more wall-clock time is required to create adversarial examples with PGD-100.\n\n\n**ImageNet.** We further evaluate our approach on the ImageNet Deng et al. (2009) classification\ndataset using standard segmentation backbones, including ResNet-50 He et al. (2016) and Vision\nTransformer Dosovitskiy et al. (2020), without adversarial training. As in the CIFAR experiments,\nwe vary the level of stochastic resonance and conduct ablation studies by testing against PGD attacks\nof different strengths. Tab. 2 reports the results for ViT-Small. Consistent with the CIFAR-10 findings, increasing the resonance level leads to consistent improvements in robustness, as measured by\nboth Top-1 and Top-5 accuracy. Notably, the vanilla ViT-Small model without attack achieves 72.9\n(Top-1) and 92.91 (Top-5), which means our method recovers the accuracy drop under adversarial\nattacks by up to a relative 55.8% (Top-1, when _ϵ_ = 4 _/_ 255) and 68.1% (Top-5, when _ϵ_ = 2 _/_ 255).\nWe further evaluate our method on ResNet-50 and observe a consistent trend, as shown in Tab. 4.\n\n\n\nWe also explored group transformations other\nthan translation, e.g. rotations in Tab. 4. For\na fair comparison, we use the same number\nof augmentations as in the translation experiments. Rotations behave similarly to translations at low levels of stochastic resonance, but\nperformance degrades as the resonance level increases. We hypothesize that since convolutional filters are inherently translation-invariant\nbut not rotation-invariant, aligning features under different rotations reduces feature quality. Moreover, rotations are approximately 30%\nslower due to interpolation overhead.\n\n\n\nAtt. Strength ( _ϵ_ ) 8/255 4/255 2/255 1/255\n\n|Metric|Top-1|Top-5|Top-1|Top-5|Top-1|Top-5|Top-1|Top-5|\n|---|---|---|---|---|---|---|---|---|\n|No Defense<br>Ours (_d_ = 1)<br>w/ Rotation<br>Ours (_d_ = 2)<br>w/ Rotation<br>Ours (_d_ = 3)<br>w/ Rotation|8.51<br>18.40<br>20.01<br>20.01<br>18.86<br>**21.17**<br>15.15|26.19<br> 40.36<br> 41.32<br> 42.22<br> 38.67<br>** 43.58**<br> 32.89|9.45<br> 19.54<br> 20.84<br> 21.02<br> 19.55<br>** 22.04**<br> 15.61|27.92<br>42.65<br>42.39<br>43.45<br> 39.5<br>**44.78**<br>33.46|11.10<br> 21.02<br> 22.10<br> 22.44<br>20.36<br>** 23.44**<br> 16.17|30.63<br>43.86<br>43.89<br>45.32<br>40.48<br>**46.53**<br>34.19|15.21<br> 24.66<br> 24.72<br> 25.80<br> 22.01<br>** 26.58**<br> 17.33|36.76<br> 47.81<br> 46.89<br> 48.97<br> 42.50<br>** 49.73**<br> 35.59|\n\n\n\nTable 4: **Stochastic resonance using translation v.s.**\n**rotation on ResNet-50.** While rotations provide similar gains at low resonance levels, performance degrades\nas the resonance level increases, likely due to the lack\nof rotational invariance in convolutional filters.\n\n\n\nSince adversarial perturbations often manifest as high-frequency noise, having a low-pass filter in\nearly layers may form an effective defense. As our method applies to arbitrarily chosen layers, we\nvary the termination layer of stochastic resonance. As shown in Tab. 3, applying it only through the\nfirst residual block already achieves strong adversarial robustness, while extending it to the second\nblock yields the strongest result. This finding is significant: running stochastic resonance through\nshallow layers can be sufficient as a defense strategy, which reduces overall computational cost.\n\n\n4.2 S TEREO M ATCHING\n\n\nStereopagnosia Wong et al. (2021) first introduced adversarial attacks to stereo matching, yet no\ntest-time method has demonstrated an effective defense. The primary challenge arises from the in\n\n7\n\n\nPreprint version. Under review.\n\n\n\n\n\nFigure 3: **Stereo matching robustness via stochastic resonance.** _We present visual results on stereo matching_\n_under various adversarial attack scenarios, including PGD and FGSM at different perturbation levels. These at-_\n_tacks significantly degrade the network’s predictions, leading to substantial errors. By incorporating stochastic_\n_resonance, we demonstrate a significant reduction in prediction errors. This technique holds significant po-_\n_tential for improving robustness in safety-critical real-world applications, such as autonomous driving, where_\n_stereo vision must remain reliable under diverse environmental conditions and adversarial threats._\n\n\nfeasibility of data augmentations, as they risk altering the physics of the input, leading to incorrect\nestimation. In contrast, our feature-level ensemble is suitable for this task, as transformations introduced by stochastic resonance are “undone” in the latent space, ensuring that features remain aligned\nprecisely with the original input. This process mirrors AugUndo Wu et al. (2024) conceptually.\n\n\nWe evaluate our method on the standard benchmark used in _Stereopagnosia_, derived from\nKITTI Geiger et al. (2012). Experiments are conducted using FGSM and PGD attacks against a\npre-trained PSMNet Chang & Chen (2018). Since no existing test-time defense is available, we\nadopt latent-space smoothing as a baseline. As shown in Tab. 5, both attacks corrupt network predictions, with PGD proving substantially more effective due to its iterative nature. Nevertheless,\nstochastic resonance consistently improves robustness across attack strengths. In particular, under\nPGD with _ϵ_ = 0 _._ 005, our method reduces errors by up to 71.89% in terms of MAE. Crucially, these\ngains are achieved entirely at test time, without additional training or prior knowledge of the attack.\n\n\nFig. 3 provides qualitative results, featuring different attack strengths and methods. The adversarial\nperturbations introduce significant distortions, as indicated by bright regions in the visualized predictions. Stochastic resonance effectively mitigates these distortions, drastically reducing prediction\nerrors. This experiment is particularly relevant for safety-critical applications such as autonomous\ndriving, where adversarial disturbances can arise not only from malicious attacks but also from environmental factors such as adverse weather conditions, varying illumination, or sensor degradation.\nWhile some defenses Zhang et al. (2023); Cheng et al. (2021; 2022); Berger et al. (2022) have been\nproposed to train a more robust network under adverse conditions, test-time defenses remain largely\nunexplored. Our method is the first to provide a viable solution in this setting.\n\n\n4.3 O PTICAL F LOW\n\n\nWe further evaluate our method on optical flow, which computes the dense motion field between\ntwo images. The accuracy of optical flow is measured using the End-Point Error (EPE). We note\nthat multiple adversarial attacks exist for optical flow Schrodi et al. (2022); Ranjan et al. (2019);\nSchmalfuss et al. (2022). Among these attacks, PGD is stronger than patch-based attacks as adversarial patches have localized effects. Therefore, in our experiments, we employ the RAFT Teed &\nDeng (2020) optical flow model and focus on global adversarial perturbations generated by PGD\nand FGSM. We test our method on the DAVIS Pont-Tuset et al. (2017) dataset under both attacks.\n\n\nTo defend against adversarial attacks, we apply stochastic resonance to the convolutional feature\nextractor of RAFT. Since the perturbation is applied only at the feature extraction stage, no additional overhead is introduced in the computationally intensive matching module. Quantitative results\n(Fig. 4) show that increasing Stochastic Resonance reduces EPE, which aligns with our findings in\nclassification. As in Fig. 5, our approach effectively removes errors caused by adversarial noise.\n\n\n8\n\n\nPreprint version. Under review.\n\n\n\nPGD FGSM\n\n\nFigure 5: **Optical flow robustness via stochastic resonance.** _Qualitative results (visualized with a color_\n_wheel) show that our method substantially mitigates the degradation caused by both PGD and FGSM attacks._\n_This robustness is particularly relevant for visual perception systems that rely on accurate motion estimation._\n\n\n\nWe further compare our method to an alternative ensembling approach that aggregates predictions in the output space, conceptually similar to TTE P´erez et al. (2021). In this variant, we apply the same stochastic transformations but instead ensemble at the output level\nrather than in the feature space. While this\nmethod provides marginal improvements, it remains less effective than our approach. This\nfinding highlights the advantage of having the\nfreedom to choose from any stage of the model\nto perform ensemble. In this particular experiment, we demonstrate that ensembling solely\nat the image encoding sub-module, while leaving the rest of the RAFT network unchanged,\nyields substantial improvements in robustness,\nthanks to the flexibility of our method.\n\n\n5 D ISCUSSION AND C ONCLUSION\n\n\n\nFigure 4: **Enhanced optical flow robustness with**\n**stochastic resonance.** _Under PGD and FGSM,_\n_stochastic resonance significantly reduces endpoint er-_\n_ror in optical flow estimation. Notably, our method per-_\n_forms ensembling in the latent feature space rather than_\n_the output space, providing greater flexibility. While_\n_ensembling in the output space offers minor perfor-_\n_mance gains, our approach consistently achieves supe-_\n_rior robustness across all levels of stochastic resonance._\n\n\n\n**Speed.** Stochastic resonance incurs low computational overhead when executed in parallel: raising\nthe stochastic resonance level to 3 with ResNet-50 adds only 0.06 seconds to the inference time on\nan NVIDIA 1080Ti GPU. Most of the overhead arises from creating perturbations implemented via\nPython loops with torchvision; we expect further speedups with efficient CUDA implementations. Even when executed _sequentially_, the computational overhead is 0.095 seconds. Moreover,\nstrong robustness can be attained by applying stochastic resonance only to shallow layers, offering\nsubstantially greater efficiency than existing ensemble-based defenses (e.g. P´erez et al. (2021)) that\nrequire multiple passes through the entire network. Moreover, our method is fully plug-and-play.\nIn contrast, attack-specific adversarial training is over 6x slower than a vanilla training pipeline. As\nsuch, the computation of our method is well justified by its robustness gains and training-free nature.\n\n\n**On-demand scaling.** One of the key strengths of our approach is its flexibility: providing a tradeoff between robustness and computational cost. We offer a tunable “knob” that allows practitioners\nto adjust the level of resilience based on available resources on the fly: when the system has more\ncomputational capacity, add a higher level of stochastic resonance, vice versa. Note that, such a\ndesign does not _rely_ on additional computation, yet more computation can bring _extra_ performance.\nMoreover, our experiments show that the method generalizes across a wide range of tasks and architectures that include an encoder. This on-demand scaling mirrors inference-time scaling in language\nmodels, where performance can be improved without modifying the underlying pre-trained model.\n\n\n9\n\n\nPreprint version. Under review.\n\n\n**Limitations.** Despite its strengths, our method has some limitations. First, while we offer parallel\ncomputation as a remedy, the computational overhead introduced by stochastic resonance may not\nbe negligible for scenarios with memory and power constraints. Also, our current study focuses\non integer-pixel translations. While this choice avoids interpolation artifacts and preserves spatial\nconsistency, more generic transformations, including learned transformations, could be explored.\n\n\n**Conclusion.** In this work, we present a signal-processing perspective for defending against adversarial attacks, motivated by the connection between adversarial perturbations and aliasing artifacts.\nAccordingly, we propose a “combat noise with noise” approach by introducing stochastic resonance\nas a defense mechanism. We formalize the problem and implement stochastic resonance using pixellevel translations paired with their inverse transformation in the feature space. The resulting method\nis training-free, agnostic to both tasks and attack types, and independent of network architectures.\n\n\nWe evaluate our method across various tasks. Empirical results on image classification demonstrate\nthat our stochastic resonance approach achieves state-of-the-art robustness against diverse attack\ntypes, offering a clear advantage over feature-level denoising and filtering. Even in the adaptive adversary scenario, where an attacker is aware of the use of stochastic resonance, our method maintains\nstrong robustness. Furthermore, we are the first to introduce test-time defense to dense prediction\ntasks. Specifically, we apply this method to stereo matching and optical flow, achieving up to a 71%\nreduction in prediction error. More importantly, these findings highlight the practical potential of\nstochastic resonance as a universal defense in real-world adversarial scenarios.\n\n\n10\n\n\nPreprint version. Under review.\n\n\nR EPRODUCIBILITY S TATEMENT\n\n\nWe provide sufficient technical details in the paper to ensure reproducibility. Specifically, we\ndescribe the augmentations used for stochastic resonance, including augmentations (e.g. translation, rotation) and their corresponding inverse transformations, as well as the model architectures,\ndatasets, and the network layers where our method is applied. Attack settings and evaluation protocols are drawn directly from standard benchmark datasets and publicly available code base, ensuring\ncomparability with prior work. All implementation details necessary to reproduce our experiments,\nincluding parameters and ablation settings, are provided in the main paper and further expanded in\nthe Appendix. Our experiments can be reproduced on a single desktop-level GPU without requiring\nlarge-scale computational resources. We will release the complete source code and pre-computed\nadversarial data upon publication.\n\n\nLLM S TATEMENT\n\n\nAll technical content of this work, including literature review, methodology, experiments, and analyses, was developed entirely by the authors. Large Language Models (LLMs) were employed as a\ntool for proofreading, without contributing to the scientific or technical substance of the manuscript.\n\n\nR EFERENCES\n\n\nAharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small\nimage transformations? _Journal of Machine Learning Research_, 20(184):1–25, 2019.\n\n\nYang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving\nadversarial robustness via channel-wise activation suppressing. _arXiv preprint arXiv:2103.08307_,\n2021.\n\n\nYoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In\n_Proceedings of the 26th annual international conference on machine learning_, pp. 41–48, 2009.\n\n\nPhilipp Benz, Chaoning Zhang, and In So Kweon. Batch normalization increases adversarial vulnerability and decreases adversarial transferability: A non-robust feature perspective. In _Proceedings_\n_of the IEEE/CVF International Conference on Computer Vision_, pp. 7818–7827, 2021.\n\n\nRoberto Benzi, Alfonso Sutera, and Angelo Vulpiani. The mechanism of stochastic resonance.\n_Journal of Physics A: mathematical and general_, 14(11):L453, 1981.\n\n\nRoberto Benzi, Giorgio Parisi, Alfonso Sutera, and Angelo Vulpiani. Stochastic resonance in climatic change. _Tellus_, 34(1):10–16, 1982.\n\n\nZachary Berger, Parth Agrawal, Tian Yu Liu, Stefano Soatto, and Alex Wong. Stereoscopic universal perturbations across different architectures and datasets. In _Proceedings of the IEEE/CVF_\n_Conference on Computer Vision and Pattern Recognition_, pp. 15180–15190, 2022.\n\n\nQi-Zhi Cai, Min Du, Chang Liu, and Dawn Song. Curriculum adversarial training. _arXiv preprint_\n_arXiv:1805.04807_, 2018.\n\n\nNicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In _2017_\n_ieee symposium on security and privacy (sp)_, pp. 39–57. Ieee, 2017.\n\n\nJia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In _Proceedings of the_\n_IEEE conference on computer vision and pattern recognition_, pp. 5410–5418, 2018.\n\n\nHao Chen, Pramod K Varshney, Steven M Kay, and James H Michels. Theory of the stochastic resonance effect in signal detection: Part i—fixed detectors. _IEEE transactions on Signal Processing_,\n55(7):3172–3184, 2007.\n\n\nKelvin Cheng, Christopher Healey, and Tianfu Wu. Towards adversarially robust and domain generalizable stereo matching by rethinking dnn feature backbones. _arXiv preprint arXiv:2108.00335_,\n2021.\n\n\n11\n\n\nPreprint version. Under review.\n\n\nKelvin Cheng, Tianfu Wu, and Christopher Healey. Revisiting non-parametric matching cost volumes for robust and generalizable stereo matching. _Advances in Neural Information Processing_\n_Systems_, 35:16305–16318, 2022.\n\n\nGilad Cohen and Raja Giryes. Simple post-training robustness using test time augmentations and\nrandom forest. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer_\n_Vision_, pp. 3996–4006, 2024.\n\n\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_,\npp. 248–255. Ieee, 2009.\n\n\nGuneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi,\nAran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. _arXiv preprint arXiv:1803.01442_, 2018.\n\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint_\n_arXiv:2010.11929_, 2020.\n\n\nAngus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham W Taylor. Batch\nnormalization is a cause of adversarial vulnerability. _arXiv preprint arXiv:1905.02161_, 2019.\n\n\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti\nvision benchmark suite. In _2012 IEEE conference on computer vision and pattern recognition_,\npp. 3354–3361. IEEE, 2012.\n\n\nSajjad Ghiasvand, Amirhossein Reisizadeh, Mahnoosh Alizadeh, and Ramtin Pedarsani. Robust\ndecentralized learning with local updates and gradient tracking. _arXiv preprint arXiv:2405.00965_,\n2024.\n\n\nJames J Gibson. _The ecological approach to visual perception: classic edition_ . Psychology press,\n2014.\n\n\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. _arXiv preprint arXiv:1412.6572_, 2014.\n\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp.\n770–778, 2016.\n\n\nGaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xiaowei Huang. Enhancing adversarial\ntraining with second-order statistics of weights. In _Proceedings of the IEEE/CVF conference on_\n_computer vision and pattern recognition_, pp. 15273–15283, 2022.\n\n\nHarini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. _arXiv preprint_\n_arXiv:1803.06373_, 2018.\n\n\nWoo Jae Kim, Yoonki Cho, Junsik Jung, and Sung-Eui Yoon. Feature separation and recalibration\nfor adversarial robustness. In _Proceedings of the IEEE/CVF conference on computer vision and_\n_pattern recognition_, pp. 8183–8192, 2023.\n\n\nBart Kosko and Sanya Mitaim. Robust stochastic resonance: Signal detection and adaptation in\nimpulsive noise. _Physical review E_, 64(5):051110, 2001.\n\n\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\n2009.\n\n\nAlexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world.\nIn _Artificial intelligence safety and security_, pp. 99–112. Chapman and Hall/CRC, 2018.\n\n\nDong Lao, Yangchao Wu, Tian Yu Liu, Alex Wong, and Stefano Soatto. Sub-token vit embedding\nvia stochastic resonance transformers. In _International Conference on Machine Learning_, pp.\n25995–26006. PMLR, 2024.\n\n\n12\n\n\nPreprint version. Under review.\n\n\nBo Luo, Yannan Liu, Lingxiao Wei, and Qiang Xu. Towards imperceptible and robust adversarial\nexample attacks against neural networks. In _Proceedings of the AAAI conference on artificial_\n_intelligence_, volume 32, 2018.\n\n\nDivyam Madaan, Jinwoo Shin, and Sung Ju Hwang. Adversarial neural pruning with latent vulnerability suppression. In _International conference on machine learning_, pp. 6575–6585. PMLR,\n2020.\n\n\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.\nTowards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_,\n2017.\n\n\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal\nadversarial perturbations. In _Proceedings of the IEEE conference on computer vision and pattern_\n_recognition_, pp. 1765–1773, 2017.\n\n\nHaniyeh Ehsani Oskouie, Mohammad-Shahram Moin, and Shohreh Kasaei. Attack on scene flow\nusing point clouds. In _2024 IEEE 34th International Workshop on Machine Learning for Signal_\n_Processing (MLSP)_, pp. 1–6. IEEE, 2024.\n\n\nJuan C P´erez, Motasem Alfarra, Guillaume Jeanneret, Laura Rueda, Ali Thabet, Bernard Ghanem,\nand Pablo Arbel´aez. Enhancing adversarial robustness via test-time transformation ensembling.\nIn _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 81–91, 2021.\n\n\nJordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel´aez, Alex Sorkine-Hornung, and\nLuc Van Gool. The 2017 davis challenge on video object segmentation. _arXiv preprint_\n_arXiv:1704.00675_, 2017.\n\n\nAnurag Ranjan, Joel Janai, Andreas Geiger, and Michael J. Black. Attacking optical flow. In\n_Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, October\n2019.\n\n\nJenny Schmalfuss, Philipp Scholze, and Andr´es Bruhn. A perturbation-constrained adversarial attack for evaluating the robustness of optical flow. In _European Conference on Computer Vision_,\npp. 183–200. Springer, 2022.\n\n\nSimon Schrodi, Tonmoy Saikia, and Thomas Brox. Towards understanding adversarial robustness\nof optical flow networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and_\n_Pattern Recognition_, pp. 8916–8924, 2022.\n\n\nJi Shu-Yao, Yuan Fei, Chen Ke-Yu, and Cheng En. Application of stochastic resonance technology\nin underwater acoustic weak signal detection. In _OCEANS 2016-Shanghai_, pp. 1–5. IEEE, 2016.\n\n\nNigel G Stocks, D Allingham, and Robert P Morse. The application of suprathreshold stochastic\nresonance to cochlear implant coding. _Fluctuation and noise letters_, 2(03):L169–L181, 2002.\n\n\nGanesh Sundaramoorthi and Timothy E Wang. Translation insensitive cnns. _arXiv preprint_\n_arXiv:1911.11238_, 2019.\n\n\nLinyu Tang and Lei Zhang. Robust overfitting does matter: Test-time adversarial purification with\nfgsm. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_,\npp. 24347–24356, 2024.\n\n\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In _Computer_\n_Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,_\n_Part II 16_, pp. 402–419. Springer, 2020.\n\n\nFlorian Tram`er, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space\nof transferable adversarial examples. _arXiv preprint arXiv:1704.03453_, 2017.\n\n\nFlorian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks\nto adversarial example defenses. _Advances in neural information processing systems_, 33:1633–\n1645, 2020.\n\n\n13\n\n\nPreprint version. Under review.\n\n\nYun-Yun Tsai, Ju-Chin Chao, Albert Wen, Zhaoyuan Yang, Chengzhi Mao, Tapan Shah, and Junfeng\nYang. Test-time detection and repair of adversarial samples via masked autoencoder. _arXiv_\n_preprint arXiv:2303.12848_, 2023.\n\n\nDequan Wang, An Ju, Evan Shelhamer, David Wagner, and Trevor Darrell. Fighting gradients with\ngradients: Dynamic defenses against adversarial attacks. _arXiv preprint arXiv:2105.08714_, 2021.\n\n\nHaotao Wang, Aston Zhang, Shuai Zheng, Xingjian Shi, Mu Li, and Zhangyang Wang. Removing\nbatch normalization boosts adversarial training. In _International Conference on Machine Learn-_\n_ing_, pp. 23433–23445. PMLR, 2022.\n\n\nHongjun Wang and Yisen Wang. Self-ensemble adversarial training for improved robustness. _arXiv_\n_preprint arXiv:2203.09678_, 2022.\n\n\nJun Wang, Qingbo He, and Fanrang Kong. Adaptive multiscale noise tuning stochastic resonance\nfor health diagnosis of rolling element bearings. _IEEE Transactions on instrumentation and mea-_\n_surement_, 64(2):564–577, 2014.\n\n\nYisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving\nadversarial robustness requires revisiting misclassified examples. In _International conference on_\n_learning representations_, 2019.\n\n\nThomas Wellens, Vyacheslav Shatokhin, and Andreas Buchleitner. Stochastic resonance. _Reports_\n_on progress in physics_, 67(1):45, 2003.\n\n\nAlex Wong, Mukund Mundhra, and Stefano Soatto. Stereopagnosia: Fooling stereo networks with\nadversarial perturbations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 2879–2888, 2021.\n\n\nYangchao Wu, Tian Yu Liu, Hyoungseob Park, Stefano Soatto, Dong Lao, and Alex Wong. Augundo: Scaling up augmentations for monocular depth completion and estimation. In _European_\n_Conference on Computer Vision_, pp. 274–293. Springer, 2024.\n\n\nChang Xiao, Peilin Zhong, and Changxi Zheng. Enhancing adversarial defense by k-winners-takeall. _arXiv preprint arXiv:1905.10510_, 2019.\n\n\nCihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He. Feature denoising\nfor improving adversarial robustness. In _Proceedings of the IEEE/CVF conference on computer_\n_vision and pattern recognition_, pp. 501–509, 2019.\n\n\nHanshu Yan, Jingfeng Zhang, Gang Niu, Jiashi Feng, Vincent Tan, and Masashi Sugiyama. Cifs:\nImproving adversarial robustness of cnns via channel-wise importance-based feature selection. In\n_International Conference on Machine Learning_, pp. 11693–11703. PMLR, 2021.\n\n\nCheng-Han Yeh, Kuanchun Yu, and Chun-Shien Lu. Test-time adversarial defense with opposite\nadversarial path and high attack time cost. _arXiv preprint arXiv:2410.16805_, 2024.\n\n\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. _arXiv preprint_\n_arXiv:1605.07146_, 2016.\n\n\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.\nTheoretically principled trade-off between robustness and accuracy. In _International conference_\n_on machine learning_, pp. 7472–7482. PMLR, 2019.\n\n\nHoward Zhang, Yunhao Ba, Ethan Yang, Varan Mehra, Blake Gella, Akira Suzuki, Arnold Pfahnl,\nChethan Chinder Chandrappa, Alex Wong, and Achuta Kadambi. Weatherstream: Light transport automation of single image deweathering. In _Proceedings of the IEEE/CVF Conference on_\n_Computer Vision and Pattern Recognition_, pp. 13499–13509, 2023.\n\n\nJingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In _International_\n_conference on machine learning_, pp. 11278–11287. PMLR, 2020.\n\n\nDaniel Zoran, Mike Chrzanowski, Po-Sen Huang, Sven Gowal, Alex Mott, and Pushmeet Kohli.\nTowards robust image classification using sequential attention models. In _Proceedings of the_\n_IEEE/CVF conference on computer vision and pattern recognition_, pp. 9483–9492, 2020.\n\n\n14\n\n\nPreprint version. Under review.\n\n\nA I MPLEMENTATION FOR C LASSIFICATION\n\n\nFor our classification experiments, we built our implementation on top of standard network architectures, implementing SR on two main architectures derived from the FSR codebase Kim et al.\n(2023):\n\n\n    - ResNet-18 He et al. (2016): A standard residual network with 18 layers organized in four\nmain blocks with increasing channel dimensions (64, 128, 256, 512).\n\n\n   - WideResNet-34 Zagoruyko & Komodakis (2016): A wider variant of ResNet with depth\n34 and width factor 10, resulting in higher representational capacity with three main blocks\nwith channel dimensions scaled by the width factor (160, 320, 640).\n\n\nFor both architectures, we apply SR at the bottleneck layer (after the final convolutional block).\n\n\nGiven an input image batch _x ∈_ R _[b][×]_ [3] _[×][h][×][w]_ (where _b_ is the batch size), our SR approach operates as\nfollows. First, we create a set of (2 _d_ _x_ + 1) _×_ (2 _d_ _y_ + 1) perturbed versions of the input by applying\npixel-level translations within the range [ _−d_ _x_ _, d_ _x_ ] _×_ [ _−d_ _y_ _, d_ _y_ ] pixels:\n\n\n_X_ perturbed = _{g_ _i,j_ ( _X_ ) _| i ∈_ [ _−d_ _y_ _, d_ _y_ ] _, j ∈_ [ _−d_ _x_ _, d_ _x_ ] _}_ (2)\n\n\nwhere _g_ _i,j_ translates the image by ( _i, j_ ) pixels. These transformations are applied using PyTorch’s\n“transforms.functional.affine” function with translation parameters while preserving the original image properties.\n\n\nAll images are concatenated into one batch and processed through the network in parallel up to the\nbottleneck layer:\n_F_ = _ϕ_ ( _X_ perturbed ) (3)\n\n\nwhere _ϕ_ represents the network up to the bottleneck layer. This batch processing approach significantly improves computational efficiency compared to individual forward passes.\n\n\nAfter obtaining feature maps for all perturbed inputs, we aggregate them to create a single enhanced\nfeature map:\n\n\n\n_F_ ensembled = [1]\n\n_n_\n\n\n\n� _T_ _−i,−j_ ( _F_ _i,j_ ) (4)\n\n_i,j_\n\n\n\nwhere _T_ _−i,−j_ represents the inverse spatial shift operation that realigns the feature map and _n_ the\nnumber of augmentations.\n\n\nOur implementation requires (2 _d_ _x_ + 1) _×_ (2 _d_ _y_ + 1) forward passes through the network up to the\nbottleneck layer.\n\n\nFor evaluation, we tested our approach against standard adversarial attacks (FGSM Goodfellow\net al. (2014), PGD-20 and PGD-100 Madry et al. (2017), and C&W Carlini & Wagner (2017)),\nall bounded within _ϵ_ = 8 _/_ 255 under _ℓ_ _∞_ -norm. We also report an Ensemble metric that measures\nthe worst-case performance across all attacks for each test example, providing a comprehensive\nrobustness assessment.\n\n\nB I MPLEMENTATION FOR S TEREO M ATCHING\n\n\nFor our stereo matching experiments, we built our implementation on top of standard stereo network\narchitectures to ensure our approach remains model-agnostic and requires no training or fine-tuning.\nWe integrated SR with PSMNet Chang & Chen (2018), a pyramid stereo matching network with a\nstacked hourglass architecture that uses 3D convolutions on a cost volume constructed by concatenating features. We apply SR at the feature extraction stage, before cost volume construction, where\nstereo correspondences are first established.\n\n\nGiven a pair of input stereo images _x_ _L_ _, x_ _R_ _∈_ R _[b][×]_ [3] _[×][h][×][w]_ (where _b_ is the batch size), our SR approach\noperates as follows. First, we create a set of (2 _d_ _x_ + 1) _×_ (2 _d_ _y_ + 1) perturbed versions of each input\nimage by applying translations within the range [ _−d_ _x_ _, d_ _x_ ] _×_ [ _−d_ _y_ _, d_ _y_ ] pixels:\n\n\n_X_ _L,_ perturbed = _{g_ _i,j_ ( _x_ _L_ ) _| i ∈_ [ _−d_ _y_ _, d_ _y_ ] _, j ∈_ [ _−d_ _x_ _, d_ _x_ ] _}_ (5)\n\n\n15\n\n\nPreprint version. Under review.\n\n\n_X_ _R,_ perturbed = _{g_ _i,j_ ( _x_ _R_ ) _| i ∈_ [ _−d_ _y_ _, d_ _y_ ] _, j ∈_ [ _−d_ _x_ _, d_ _x_ ] _}._ (6)\n\n\nAll images are concatenated into batches and processed through the feature extraction component\nof the network:\n_F_ _L_ = _ϕ_ ( _X_ _L,_ perturbed ) (7)\n_F_ _R_ = _ϕ_ ( _X_ _R,_ perturbed ) (8)\nwhere _ϕ_ represents the feature extraction component of the stereo network. This batch processing\napproach significantly improves computational efficiency compared to individual forward passes.\n\n\nAfter obtaining feature maps for all perturbed inputs, we aggregate them to create a single enhanced\nfeature map.\n\n\nOur implementation requires (2 _d_ _x_ + 1) _×_ (2 _d_ _y_ + 1) forward passes through the feature extraction\ncomponent of the network for each stereo image.\n\n\nFor evaluation, we tested our approach against adversarial attacks generated using FGSM Goodfellow et al. (2014) and I-FGSM Kurakin et al. (2018) (a special case of PGD), bounded within various\n_ϵ_ values ( _{_ 0 _._ 002 _,_ 0 _._ 005 _,_ 0 _._ 01 _,_ 0 _._ 02 _}_ ) under _ℓ_ _∞_ -norm. We measured performance using three standard stereo matching metrics: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and\nD1-error (percentage of pixels with disparity error greater than 3 pixels or 5% of the ground truth)\nLuo et al. (2018).\n\n\nC D ETAILS ABOUT A TTACK ON O PTICAL F LOW\n\n\nTo find an adversarial attack for optical flow estimated by a given neural network _f_, we utilize\na similar approach to Oskouie et al. (2024) that aims to find a perturbation _δ_ for given frames\n_F_ 1 and _F_ 2, maximizing the discrepancy between predicted and ground-truth optical flow _OF_ . If\nthe ground-truth optical flow is unavailable, we use the predicted optical flow from the unattacked\nframe as our surrogate ground-truth. Our method applies _δ_ to the first input frame, then uses a deep\nneural network to estimate optical flow from the perturbed frames. The objective is to maximize\nthe average end-point error (EPE) between the predicted and ground-truth optical flow, calculated\nas the mean Euclidean distance between corresponding 2D flow vectors. In other words, the _ϵ_ -norm\nbounded adversary _δ_ for optical-flow is calculated by optimizing the following\nmax � _OF, f_ ( _F_ 1 + _δ, F_ 2 )� _._ (9)\n_δ_ : _∥δ∥≤ϵ_ [EPE]\n\n\nOne _l_ _∞_ -bounded adversary _A_ for the aforementioned optimization problem is Fast Gradient Sign\nMethod (FGSM) Goodfellow et al. (2014) which can be obtained by\nL = EPE( _OF, f_ ( _F_ 1 _, F_ 2 )) _,_\n\n_A_ = _F_ 1 + _ϵ ·_ sign� _∇_ _F_ 1 L� _._ (10)\n\n\nProjected gradient descent (PGD) Madry et al. (2017) represents an enhanced and more complex\nversion of FGSM. This attack method generates adversarial examples through an iterative process\nand the formulation for this attack is as following\n\n_F_ 1 [(] _[t]_ [+1)] = Π _F_ 1 + _S_ � _F_ 1 [(] _[t]_ [)] + _α ·_ sign( _∇_ _F_ 1 L)� _._ (11)\n\n\nNote that in PGD, since the perturbations are considered to be too minimal to significantly alter the\nflow dynamics, the ground-truth optical flow is not updated by intermediate perturbations applied to\nthe input data.\n\n\nFor our experimental setup, we chose to set the norm value _ϵ_ at 256 [10] [. Furthermore, we configured the]\n\nPGD algorithm to run for 10 iterations. The step size _α_ was determined by dividing 2 _._ 5 _· ϵ_ by the\ntotal number of iterations, ensuring a balanced progression throughout the optimization process.\n\n\nD A DDITIONAL V ISUALIZATIONS\n\n\nHere we provide additional visualizations from our experiments comparing SR under both FGSM\nand PGD attack. We also provide a visual showing the results of FGSM and PGD pertubation on\nvarious images.\n\n\n16\n\n\nPreprint version. Under review.\n\n\nFigure 6: Visual results on stereo matching against FGSM attack, without and with SR, _ϵ_ = 0 _._ 02\n\n\nFigure 7: Visual results on stereo matching against FGSM attack, without and with SR, _ϵ_ = 0 _._ 01\n\n\nFigure 8: Visual results on stereo matching against FGSM attack, without and with SR, _ϵ_ = 0 _._ 005\n\n\nFigure 9: Visual results on stereo matching against PGD attack, without and with SR, _ϵ_ = 0 _._ 01\n\n\n17\n\n\nPreprint version. Under review.\n\n\nFigure 10: Visual results on stereo matching against PGD attack, without and with SR, _ϵ_ = 0 _._ 005\n\n\nFigure 11: Visual results on stereo matching against PGD attack, without and with SR, _ϵ_ = 0 _._ 002\n\n\n**Image** **FGSM Attacked** **FGSM Noise** **PGD Attacked** **PGD Noise**\n\n\nFigure 12: Original images and their corresponding attacked images and perturbations using FGSM\nand PGD methods on optical flow. The attacks mostly target the main object observed in the image.\n\n\n18\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03223v1",
      "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment",
      "authors": [
        "Hongxiang Zhang",
        "Yuan Tian",
        "Tianyi Zhang"
      ],
      "summary": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
      "published": "2025-10-03T17:56:33Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03223v1"
    },
    "text": "Preprint\n\n## - S ELF -A NCHOR : L ARGE L ANGUAGE M ODEL R EASON - - ING VIA S TEP BY STEP A TTENTION A LIGNMENT\n\n\n**Hongxiang Zhang& Yuan Tian & Tianyi Zhang**\nDepartment of Computer Science\nPurdue University\nLafayette, IN 47907, USA\n_{_ hxxzhang,tian211,tianyi _}_ @purdue.edu\n\n\nA BSTRACT\n\n\nTo solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate\nsteps and the original prompt will be buried in the context, receiving insufficient\nattention and leading to errors. In this paper, we propose S ELF -A NCHOR, a novel\npipeline that leverages the inherent structure of reasoning to steer LLM attention.\nS ELF -A NCHOR decomposes reasoning trajectories into structured plans and automatically aligns the model’s attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows\nthat S ELF -A NCHOR outperforms SOTA prompting methods across six benchmarks. Notably, S ELF -A NCHOR significantly reduces the performance gap between “non-reasoning” models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining.\n\n\n1 I NTRODUCTION\n\n\nReasoning abilities in Large language models (LLMs) are key to solve complex tasks, from mathematical problem solving to logical inference and multi-step reasoning (Ahn et al., 2024; Huang &\nChang, 2023; Cheng et al., 2025). Recent LLMs, such as OpenAI o1 (OpenAI, 2024) and DeepSeekR1 (DeepSeek-AI, 2025), have further advanced their reasoning capabilities through fine-tuning and\nreinforcement learning (Luo et al., 2024; et al., 2025). Howevere, despite their impressive performance, they require substantial computation and a considerable amount of training data.\n\n\nAs an alternative, prompting-based methods emerged to induce LLMs’ inherent reasoning capabilities at test time without updating model parameters. Methods such as Self-Refine (Madaan et al.,\n2023) and ReAct (Yao et al., 2023) facilitate reasoning by iteratively expanding and refining the\ngeneration process. Methods like Self-planning (Jiang et al., 2024), Plan-and-Solve (Wang et al.,\n2023), and Re-Reading (Xu et al., 2024) explicitly decompose complex problems before solving\nthem. However, a tradeoff of being training-free is that these prompting-based methods necessitate\nlong reasoning chains because they depend on iterative generation or explicit planning. While this\ncan be seen as the cost of training-free methods, recent studies have revealed another issue: longcontext reasoning can cause severe attention misalignmentt issue (Gu et al., 2024; Chi et al., 2023;\nLiu et al., 2024; Sun et al., 2024; Yao et al., 2021; Tian & Zhang, 2024; Li et al., 2024; Hong et al.,\n2025).\n\n\nAs a key component in LLMs, the attention mechanism (Vaswani et al., 2023) enables LLMs to\nselectively integrate relevant information from preceding context. However, LLM attention is an\ninherently limited resource. As the generation proceeds, the number of preceding tokens increases,\nmaking it increasingly difficult for the model to pay attention to the relevant context, especially\nwhen the context is long and complex (Tian & Zhang, 2024; Li et al., 2024; Hong et al., 2025). In\nsuch cases, even with the ability to correctly predict next token based on the corresponding context,\nLLMs may attend to irrelevant context, thereby generating off-topic or wrong results. As shown\nin Figure 1, the intermediate reasoning steps and most of the original prompt will be buried in the\nmiddle, receive insufficient attention, and consequently, introduce errors (Liu et al., 2024).\n\n\n1\n\n\nPreprint\n\n\nTo mitigate such attention misalignment issues, recent works explicitly steer LLM attention to influence generation behavior. For example, PASTA (Zhang et al., 2023) adjusts the self-attention\ndistribution within a subset of attention heads, while SPA (Tian & Zhang, 2024) simulates attention\nsteering through logit arithmetic. However, these methods mainly focus on developing robust attention steering mechanisms, while requiring humans to specify which tokens the model should pay\nmore attention to. Since such tokens can vary significantly at different generation steps and across\ndifferent tasks, it is unrealistic for humans to manually decide for every generation step.\n\n\nFigure 1: Comparison between existing reasoning methods and S ELF -A NCHOR . Due to inherent\nattention patterns, existing reasoning methods may easily overlook intermediate reasoning and make\nmistakes. S ELF -A NCHOR addresses this by decomposes the task into plans, and takes each plan as\na component for attention alignment.\n\n\nTo reduce human efforts, we introduce S ELF -A NCHOR, a novel generative pipeline that explicitly\naligns LLM attention by leveraging the inherent structure of a reasoning chain. S ELF -A NCHOR\nbuilds on two key insights: (1) complex reasoning problems can be decomposed into structured\nplans, and (2) each decomposed plan can naturally serve as a component for attention alignment.\nAs illustrated in Figure 1, S ELF -A NCHOR decomposes the original prompt into plan steps with\ncorresponding reasoning steps. During generation, it automatically selects and steers the model\nattention to the prompt and the corresponding plan. This enables the LLM to keep attending to both\nthe problem statement and immediate reasoning goals, thereby preventing attention mistakes among\ncontinuously expanding reasoning steps.\n\n\nWe evaluate S ELF -A NCHOR on six benchmarks and six base LLMs with varying sizes, and compare\nit with SOTA prompting methods. The evaluation benchmarks include three mathematical reasoning\nbenchmarks (GSM8K, AQuA, MATH), two commonsense benchmarks (StrategyQA & Things for\nDoing), and an multi-task evaluation benchmark (BIG-Bench Hard, BBH). S ELF -A NCHOR consistently improves accuracy on all the settings, outperforming all the prompting-based baselines by at\nleast 5.44% on average. In addition, we demonstrate that S ELF -A NCHOR achieves performance on\npar with five reasoning models but with substantially lower cost and complexity, suggesting a practical alternative to applying reinforcement learning to enhance the reasoning capability of LLMs.\n\n\n2 M ETHOD\n\n\n2.1 S ELF -A NCHOR\n\n\n**Reasoning as the Scaffold for Attention Alignment.** We are inspired by the observation that planning offers a natural scaffold for attention alignment. Specifically, planning helps the model to\nunderstand and break down complex problems into subproblems (Jiang et al., 2024; Wang et al.,\n2023). Each decomposed plan step provides a correspondence to subsequent reasoning, which can\nnaturally serve to guide attention alignment. Building on this idea, we design S ELF -A NCHOR to\nleverage the inherent structure in the reasoning chain to conduct the attention alignment. S ELF \n\n2\n\n\nPreprint\n\n\nA NCHOR prompts the model to decompose the prompt into plans, and then generates and corresponding reasoned solutions with explicit attention steering.\n\n\n**Attention Steering Mechanism.** Several studies have explored methods for attention steering (Zhang et al., 2023; Shi et al., 2023; Tian & Zhang, 2024). These methods primarily address\n_how to steer the model attention_ . By contrast, our work focuses on an orthogonal problem— _how_\n_to automatically select relevant context_ and uses these methods as a plug-and-play component. In\nthis work, we choose Selective Prompt Anchoring (SPA) (Tian & Zhang, 2024) as the underlying\nattention steering mechanism, since it is low-cost and efficient. We briefly summarize how SPA\nworks here and refer interested readers to the original paper for more technical details.\n\n\nGiven a set of tokens that an LLM should pay more attention to, SPA simulates attention steering\nthrough logit arithmetic. Specifically, it estimates the influence of selected tokens by contrasting the\noriginal logits with the logits when these tokens are masked, and then add this influence back to the\noriginal logits. The steered logit is represented by the linear combination of the original logits and\nthe logits with selected tokens masked, where _ω_ _i_ is a coefficient determining the attention steering\nstrength. Formally, it can be represented as\n\n**logits** [steered] _i_ = _ω_ _i_ _·_ **logits** [original] _i_ + (1 _−_ _ω_ _i_ ) _·_ **logits** [mask] _i_ _._ (1)\n\n\n**Selection of Attention Alignment.** Building on this attention steering mechanism, we propose a\nnovel strategy that dynamically aligns model attention to changing context tokens during generation.\n\n\nAccording to the attention sink phenomenon (Xiao et al., 2023), the model already attends strongly\nto initial tokens and the recent generated tokens. Our method complementarily steer the model\nattention to the planning steps when conducting the corresponding reasoning. Furthermore, S ELF A NCHOR additionally steers the model attention to the question in the prompt, which serves as the\ncore generation purpose, ensuring the model keeps focusing on the problem statement. [1]\n\n\nFormally, we use _f_ ( _x,_ **S** ) to represent the generation function with attention steering, where _x_ represents the entire preceding tokens and **S** represents the selected tokens where the model’s attention\nshould be steered to. Let sys denote the system prompt together with high-level background instructions (e.g., _“You are a helpful assistant”_ ), and let _Q_ denote the core question under consideration.\nThus, the original prompt can be represented as concat(sys _, Q_ ). The generation of S ELF -A NCHOR\nconsists of two parts: the _planning_ and the corresponding _reasoning_ .\n\n\nAt step _i_, the planning is generated by\n\nplan _i_ = _f_ �concat�sys _, Q,_ plan 1 _,_ plan 2 _, . . .,_ plan _i−_ 1 � _,_ _**Q**_ � _._ (2)\n\nThe planning is generated by\n\nreason _i_ = _f_ �concat�sys _, Q,_ plan 1 _,_ plan 2 _, . . .,_ plan _i−_ 1 � _,_ **concat** ( _**Q**_ **, plan** i )� _._ (3)\n\nThe generation proceeds by alternating between plan _i_ and reason _i_, until the process terminates with\nthe final result.\n\n\n**Dynamic Tuning of Attention Alignment Strength.** As reasoning trajectories progress, the required degree of attention alignment may vary at different steps. According to prior work (Geng\net al., 2024; Fu et al., 2025), LLMs’ predicted probability distribution can be viewed as a confidence\nsignal to determine their prediction quality. High-confidence prediction suggests reliable generation,\nimplying a correct attention, whereas low confidence may indicate unreliable generation and attention drift. Therefore, we introduce a step-level anchoring strength _ω_ _i_ that is dynamically adjusted\nbased on model confidence. [2] Let _P_ _i_ = _{p_ 1 _, p_ 2 _, . . ., p_ _m_ _}_ represent the predicted probability at step\n_i_ . We calculate the confidence score using the harmonic mean of _P_ _i_\n\n\n_n_\n_p_ avg = ~~�~~ ~~_n_~~ _i_ =1 _p_ 1 _i_ _._ (4)\n\nThis confidence score serves as an additional factor to scale the attention steering strength _w_ _i_ in\nEquation 1. We discuss detailed design choices and experiments in Appendix B.\n\n\n1 Alternative anchoring strategies are discussed in Appendix D\n2 Our strength adjustment strategy builds upon the confidence-modulated strength strategy in SPA (Tian &\nZhang, 2024). While SPA adjusts the strength based on confidence for each logit at the vocabulary level, we\nintroduce an additional factor to adjust the strength for each step.\n\n\n3\n\n\nPreprint\n\n\n3 E XPERIMENTS\n\n\n3.1 B ENCHMARKS\n\n\nWe evaluated S ELF -A NCHOR on six benchmarks. The first three benchmarks incorporate\nGSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), and MATH (Hendrycks et al., 2021) represent arithmetic reasoning. The second two benchmarks include StrategyQA (Geva et al., 2021), and\nThinking for Doing (T4D) (Zhou et al., 2023) represent commonsense reasoning. Lastly, we evaluated on a subset of BIG-Bench Hard (BBH) (Suzgun et al., 2022), which covers a diverse range of\nreasoning problems spanning the multi-step algorithmic reasoning, natural language understanding,\nthe application of world knowledge, and Multilingual Knowledge. We report final answer accuracy\nacross all benchmarks [3] .\n\n\n3.2 M ODELS AND B ASELINES\n\n\n**Base Models.** We conduct our experiments on six non-reasoning LLMs spanning various scales.\nFor non-reasoning models, we select Llama-3.1-8B-Instruct (Grattafiori et al., 2024), Llama-3.2-3BInstruct (Grattafiori et al., 2024), Phi-4-mini-instruct (Abouelenin et al., 2025), Qwen3-4B-Instruct2507 (Team, 2025), Phi-4 (Abdin et al., 2024), and Qwen3-30B-A3B-Instruct-2507 (Team, 2025).\n\n\n**Comparison Baselines.** We compare our method against three representative prompting methods for LLM reasoning. First, we include **CoT** (Wei et al., 2022; Kojima et al., 2022), a widely\nused baseline that models are prompted to generate a reasoning process leading to the final answer.\nSecond, we include **Plan-and-Solve+ (PS+)** (Wang et al., 2023), a prompting method that models\nare prompted to first generate a plan and then solve the problem. Third, we include **Re-Reading**\n**(RE2)** (Xu et al., 2024), which asks the model to read the question again and then solve the problem.\n\n\nFurthermore, we consider five state-of-the-art reasoning LLMs as baselines to see if non-reasoning\nLLMs combined with S ELF -A NCHOR achieve competitive performance against reasoning models. The reasoning models include Phi-4-mini-reasoning (Abouelenin et al., 2025), Qwen34B-Thinking-2507 (Team, 2025), DeepSeek-R1-Distill-Llama-8B (DeepSeek-AI, 2025), Phi-4reasoning (Abdin et al., 2024), and Qwen3-30B-A3B-Thinking-2507 (Team, 2025).\n\n\n3.3 M AIN R ESULTS\n\n\n**Mathematical Reasoning.** Arithmetic reasoning represents one of the most challenging aspects of\nLLM reasoning capabilities. As shown in Table 1, S ELF -A NCHOR consistently improves accuracy\nacross three arithmetic benchmarks. These gains reach over 10% improvements on GSM8K, over\n5% on AQuA, and up to 8% on MATH across most models, outperforming all competing methods.\nWhile PS+ and RE2 also demonstrate potential for enhancing mathematical reasoning performance,\nour experiments show performance degradation on certain LLMs, suggesting limited generalization\ncapabilities.\n\n\nInterestingly, the three benchmarks span increasing difficulty levels, from grade-school problems (Cobbe et al., 2021) to GMET/GRE (Ling et al., 2017) level and competition-level problems\n(Hendrycks et al., 2021). S ELF -A NCHOR demonstrates performance gain in all three benchmarks,\nsuggesting superior generalization capabilities across diverse model architectures and reasoning\ncomplexity levels.\n\n\n**Commonsence Reasoning.** StrategyQA requires multi-hop reasoning over commonsense knowledge. As detailed in Table 1, S ELF -A NCHOR persistently improves accuracy across six evaluated\nLLMs. In contrast, PS+ and RE2 occasionally outperform the baseline CoT method.\n\n\nFor T4D, a grounded social agent reasoning task requires mental state reasoning to determine appropriate actions. S ELF -A NCHOR demonstrates significant performance gains over 9% in four LLMs.\nIn comparison, both PS+ and RE2 exhibit mixed performance; they tend to be more effective in\nlarger models. These findings highlight the challenge of applying generic prompting strategies to\nspecialized reasoning domains.\n\n\n3 Prompt templates and evaluation details are provided in Appendix E.\n\n\n4\n\n\nPreprint\n\n\nTable 1: Evaluation results on six benchmarks. Best results are shown in **green**, and those indicating\na performance drop compared to standard greedy decoding are shown in grey.\n\n\nMath CommonSense\n\n\n\n**Model** **Method**\n\n\n\nBBH\nGSM8K AQuA MATH StrQA T4D\n\n\n\n\n\n\n\n**BBH.** BBH aggregates challenging algorithmic and symbolic tasks. S ELF -A NCHOR demonstrates\naverage performance gains ranging from 1.61% to 15.39% [4] . Among all sub-benchmarks, we find\nthat tasks requiring tracking of intermediate reasoning benefit the most, for example _date under-_\n_standing_, and _logical deduction_ . We attribute this to S ELF -A NCHOR ’s attention steering that augments critical reasoning steps and the original question throughout generation. In contrast, PS+ and\nRE2 show inconsistent improvements.\n\n\nIn summary, these results highlight two takeaways. First, while prompting strategies are effective\nin some tasks, they lack robustness across benchmarks and model architectures, tending to be more\neffective on larger LLMs and simpler reasoning tasks. This may be because larger models are more\ncapable of following instructions and external guidance to align reasoning trajectories. Second,\nby integrating planning, structured reasoning, and automatic anchoring, S ELF -A NCHOR achieves\nconsistent improvements across tasks, model sizes, and architectures, demonstrating both robustness\nand effectiveness.\n\n\n3.4 C AN S ELF -A NCHOR R IVAL RL- ENHANCED T HINKING M ODEL ?\n\n\nRecent advances in reasoning capabilities have been dominated by reinforcement learning-enhanced\n“thinking” models that employ extensive internal reasoning chains during inference. However, these\nmodels are costly to fine-tune and require large-scale training data. This raises a question: _Can non-_\n_reasoning LLMs combined with_ S ELF -A NCHOR _achieve competitive performance against reasoning_\n_models?_ To investigate this question, we compare our method applied to non-reasoning LLMs\nagainst corresponding thinking models. Since thinking models typically require longer generated\ncontexts to support their internal reasoning, we set 1.5x larger maximum token length than the nonreasoning models in our experiments.\n\n\nTable 2 presents our findings across mathematical reasoning, commonsense reasoning, and symbolic\nreasoning tasks. Remarkably, our method applied to non-thinking models achieves competitive or\nsuperior performance compared to RL-enhanced thinking models. Specifically, our approach closes\nthe performance gap significantly with three arithmetic benchmarks in varying difficulties, achieving\n\n\n4 We detail the subtask performance in Appendix G\n\n\n5\n\n\nPreprint\n\n\n\nTable 2: Evaluation comparison with thinking models\n\n\nMath CommonSense\n\n\n\n**Model** **Method**\n\n\n\nBBH\nGSM8K AQuA MATH StrQA T4D\n\n\n\nCoT 75.36 61.81 51.00 67.03 39.54 60.51\n\nPhi-4-mini-4B S ELF -A NCHOR 88.02 68.50 59.00 68.69 49.47 62.42\nReasoning 81.27 60.62 75.00 66.38 45.04 59.85\n\n\nCoT 86.66 73.62 82.00 68.03 70.21 73.33\n\nQwen3-4B S ELF -A NCHOR 87.26 79.92 86.50 70.13 71.56 75.31\nReasoning 83.24 67.32 87.00 68.31 73.40 75.34\n\n\nCoT 61.85 50.79 44.50 70.24 26.77 49.45\n\nLlama3.1-8B S ELF -A NCHOR 76.72 55.51 52.50 73.54 40.01 58.53\nReasoning 73.62 62.99 72.50 65.41 48.58 64.98\n\n\nCoT 73.16 68.11 74.50 77.51 73.94 72.08\n\nPhi-4-15B S ELF -A NCHOR 82.41 79.13 81.00 77.82 85.99 75.31\nReasoning 81.12 83.20 95.5 75.43 74.11 74.98\n\n\nCoT 84.46 81.10 78.00 78.60 84.92 67.69\nQwen3-30B S ELF -A NCHOR 87.41 83.46 87.00 79.65 85.56 69.30\nReasoning 94.5 80.31 85.00 77.26 80.96 76.54\n\n\nwithin 5% difference of most thinking models; On commonsense reasoning tasks and BBH, S ELF A NCHOR exceeds thinking model performance on most benchmarks and LLMs.\n\n\nNoteably, we observe that thinking models demonstrate superior performance on tasks where corresponding non-reasoning models show poor baseline performance. For example, Llama3.1 and\nPhi-4-mini show large gaps on MATH, and Llama3.1 underperforms on AQuA and BBH. In these\nsettings, post-training with reinforcement learning significantly boosts performance in areas where\nmodels previously performed poorly. In contrast, for tasks where non-reasoning models already\ndemonstrate strong performance, reinforcement learning provides limited improvement. This pattern is also observed in Kirk et al. (2023). Nevertheless, S ELF -A NCHOR shows consistent performance improvements across all tasks and difficulty levels.\n\n\nIn summary, rather than learning implicit reasoning patterns through training, our approach leverages the inherent structure in the reasoning chain for attention alignment to improve the reasoning\ncapabilities, yielding stable improvement across varying difficulty levels without additional training\ncost. These findings suggest that S ELF -A NCHOR can serve as an effective alternative to computationally expensive RL-enhanced reasoning.\n\n\n3.5 W HICH T YPES OF T ASKS D O S ELF -A NCHOR H ELP THE M OST ?\n\n\nTo understand which types of tasks S ELF -A NCHOR help the most, we analyze performance gains\nacross task complexity. We adapt the method for quantifying task complexity from Wu et al. (2025);\nJin et al. (2024), where each task is represented by an individual question in a benchmark. For each\nbenchmark, 200 questions are randomly sampled to compute the average accuracy per task across\nall experimented models. Task complexity is then defined as 1 _−_ accuracy [5], where lower accuracy\nindicates higher complexity.\n\n\n**Reasoning Task Complexity.** First, we examine the relationship between task complexity and\nS ELF -A NCHOR ’s performance gains to understand how our method scales with task difficulty. We\ncompare S ELF -A NCHOR ’s performance gains against RE2, a strong baseline identified in our main\nresults (subsection 3.3). For each task, we compute performance gain as the difference in accuracy\nbetween the two methods.\n\n\nAs shown in Figure 2a, the box plot summarizes the distribution of performance gains for tasks\nacross complexity ranges. While tasks in the 0.67–0.83 complexity range show a few negative cases,\nthe average improvement remains positive across all task complexities. Overall, S ELF -A NCHOR\npresents performance gains spanning all complexity levels, achieving approximately 7% perfor\n\n5 Segmenting reasoning steps and task complexity details are provided in Appendix E.\n\n\n6\n\n\nPreprint\n\n|60<br>40<br>20<br>0<br>20<br>40<br>60<br>(0.17-0.33) (0.33-0.50) (0.5<br>Task Comp|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>|||||||||\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>|||||||||\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>|||||||||\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>|||||||||\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>|||||||||\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>|||||||||\n|(0.17~~-~~0.33)<br>(0.33~~-~~0.50)<br>(0.5<br>Task Comp<br>60<br>40<br>20<br>0<br>20<br>40<br>60<br>||||~~-~~0.67)<br>(0.67~~-~~0.83)<br>(0.83~~-~~1.00)<br>exity Range|~~-~~0.67)<br>(0.67~~-~~0.83)<br>(0.83~~-~~1.00)<br>exity Range|~~-~~0.67)<br>(0.67~~-~~0.83)<br>(0.83~~-~~1.00)<br>exity Range|~~-~~0.67)<br>(0.67~~-~~0.83)<br>(0.83~~-~~1.00)<br>exity Range|~~-~~0.67)<br>(0.67~~-~~0.83)<br>(0.83~~-~~1.00)<br>exity Range|\n\n\n\n(a) Reasoning Tasks Complexity and performance\ngains comparison\n\n\n\n|30<br>25|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|25<br>30<br>||||||||||\n|(0.00~~-~~0.13)<br>(0.13~~-~~0.27)<br>(0.40~~-~~0.53)<br>(0.53~~-~~0.67)<br>(0.67~~-~~0.80)<br>Task Complexity Range<br>5<br>10<br>15<br>20<br><br>||||||||||\n|(0.00~~-~~0.13)<br>(0.13~~-~~0.27)<br>(0.40~~-~~0.53)<br>(0.53~~-~~0.67)<br>(0.67~~-~~0.80)<br>Task Complexity Range<br>5<br>10<br>15<br>20<br><br>||||||||||\n|(0.00~~-~~0.13)<br>(0.13~~-~~0.27)<br>(0.40~~-~~0.53)<br>(0.53~~-~~0.67)<br>(0.67~~-~~0.80)<br>Task Complexity Range<br>5<br>10<br>15<br>20<br><br>||||||||||\n|(0.00~~-~~0.13)<br>(0.13~~-~~0.27)<br>(0.40~~-~~0.53)<br>(0.53~~-~~0.67)<br>(0.67~~-~~0.80)<br>Task Complexity Range<br>5<br>10<br>15<br>20<br><br>||||||||||\n\n\n(b) Reasoning Tasks Complexity and reasoning chain\nlength comparison. The lower value represents the\neasier task.\n\n\n\nFigure 2: Analysis of task complexity and reasoning chain length\n\n\nmance improvement. These results demonstrate that S ELF -A NCHOR achieves consistent gains and\ngeneralizes effectively across varying task complexities.\n\n\n**Reasoning Chain Length vs. Task Complexity.** Next, we analyze how reasoning chain length\nscales with task difficulty. Figure 2b shows the distribution of successful reasoning chain lengths\nacross tasks of varying difficulty on Llama3.2-3B. The results show a clear trend that as complexity\nincreases, S ELF -A NCHOR tends to generate longer reasoning chains. This aligns with the observation in Wu et al. (2025) that harder problems require longer reasoning chains to solve. We attribute\nthis capability to the attention steering mechanism, which enables the model to focus on both problem context and immediate reasoning object throughout the reasoning, preventing attention drift as\nthe reasoning chain extends.\n\n\nIn summary, our analysis highlights two takeaways: (1) S ELF -A NCHOR demonstrates consistent\nimprovements across all complexity levels, confirming its ability to generalize beyond narrow task\ncategories. (2) S ELF -A NCHOR encourage the model to generate longer reasoning chains for difficult\nproblems, supporting its effectiveness in scaling to complex tasks.\n\n\n3.6 E FFICIENCY\n\n\nTable 3: Efficiency comparison\n\n\nToken/Second\n\n**Model**\n\nS ELF -A NCHOR CoT PS+ RE2\n\n\nLlama3.2-3B 9.97 10.87 10.84 12.01\n\n\nPhi-4-mini-4B 11.02 12.25 12.24 12.21\n\n\nQwen3-4B 5.41 6.22 6.22 6.23\n\n\nLlama3.1-8B 7.54 8.38 8.26 8.19\n\n\nPhi-4-15B 7.92 9.00 8.94 8.88\n\n\nQwen3-30B 1.07 1.39 1.39 1.37\n\n\nS ELF -A NCHOR achieved superior performance compared to state-of-the-art methods; we are committed to evaluating its efficiency. Table 3 reports inference throughput (tokens/sec) for S ELF A NCHOR compared to baseline methods. On average, S ELF -A NCHOR introduces minimal computational overhead, with throughput reductions typically below 1–2 tokens/sec relative to baselines. The results highlight a favorable trade-off; S ELF -A NCHOR consistently improves reasoning\nwhile maintaining comparable runtime cost. Importantly, the cost-performance balance remains\nstable across both smaller (3B–4B) and larger (15B–30B) models, suggesting scalability without\nprohibitive latency.\n\n\n7\n\n\nPreprint\n\n\nTable 4: Performance comparing S ELF -A NCHOR with and without attention steering\n\n\nMath CommonSense\n\n\n\n**Model** **Method**\n\n\n\nBBH\nGSM8K AQuA MATH StrQA T4D\n\n\n\nS ELF -A NCHOR (w/o attention steering) 73.01 53.15 40.90 71.17 35.28 52.77\nLlama3.1-8B\nS ELF -A NCHOR 76.72 55.51 52.50 73.54 40.01 58.53\n\n\nPhi-4-mini-4B S ELF -A NCHOR (w/o attention steering) 78.77 58.66 55.50 68.60 38.83 57.44\nS ELF -A NCHOR 88.02 68.50 59.00 68.69 49.47 62.42\n\n\nTable 5: An example of BBH. Incorrect inferences are highlighted in red. Correct ones are highlighted in **green** .\n\n\n_**Sentence: The secretary came to the analyst’s office and helped her to book a flight.**_\n**Prompt**\n_**Options: (A) Helped the secretary book a flight (B) Helped the analyst book a flight (C) Ambiguous**_\n\n\n**S** **ELF** **-A** **NCHOR** _{_ ”Step 1: Identify the pronoun”: ”The pronoun is ’her’.”,\n(w/o attention steering) ”Step 2: Determine the antecedent of the pronoun”: ”The antecedent of ’her’ is the subject of the sentence, which is ’The secretary’”,\n”Step 3: Analyze the sentence structure”: The subject of the sentence is ’The secretary’, and the verb is ’came’. The object of the\nverb ’came’ is ’to the analyst’s office’. The verb ’helped’ has the pronoun ’her’ as its object.”,\n”Step 4: Determine the antecedent of the pronoun based on the sentence structure”: ”Based on the sentence structure, the antecedent\nof the pronoun ’her’ is the subject of the sentence, which is ’The secretary’.”,\n”Step 5: Choose the correct option”: ”The correct option is (A) Helped the secretary book a flight.” _}_\n\n\n**S** **ELF** **-A** **NCHOR** _{_ ”Step 1: Identify the pronoun”: ”The pronoun in the sentence is ’her’.”,\n”Step 2: Determine the antecedent of the pronoun”: ”To find the antecedent, we need to look for the noun that ’her’ refers to. In this\ncase, **’her’ is a possessive pronoun that refers to the analyst.** ”,\n”Step 3: Analyze the sentence structure”: ”The subject of the sentence is ’The secretary’, and the object of the verb ’helped’ is ’her’”,\n”Step 4: Identify the antecedent based on the sentence structure”: ””Step 5: Choose the correct option”: ”Since ’her’ refers to the analyst, the correct option is **’her’ refers to the analyst, not the secretary. (B) Helped the analyst book a flight.** ”, ” _}_\n\n\nIn summary, S ELF -A NCHOR achieves strong effectiveness and efficiency, offering improved reasoning performance with only minor latency overhead compared to state-of-the-art baselines.\n\n\n3.7 A BLATION STUDY : T HE EFFECTIVENESS OF ATTENTION STEERING\n\n\nTo isolate the contribution of attention steering, we conducted an ablation study comparing S ELF A NCHOR with and without attention steering on two representative LLMs across all six benchmarks.\nAs illustrated in Table 4, S ELF -A NCHOR consistently outperformed its variant without attention\nsteering, across all benchmarks, demonstrating the effectiveness of attention steering.\n\n\nTable 5 further illustrates a representative example where attention steering prevents reasoning errors. In Steps 2 and 4, the S ELF -A NCHOR without attention steering approach incorrectly identifies\n“her” as referring to “the secretary”; this may be because models over-focus on sentence subjects\nrather than maintaining focus on the syntactic relationships that determine pronoun reference. In\ncontrast, S ELF -A NCHOR correctly identifies that “her” refers to “the analyst” by maintaining attention on both the original question context and the current reasoning step [6] .\n\n\nThis ablation confirms that, while structured reasoning provides a foundation for improved performance, it is often insufficient to prevent attention drift on its own. The attention anchoring component is crucial to ensure the model maintains focus throughout the reasoning process, leading to\nmore robust and accurate results.\n\n\n3.8 F AILURE C ASE A NALYSIS\n\n\nTo understand the failure modes in S ELF -A NCHOR . We conducted a manual failure case analysis\non 200 randomly sampled cases from _casual_ ~~_j_~~ _udgement_ and _AQuA_ . Our analysis identified three\nprimary failure modes:\n\n\n**Reasoning Errors (42%).** The most frequent failure mode involves LLM making mistakes during\nthe reasoning. These include the misapplication of causal principles, flawed deductions, and incorrect conditional reasoning. For example, in one _casual_ ~~_j_~~ _udgement_ instance, the model incorrectly\ntreated a necessary but insufficient condition as the sole causal factor, leading to an invalid conclu\n\n6 We provide additional case studies in Appendix F for further illustration.\n\n\n8\n\n\nPreprint\n\n\nsion. Such errors suggest that, while attention anchoring helps maintain focus on relevant steps, it\ncannot fully compensate for weak logical priors or gaps in world knowledge.\n\n\n**Misunderstanding the problem (36.5%).** A substantial portion of failures stems from incomprehension of complex questions, leading to errors such as misidentifying all variables, misinterpreting\nthe problem’s requirements, or incorrectly parsing the relationships between entities. For instance,\nin multi-variable problems, the model sometimes assigns numerical values to the wrong entity.\n\n\n**Computational Errors (21.5%).** The remaining failures involve arithmetic mistakes, unit conversion errors, or algebraic slips. Even when the reasoning chain is correct, a single miscalculation\noften propagates to the final answer.\n\n\nTaken together, these findings suggest that S ELF -A NCHOR primarily mitigates _attention misalign-_\n_ment_, but does not fully resolve deeper issues of logical validity, semantic understanding, or computational precision.\n\n\n4 R ELATED WORK\n\n\n**Prompt engineering for reasoning.** Prompt engineering has been widely adopted as a fundamental approach for enhancing LLM reasoning capabilities (Liu et al., 2023; Brown et al., 2020). A\nfoundational line of work, initiated by Chain-of-Thought (CoT) prompting (Wei et al., 2022), encourages explicit intermediate steps, which significantly improve performance on multi-step reasoning tasks. This has inspired numerous derivatives, including problem decomposition methods (Wang\net al., 2023; Zhou et al., 2022; Khot et al., 2022; Drozdov et al., 2022), as well as techniques focused\non enhancing query comprehension (Xu et al., 2024; Zheng et al., 2023; Mekala et al., 2023; Deng\net al., 2023; Mishra & Nouri, 2022).\n\n\nWhile these prompting methods demonstrate effectiveness in specific domains, they rely on predetermined, static prompt formats for different tasks. On the other hand, LLMs remain sensitive to\nprompt variations and suffer from attention dilution during long generations (Liu et al., 2024; Li\net al., 2024; Hong et al., 2025; Lu et al., 2021; Gu et al., 2024). S ELF -A NCHOR addresses these\nlimitations by integrating structured reasoning with dynamic attention steering. It goes beyond static\nprompting by enabling the model to recalibrate its focus on the most salient context at each step of\nthe reasoning trajectory.\n\n\n**Attention steering.** In contrast to the aforementioned prompt engineering, which devises better\nprompt strategies, attention steering methods directly guide LLMs during inference to emphasize\nthe user-specified part of context. Specifically, Selective Prompt Anchoring (SPA) (Tian & Zhang,\n2024) adjusts the logit probability distribution to emphasize the specified context. PASTA(Zhang\net al., 2023) identifies and reweights a subset of attention heads to redirect the model’s attention\nto user-specified parts. Selective Self-Attention (SSA) (Zhang et al., 2024) augments the softmax\nnonlinearity with a principled temperature scaling strategy. TOAST (Shi et al., 2023) learns feature\nselection modules that guide attention toward task-relevant information. However, these methods\nrequire manual specification of anchor content, limiting their adaptability to diverse reasoning contexts. Real-world applications demand automatic identification of relevant context elements across\nvarying task requirement and reasoning patterns. S ELF -A NCHOR addresses this limitation by leveraging structured intermediate representations to enable context-aware anchor selection without human intervention.\n\n\n5 C ONCLUSION\n\n\nWe presented S ELF -A NCHOR, a lightweight pipeline that leverages the inherent structure of reasoning for attention alignment. Across six diverse reasoning benchmarks, S ELF -A NCHOR consistently\noutperforms existing baselines. Notably, S ELF -A NCHOR enhanced “non-reasoning” models achieve\ncompetitive performance with specialized reasoning models while maintaining significantly lower\ncost. Moreover, our analysis reveals that S ELF -A NCHOR ’s advantages are generalizable to varying\ntask complexities. We hope S ELF -A NCHOR serves as a step toward more reliable LLMs reasoning\nthat requires neither parameter updates nor additional sampling.\n\n\n9\n\n\nPreprint\n\n\n6 R EPRODUCIBILITY STATEMENT\n\n\nWe have made extensive efforts to ensure the reproducibility of our work. Additional implementation details, hyperparameters, and ablation studies are provided in the appendix. We also include\ncomplete descriptions of benchmark datasets, sampling procedures, and task complexity measures.\n\n\nLastly, to foster reproducibility and further research, source code will be made publicly available\nupon acceptance of this paper.\n\n\n7 E THICS STATEMENT\n\n\nThis paper does not involve any ethical concerns. The proposed methods focus on improving reasoning ability and robustness in LLMs and do not raise issues related to the code of ethics.\n\n\nR EFERENCES\n\n\nMarah Abdin, Jyoti Aneja, Harkirat Behl, S´ebastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. _arXiv preprint arXiv:2412.08905_, 2024.\n\n\nAbdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin\nBao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical\nreport: Compact yet powerful multimodal language models via mixture-of-loras. _arXiv preprint_\n_arXiv:2503.01743_, 2025.\n\n\nJanice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models\nfor mathematical reasoning: Progresses and challenges. In Neele Falk, Sara Papi, and Mike\nZhang (eds.), _Proceedings of the 18th Conference of the European Chapter of the Association_\n_for Computational Linguistics: Student Research Workshop_, pp. 225–237, St. Julian’s, Malta,\nMarch 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.eacl-srw.17.\n[URL https://aclanthology.org/2024.eacl-srw.17/.](https://aclanthology.org/2024.eacl-srw.17/)\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. _Advances in neural information processing systems_, 33:1877–1901, 2020.\n\n\nFengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, and Zhouchen Lin. Empowering llms with logical reasoning: A comprehensive survey. _arXiv preprint arXiv:2502.15652_,\n2025.\n\n\nTa-Chung Chi, Ting-Han Fan, and Alexander I Rudnicky. Attention alignment and flexible positional\nembeddings improve transformer length extrapolation. _arXiv preprint arXiv:2311.00684_, 2023.\n\n\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola:\nDecoding by contrasting layers improves factuality in large language models. In _The Twelfth_\n_International Conference on Learning Representations_ [, 2024. URL https://openreview.](https://openreview.net/forum?id=Th6NyL07na)\n[net/forum?id=Th6NyL07na.](https://openreview.net/forum?id=Th6NyL07na)\n\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.\n\n\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\n[2025. URL https://arxiv.org/abs/2501.12948.](https://arxiv.org/abs/2501.12948)\n\n\nYihe Deng, Weitong Zhang, Zixiang Chen, and Quanquan Gu. Rephrase and respond: Let large\nlanguage models ask better questions for themselves. _arXiv preprint arXiv:2311.04205_, 2023.\n\n\nAndrew Drozdov, Nathanael Sch¨arli, Ekin Aky¨urek, Nathan Scales, Xinying Song, Xinyun Chen,\nOlivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models.\n_arXiv preprint arXiv:2209.15003_, 2022.\n\n\n10\n\n\nPreprint\n\n\n[Guiyao Tie et al. A survey on post-training of large language models, 2025. URL https://](https://arxiv.org/abs/2503.06072)\n[arxiv.org/abs/2503.06072.](https://arxiv.org/abs/2503.06072)\n\n\nYichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025.\n[URL https://arxiv.org/abs/2508.15260.](https://arxiv.org/abs/2508.15260)\n\n\nJiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. A survey\nof confidence estimation and calibration in large language models. In Kevin Duh, Helena Gomez,\nand Steven Bethard (eds.), _Proceedings of the 2024 Conference of the North American Chapter_\n_of the Association for Computational Linguistics: Human Language Technologies (Volume 1:_\n_Long Papers)_, pp. 6577–6595, Mexico City, Mexico, June 2024. Association for Computational\n[Linguistics. doi: 10.18653/v1/2024.naacl-long.366. URL https://aclanthology.org/](https://aclanthology.org/2024.naacl-long.366/)\n[2024.naacl-long.366/.](https://aclanthology.org/2024.naacl-long.366/)\n\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of_\n_the Association for Computational Linguistics_, 9:346–361, 2021.\n\n\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd\nof models. _arXiv preprint arXiv:2407.21783_, 2024.\n\n\nXiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and\nMin Lin. When attention sink emerges in language models: An empirical view. _arXiv preprint_\n_arXiv:2410.10781_, 2024.\n\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _NeurIPS_,\n2021.\n\n\nKelly Hong, Anton Troynikov, and Jeff Huber. Context rot: How increasing input tokens im[pacts llm performance. Technical report, Chroma, July 2025. URL https://research.](https://research.trychroma.com/context-rot)\n[trychroma.com/context-rot.](https://research.trychroma.com/context-rot)\n\n\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), _Findings of the As-_\n_sociation for Computational Linguistics: ACL 2023_, pp. 1049–1065, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.67. URL\n[https://aclanthology.org/2023.findings-acl.67/.](https://aclanthology.org/2023.findings-acl.67/)\n\n\nXue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin\nJiao. Self-planning code generation with large language models. _ACM Transactions on Software_\n_Engineering and Methodology_, 33(7):1–30, 2024.\n\n\nMingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and\nMengnan Du. The impact of reasoning step length on large language models. _arXiv preprint_\n_arXiv:2401.04925_, 2024.\n\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\nSabharwal. Decomposed prompting: A modular approach for solving complex tasks. _arXiv_\n_preprint arXiv:2210.02406_, 2022.\n\n\nRobert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward\nGrefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and\ndiversity. _arXiv preprint arXiv:2310.06452_, 2023.\n\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. _Advances in neural information processing systems_,\n35:22199–22213, 2022.\n\n\nKenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda Vi´egas, Hanspeter Pfister, and\nMartin Wattenberg. Measuring and controlling instruction (in)stability in language model dialogs,\n[2024. URL https://arxiv.org/abs/2402.10962.](https://arxiv.org/abs/2402.10962)\n\n\n11\n\n\nPreprint\n\n\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Regina Barzilay and Min-Yen\nKan (eds.), _Proceedings of the 55th Annual Meeting of the Association for Computational Lin-_\n_guistics (Volume 1: Long Papers)_, pp. 158–167, Vancouver, Canada, July 2017. Association for\n[Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.](https://aclanthology.org/P17-1015/)\n[org/P17-1015/.](https://aclanthology.org/P17-1015/)\n\n\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts. _Transactions of the_\n_Association for Computational Linguistics_, 12:157–173, 2024. doi: 10.1162/tacl ~~a 0~~ 0638. URL\n[https://aclanthology.org/2024.tacl-1.9/.](https://aclanthology.org/2024.tacl-1.9/)\n\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM computing surveys_, 55(9):1–35, 2023.\n\n\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered\nprompts and where to find them: Overcoming few-shot prompt order sensitivity. _arXiv preprint_\n_arXiv:2104.08786_, 2021.\n\n\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li,\nLei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning\n[in language models by automated process supervision, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2406.06592)\n[abs/2406.06592.](https://arxiv.org/abs/2406.06592)\n\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. _Advances in Neural Information Processing Systems_, 36:46534–46594, 2023.\n\n\nRajasekhar Reddy Mekala, Yasaman Razeghi, and Sameer Singh. Echoprompt: instructing the\nmodel to rephrase queries for improved in-context learning. _arXiv preprint arXiv:2309.10687_,\n2023.\n\n\nSwaroop Mishra and Elnaz Nouri. Help me think: A simple prompting strategy for non-experts to\ncreate customized content with models. _arXiv preprint arXiv:2208.08232_, 2022.\n\n\n[OpenAI. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720.](https://arxiv.org/abs/2412.16720)\n\n\nBaifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast: Transfer learning via attention steering.\n_arXiv preprint arXiv:2305.15542_, 2023.\n\n\nMingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language\nmodels. _arXiv preprint arXiv:2402.17762_, 2024.\n\n\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.\n\n\n[Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.](https://arxiv.org/abs/2505.09388)\n\n\nYuan Tian and Tianyi Zhang. Selective prompt anchoring for code generation. _arXiv preprint_\n_arXiv:2408.09121_, 2024.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n[Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.](https://arxiv.org/abs/1706.03762)\n[org/abs/1706.03762.](https://arxiv.org/abs/1706.03762)\n\n\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\nPlan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language\nmodels. _arXiv preprint arXiv:2305.04091_, 2023.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in_\n_neural information processing systems_, 35:24824–24837, 2022.\n\n\n12\n\n\nPreprint\n\n\nYuyang Wu, Yifei Wang, Ziyu Ye, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is\nless: Understanding chain-of-thought length in llms. _arXiv preprint arXiv:2502.07266_, 2025.\n\n\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming\nlanguage models with attention sinks. _arXiv preprint arXiv:2309.17453_, 2023.\n\n\nXiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-Guang Lou, and\nShuai Ma. Re-reading improves reasoning in large language models. In _Proceedings of the 2024_\n_Conference on Empirical Methods in Natural Language Processing_, pp. 15549–15575, 2024.\n\n\nShunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. _arXiv preprint arXiv:2105.11115_, 2021.\n\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. In _International Conference on_\n_Learning Representations (ICLR)_, 2023.\n\n\nQingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo\nZhao. Tell your model where to attend: Post-hoc attention steering for llms. _arXiv preprint_\n_arXiv:2311.02262_, 2023.\n\n\nXuechen Zhang, Xiangyu Chang, Mingchen Li, Amit Roy-Chowdhury, Jiasi Chen, and Samet Oymak. Selective attention: Enhancing transformer through principled context control. _Advances in_\n_Neural Information Processing Systems_, 37:11061–11086, 2024.\n\n\nHuaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le,\nand Denny Zhou. Take a step back: Evoking reasoning via abstraction in large language models.\n_arXiv preprint arXiv:2310.06117_, 2023.\n\n\nDenny Zhou, Nathanael Sch¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex\nreasoning in large language models. _arXiv preprint arXiv:2205.10625_, 2022.\n\n\nPei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R McKee, Ari Holtzman,\nJay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, et al. How far are large language\nmodels from agents with theory-of-mind? _arXiv preprint arXiv:2310.03051_, 2023.\n\n\nPei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed Chi, Denny Zhou,\nSwaroop Mishra, and Huaixiu Steven Zheng. Self-discover: Large language models self-compose\nreasoning structures. _Advances in Neural Information Processing Systems_, 37:126032–126058,\n2024.\n\n\n13\n\n\nPreprint\n\n\nA T HE U SE OF L ARGE L ANGUAGE M ODELS (LLM S )\n\n\nWe leverage Large Language Models (LLMs) primarily for grammar checking and polishing for our\nmanuscript.\n\n\nB C ALCULATING A VERAGE C ONFIDENCE S CORES\n\n\nWe conduct an ablation study to evaluate different methods for calculating confidence scores from\ntoken-level probabilities. Let _P_ _i_ = _{p_ 1 _, p_ 2 _, . . ., p_ _m_ _}_ represent the set of token-level confidence\nscores for tokens generated in the current reasoning step _i_ .\n\n\nWe compare three approaches for calculating average confidence scores at sequence level:\n\n\n**Harmonic Mean:**\n\n_n_\n_p_ harmonic = ~~�~~ ~~_n_~~ _i_ =1 _p_ 1 _i_ (5)\n\n\n**Geometric Mean:**\n\n\n\n� 1 _/n_\n\n\n\n_p_ geometric =\n\n\n\n_n_\n� _p_ _i_\n� _i_ =1\n\n\n\n(6)\n\n\n\n**Arithmetic Mean:**\n\n\n\n_p_ arithmetic = [1]\n\n_n_\n\n\n\n_n_\n� _p_ _i_ (7)\n\n\n_i_ =1\n\n\n\nTable 6 presents the results across two reasoning benchmarks using Llama3.1 and Phi-4-mini models. The harmonic mean consistently outperforms both geometric and arithmetic means across all\nsettings. This is because the harmonic mean is more sensitive to low confidence values, which better captures potential attention drift during reasoning. The geometric mean performs second-best,\nas it also penalizes low values more than the arithmetic mean, though less aggressively than the\nharmonic mean. The arithmetic mean shows the weakest performance, as it can be dominated by\nhigh-confidence tokens and may miss instances where attention drift occurs for specific reasoning\ncomponents.\n\n\nC S ELF -A NCHOR P ROMPT D ETAILS\n\n\n\n\n\nD A LTERNATIVE D ESIGN\n\n\nAs described in Section 2.1, our primary S ELF -A NCHOR design anchors attention to the original\nquestion and the current plan step during reasoning generation. We investigate an alternative design\nthat anchors to all prior plan steps in addition to the current step and the original question. The\nmotivation for this alternative is that maintaining attention to all previous planning steps might\nprovide additional context for the current reasoning step.\n\n\nWe compare two anchoring strategies:\n\n\n14\n\n\nPreprint\n\n\n    - **S** **ELF** **-A** **NCHOR** **(Primary):** Anchors to the original question and current plan step only,\nwhere _a_ _i_ = _{_ Question _,_ Plan _i_ _}_\n\n    - **Anchor to All:** Anchors to the original question, current plan step, and all prior plan steps,\nwhere _a_ _i_ = _{_ Question _,_ Plan 1 _,_ Plan 2 _, . . .,_ Plan _i_ _}_\n\n\nTable 7 presents the results comparing these two approaches across AQuA and T4D benchmarks\nusing Llama3.1-8B and Phi-4-mini models. The primary S ELF -A NCHOR design consistently outperforms the alternative that anchors to all prior steps. This performance difference may be because\nthe alternative design dilutes attention across too many anchor points, reducing focus on the most\nrelevant current step. In contrast, the primary design maintains sharp focus on the most relevant\ncontext while avoiding attention dilution.\n\n\nTable 6: Mean selection ablation study. Results show accuracy (%) on AQuA-RAT and T4D benchmarks.\n\n\n**Model** **Method** AQuA-RAT T4D\n\n\n\nLLaMA3.1-8B\n\n\nPhi-4-mini\n\n\n\nCoT 50.79 26.77\n\nS ELF -A NCHOR (Harmonic) **55.51** **40.01**\nS ELF -A NCHOR (Geometric) 55.11 35.28\nS ELF -A NCHOR (Arithmetic) 54.72 35.99\n\n\nCoT 61.81 39.54\n\nS ELF -A NCHOR (Harmonic) **68.50** **49.47**\nS ELF -A NCHOR (Geometric) 67.72 49.11\nS ELF -A NCHOR (Arithmetic) 67.71 48.40\n\n\n\nTable 7: Alternative anchoring design comparison. Results show accuracy (%) on AQuA-RAT and\nT4D benchmarks.\n\n\n**Model** **Method** AQuA-RAT T4D\n\n\nCoT 50.79 26.77\n\nLLaMA3.1-8B S ELF -A NCHOR **55.51** **40.01**\nAnchor to All Prior Steps 53.54 32.62\n\n\nCoT 61.81 39.54\n\nPhi-4-mini S ELF -A NCHOR **68.50** **49.47**\nAnchor to All Prior Steps 67.32 47.34\n\n\nE I MPLEMENTATION AND EVALUATION DETAILS\n\n\nE.1 P ROMPT EXAMPLE\n\n\n**Chain-of-Thought.**\n\n\n\n\n\n**Plan-and-solve+.** We adopt the implementation from Plan-and-solve+(Wang et al., 2023), for mathmatical reasoning we apply prompt:\n\n\n\n\n\nOtherwise, we use:\n\n\n\n\n\n15\n\n\nPreprint\n\n\n**Re-Reading.**\n\n\n\n\n\nE.2 E VALUATION D ETAILS\n\n\nWe adopt standard metrics used in prior work (Chuang et al., 2024; Wang et al., 2023; Zhou et al.,\n2024), including accuracy and exact match, for AQuA, BBH, T4D, and MATH. For GSM8K and\nStrategyQA, we follow the factual accuracy evaluation protocol introduced by Chuang et al. (2024).\n\n\nTo ensure consistent answer extraction, we prompt all models to conclude their response with the\nphrase: “Conclude with the final answer using the format: “Final answer”: ” _<_ your answer _>_ ” where\n_<_ your answer _>_ denotes either a multiple-choice option or a string answer. We then apply taskspecific heuristics to extract _<_ your answer _>_ from the output.\n\n\nFor the MATH dataset, we randomly sample 200 test instances and manually verify the extracted\n\nanswers to ensure correctness.\n\n\nE.3 I MPLEMENTATION D ETAILS\n\n\nWe follow the definitions of reasoning step segmentation and task complexity from (Wu et al., 2025;\nJin et al., 2024). The performance gain of a task is computed as the difference in accuracy between\ntwo methods on that task.\n\n\nTo measure the number of reasoning steps, we split the full reasoning chain by “ _\\_ n” and remove\nempty lines caused by consecutive breaks (“ _\\_ n _\\_ n”). The remaining lines are counted as the CoT\nlength.\n\n\nF C ASE S TUDY\n\n\nIn this section, we provided a qualitative example to illustrate the reasoning trajectory generated by\neach method. The task is to determine whether a prime number can be represented by the number\nof days in a week. CoT 1 and RE 3 immediately arrive at an incorrect conclusion by misidentifying\n7 as a non-prime number. PS+ 2 follows a more structured plan but also makes a critical error\nin its final conclusion, contradicting its own evidence. In contrast, S ELF -A NCHOR 4 successfully\ndecomposes the problem, correctly identifies 7 as a prime number, and reaches the correct answer\nby systematically executing its reasoning plan.\n\n\n**Algorithm 1** Chain-of-Thought (CoT)\n\n\n1: _**Question:**_ Can a prime number be represented by the number of days in a week?\n2: **Prompt:** Let’s think step by step.\n3: **Model Completion:** No, a prime number cannot be represented by the number of days in a\nweek because the number of days in a week is 7, which is not a prime number. Prime numbers\nare numbers greater than 1 that have no positive divisors other than 1 and themselves. Since 7\nis divisible by 1 and 7, it is not a prime number.\n4: **Final answer: No**\n\n\nG N UMERICAL E XPERIMENT R ESULT ON B IG -B ENCH -H ARD\n\n\n16\n\n\nPreprint\n\n\n**Algorithm 2** Plan-and-Solve (PS+)\n\n\n1: _**Question:**_ Can a prime number be represented by the number of days in a week?\n2: **Prompt:** Let’s first prepare relevant information and make a plan. Then, let’s answer the question step by step (pay attention to commonsense and logical coherence).\n3: **Model Completion:** Step 1: Gather Information: Prime numbers are natural numbers greater\nthan 1 that have no positive divisors other than 1 and themselves. The number of days in a week\nis 7.\n4: Step 2: Plan the Solution: We will first identify prime numbers less than 7 to see if any match\nthe number of days in a week. - Then, we will determine if any prime number can represent the\nnumber of days in a week based on our findings.\n5: Step 3: Execute the Plan: Prime numbers less than 7 are 2, 3, 5, and 7. - Among these, 2, 3, and\n5 are less than 7 and are indeed prime numbers. - None of these prime numbers (2, 3, 5) match\nthe number of days in a week (7).\n6: Step 4: Analyze and Draw a Conclusion: Since none of the prime numbers less than 7 (2, 3, 5)\nmatch the number of days in a week (7), a prime number cannot be represented by the number\nof days in a week.\n7: **Step 5: Final Answer: The final answer is No**\n\n\n**Algorithm 3** Re-Reading (RE)\n\n\n1: _**Question:**_ Can a prime number be represented by the number of days in a week?\n2: **Prompt:** Read the question again\n3: **Model Completion:** No, a prime number cannot be represented by the number of days in a\nweek because the number of days in a week is 7, which is not a prime number.\n4: Prime numbers are numbers greater than 1 that have no positive divisors other than 1 and themselves.\n5: The number 7 is a prime number, but the question seems to be asking if the number of days in a\nweek (7) can be represented by a prime number, which it already is.\n6: However, if the question is interpreted as asking if there is a prime number that equals the\nnumber of days in a week, the answer would still be ”No” because 7 is already a prime number.\n7: If the question is asking if the number of days in a week (7) can be represented by any prime\nnumber, the answer would be ”Yes” because 7 itself is a prime number.\n8: However, the phrasing of the question suggests that it is asking if the number of days in a week\ncan be represented by a prime number other than 7, in which case the answer would be ”No.”\n9: For clarity, I will assume the question is asking if the number of days in a week (7) can be\nrepresented by a prime number other than itself, and the final answer is ”No.”\n10: **”Final answer”: ”No”**\n\n\n**Algorithm 4** S ELF -A NCHOR\n\n\n1: _**Question:**_ Can a prime number be represented by the number of days in a week?\n2: **Prompt:** Decompose the given problem into step-by-step plan, reasoning the plan and solve the\nproblem step by step in JSON format. For each plan step, provide a key-value pair: the key is\nthe plan step, the value is the detailed reasoning for that step.\n3: **Model Completion:** ”Understand the question.”: ”The question asks if a prime number can be\nrepresented by the number of days in a week.”,\n4: ”Identify the number of days in a week.”: ”There are 7 days in a week.”,\n5: ”Determine if 7 is a prime number.”: ”A prime number is a natural number greater than 1 that\nhas no positive divisors other than 1 and itself.”,\n6: ”Check if 7 meets the criteria of a prime number.”: ”7 is greater than 1 and its only divisors are\n1 and 7.”,\n7: **”Final answer”: ”Yes”**\n\n\n17\n\n\nPreprint\n\n\nTable 8: BBH details\n\n\n**BBH**\n**Model** **Method**\n\n\n**disambiguation** **qa** **causal** **judgement** **date** ~~**u**~~ **nderstanding** **logical** ~~**d**~~ **eduction** **salient** ~~**t**~~ **ranslation** ~~**e**~~ **rror** **detection** **snarks**\n\n\nCoT 100 91/187 182 67 110 125\n\nLlama3.1-8B PS+ 138 108 136 78 124 122\n\nre-read 124 98 190 85 121 127\n\nS ELF -A NCHOR 152 100 160 128 132 127\n\n\nCoT 88/250 71 102 34 95 89\n\nLlama3.2-3B PS+ 104 63 97 86 98 105\n\nre-read 74 84 127 40 89 105\n\nS ELF -A NCHOR 131 101 131 101 96 129\n\n\nCoT 162/250 113 154 114 149 134\n\nPhi4-mini-4B PS+ 160 126 138 130 136 124\n\nre-read 162 115 165 113 135 148\n\nS ELF -A NCHOR 152 116 174 128 140 142\n\n\nCoT 183/250 121 174 208 167 148\n\nQwen3-mini-4B PS+ 179 108 165 207 160 150\nre-read 179 121 203 230 147 154\n\nS ELF -A NCHOR 185 115 193 224 161 150\n\n\nCoT 180 120 203 169 152 160\n\nPhi-4-15B PS+ 180 117 140 186 155 156\n\nre-read 179 120 221 191 158 154\n\nS ELF -A NCHOR 176 122 218 203 148 161\n\n\nCoT 105 125 182 185 172 155\n\nQwen3-30B PS+ 114 122 172 181 165 156\nre-read 113 125 185 210 174 156\n\nS ELF -A NCHOR 110 123 196 200 160 157\n\n\n18\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03222v1",
      "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward",
      "authors": [
        "Guanhua Huang",
        "Tingqiang Xu",
        "Mingze Wang",
        "Qi Yi",
        "Xue Gong",
        "Siheng Li",
        "Ruibin Xiong",
        "Kejiao Li",
        "Yuhao Jiang",
        "Bo Zhou"
      ],
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textbf{\\textit{reasoning\nsparks}}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\n\\textit{reasoning sparks} is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n$60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
      "published": "2025-10-03T17:56:13Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03222v1"
    },
    "text": "2025-10-06\n\n### **Low-probability Tokens Sustain Exploration in Reinforcement Learning** **with Verifiable Reward**\n\n\nGuanhua Huang [1,] _[∗]_, Tingqiang Xu [1,2,] _[∗]_ [†], Mingze Wang [1,3,†], Qi Yi [1], Xue Gong [1], Siheng Li [1,4,†],\nRuibin Xiong [1] Kejiao Li [1], Yuhao Jiang [1], Bo Zhou [1][‡]\n\n\n1 **LLM Department, Tencent** 2 **Tsinghua University**\n3 **Peking University** 4 **The Chinese University of Hong Kong**\n\n\n**Abstract**\n\n\nReinforcement Learning with Verifiable Rewards (RLVR) has propelled Large Language Models\nin complex reasoning, yet its scalability is often hindered by a training bottleneck where performance plateaus as policy entropy collapses, signaling a loss of exploration. Previous methods\ntypically address this by maintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that an unselective\nfocus on entropy risks amplifying irrelevant tokens and destabilizing training. This paper investigates the exploration dynamics within RLVR and identifies a key issue: the gradual elimination\nof valuable low-probability exploratory tokens, which we term _**reasoning sparks**_ . We find that\nwhile abundant in pre-trained models, these sparks are systematically extinguished during RLVR\ndue to over-penalization, leading to a degeneracy in exploration. To address this, we introduce\nLow-probability Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out presumed noise tokens and\nre-normalizing the distribution over the remaining candidates. The result is a less-noisy proxy\nwhere the probability of _reasoning sparks_ is amplified, which then serves as a soft regularization\ntarget to shield these valuable tokens from elimination via KL divergence. Experiments show\nthat Lp-Reg enables stable on-policy training for around 1,000 steps, a regime where baseline\nentropy-control methods collapse. This sustained exploration leads to state-of-the-art performance,\nachieving a 60.17% average accuracy on five math benchmarks, an improvement of 2.66% over\n[prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.](https://github.com/CarlanLark/Lp-Reg)\n\n\n**1** **Introduction**\n\n\nThe advent of large reasoning models has reshaped the trajectory of artificial intelligence, with paradigmatic\nexamples including OpenAI O1 (OpenAI et al., 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025). A central\ntechnique underpinning these systems is reinforcement learning with verifiable reward (RLVR), which assigns\nreward to verifiable solutions through rule-based verification. These models generate extended chain-of-thought\n(CoT) reasoning (Wei et al., 2023) to solve challenging problems in domains like mathematical olympiads (He et al.,\n2024b). However, a notable bottleneck emerges during RL training that limits its scalability, frequently culminating\nin a performance plateau and subsequent collapse. This failure is consistently accompanied by a rapid decay in\npolicy entropy, indicating a severe loss of exploration capacity (Yu et al., 2025; Cui et al., 2025; Wang et al., 2025).\n\n\nPrevious approaches have recognized this declining exploration, attempting to address it through various entropy\ncontrol mechanisms. Methods such as adaptive entropy regularization (He et al., 2025), high entropy change\nblocking (Cui et al., 2025), or selective token updates (Wang et al., 2025) aim to maintain higher entropy as a proxy\nfor exploration. However, relying on overall entropy can be an indirect and imprecise tool. An indiscriminate focus\non maximizing randomness risks amplifying noise and destabilizing training (Omer Veysel [¨] C¸ agatan & Akg ˘ un ¨,\n2025), suggesting a deeper issue beyond simply the quantity of randomness.\n\n\nOur analysis suggests the performance bottleneck may stem from the systematic elimination of valuable lowprobability exploratory tokens. We term these tokens **Reasoning Sparks** ; they include words like “wait”, “however”,\nor “perhaps”, which often serve as logical connectives or expressions of uncertainty that naturally initiate diverse\nreasoning pathways (Figure 1a). As the aggregated violin plots in Figure 1c show, standard GRPO training suppresses\nthe low-probability sampling of these important exploratory tokens, causing the suppression of _Reasoning Sparks_ .\nFurthermore, we find that indiscriminately boosting randomness amplifies the low-probability sampling of irrelevant\ntokens (e.g., “cost”, “fine”), which are semantically out of context for the mathematical reasoning task. We refer\nto the low-probability appearance of these irrelevant tokens as noise. This amplification leads to an even faster\nperformance collapse than the baseline, as shown in Figure 1b.\n\n\n_∗_ Equal contribution.\n\n  - Work completed during an internship at Tencent.\n\n  - Correspondence to Bo Zhou: chaysezhou@tencent.com.\n\n\n1\n\n\n```\nThe answer is 2.\n\n```\n\n\n99%\n\n\n1%\n\n\n```\nAnswer: \\boxed{2}\n\nWait, …, Answer: \\boxed{3}\n\n```\n\n\n(a) _Reasoning Sparks_ : low-probability exploratory tokens that initiates a new reasoning path.\n\n\n\n\n\n|0.60 cy ( GRPO + Entrop Lp-Reg|py Loss|\n|---|---|\n|0<br>200<br>40<br>Trai<br>0.45<br>0.50<br>0.55<br><br><br><br>0<br>200<br>40<br>Trai<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>Train Entropy<br>GR<br>~~GR~~<br>Lp~~-~~||\n|0<br>200<br>40<br>Trai<br>0.45<br>0.50<br>0.55<br><br><br><br>0<br>200<br>40<br>Trai<br>0.0<br>0.1<br>0.2<br>0.3<br>0.4<br>0.5<br>0.6<br>Train Entropy<br>GR<br>~~GR~~<br>Lp~~-~~||\n\n\n(b) Training Dynamics\n\n\n\n(d) Aggregated distribution of observed sampling probabilities for a class of\nirrelevant tokens (e.g., “cost”, “fine”).\n\n\n\n\n\n\n\n\n\n(c) Aggregated distribution of observed sampling probabilities for a class of\nmeaningful exploratory tokens (e.g., “wait”, “however”).\n\n\n\n\n\n\n\n\n\nFigure 1: Selectively preserving low-probability tokens is key to overcoming performance plateaus in reasoning\nRL. **(a)** An illustration of a _reasoning spark_ . **(b)** Standard GRPO training reaches a performance plateau and\ncollapses, accompanied by decaying entropy. An indiscriminate entropy bonus (GRPO + Entropy Loss) leads to an\neven faster collapse. **(c)** We reveal the cause: GRPO systematically suppresses the low-probability sampling of\nimportant exploratory tokens (like “wait”), and forces these tokens’ sampling distributions to collapse towards high\nprobabilities. Ent-Loss fails to fix this. In contrast, our method, Lp-Reg, successfully preserves a healthy, wide\ndistribution, sustaining exploration. **(d)** The failure of entropy bonuses is explained by their indiscriminate nature:\nthey amplify the low-probability sampling of irrelevant tokens, creating noise and thereby degrading exploration\nquality. The aggregated statistics in (c) and (d) demonstrate a systemic effect beyond single-token instances. Detailed\nplots for individual tokens are available in Appendix B.1.\n\n\nThese findings present a central challenge: a successful exploration strategy should protect valuable _reasoning_\n_sparks_ without simultaneously amplifying the destructive effects of irrelevant noise. To address this challenge,\nwe introduce Low-probability Regularization (Lp-Reg). The primary goal of Lp-Reg is to preserve valuable\nlow-probability tokens via regularization. To avoid amplifying noise, the method leverages a key observation:\nwithin the low-probability range, meaningful exploratory tokens (like “wait”) consistently exhibit a higher average\nprobability than irrelevant noise (like “cost”) in the immediate next-token prediction. Based on this statistical\ndistinction, Lp-Reg first discards low-probability tokens presumed to be noise using a probability threshold. It\nthen redistributes the probability mass from these discarded tokens among the remaining candidates. This process\nconstructs a less-noisy proxy distribution where valuable low-probability tokens are preserved and their relative\nprobabilities amplified. Finally, Lp-Reg penalizes the deviation of the original policy from this proxy using a\nforward KL divergence, which selectively protects the low-probability tokens that were preserved in the less-noisy\nproxy distribution.\n\n\nOur experimental evaluation demonstrates the effectiveness of Lp-Reg. Our method enables stable on-policy\ntraining for around 1,000 steps, a regime where many entropy-control methods have collapsed, resulting in better\nperformance. On five widely used math benchmarks, this results in a 60.17% average accuracy on Qwen3-14B-Base,\nimproving upon prior methods by 2.66%. Our contributions are summarized as follows:\n\n\n    - In contrast to prior work focusing on overall policy entropy, we identify the suppression of _reasoning_\n_sparks_ as a key issue and provide evidence that their preservation is crucial for sustained performance.\n\n\n    - We introduce Low-probability Regularization (Lp-Reg), a method that creates a more stable exploratory\nenvironment by filtering out presumed meaningless noise to protect the remaining low-probability tokens.\n\n\n    - We demonstrate through extensive experiments that Lp-Reg achieves state-of-the-art performance, while\nalso enabling stable on-policy training over extended periods where baselines collapse.\n\n\n    - We provide a comprehensive analysis showing that our approach of filtering presumed meaningless noise\nyields superior results compared to indiscriminate entropy-control methods.\n\n\n2\n\n\n**2** **Related Work**\n\n\n**Reinforcement learning for LLMs** Recently, reinforcement learning has become the dominant framework for\nenhancing the reasoning abilities of large language models (LLMs) (OpenAI et al., 2024; DeepSeek-AI et al., 2025).\nBy leveraging automatic checkers or symbolic verification, reinforcement learning with verifiable rewards (RLVR)\nachieved further breakthroughs in improving the reasoning capability of LLMs (Shao et al., 2024a; Yang et al., 2025;\nTeam et al., 2025). Based on RLVR and GRPO (Shao et al., 2024a), subsequent methods such as DAPO Yu et al.\n(2025), VAPO (Yue et al., 2025), and other policy optimization variants (Zhao et al., 2025; Cui et al., 2025; Zheng\net al., 2025) have been proposed to improve the stability, efficiency, and scalability of RL for reasoning models.\n\n\n**Entropy collapse in RL training** A recurring difficulty in training reasoning models with RL is the rapid collapse\nof policy entropy during the early stages of training. This phenomenon, which reflects excessive exploitation and\ninsufficient exploration, has been widely recognized as a bottleneck for scaling RL in reasoning models. To mitigate\ncollapse, researchers have explored several directions, including selectively regularizing updates at high-entropy\n“forking” tokens (Wang et al., 2025), amplifying advantages at exploratory positions (Cheng et al., 2025), modifying\nclipping strategies (Yu et al., 2025; Zhao et al., 2025; Cui et al., 2025; Zheng et al., 2025), or doing weight\nclipping (MiniMax et al., 2025; Su et al., 2025). While these methods primarily operate by monitoring policy\nentropy, which is correlational rather than causal to exploration, our analysis delves directly into the next-token\nprediction distribution. This allows for a more semantically grounded and causally-informed investigation of the\nprobabilities of individual candidates and their role in exploration dynamics.\n\n\n**Intrinsic confidence of LLMs** As the capabilities of Large Language Models (LLMs) have rapidly advanced,\nthey have demonstrated an increasingly strong and reliable sense of intrinsic confidence (Saurav et al., 2022; Loka\net al., 2024; Amir et al., 2025). Research investigates how these intrinsic confidence signals, often reflected in the\nnext-token prediction distribution, can guide complex reasoning and exploration (Amirhosein et al., 2025; Xuezhi\n& Denny, 2024; Xuandong et al., 2025). Studies have shown that tokens with higher relative probabilities in the\nnext-token prediction are often more contextually appropriate than their lower-probability counterparts (Nguyen\net al., 2025; Xu et al., 2025; Fu et al., 2025). Building on this, some work has explored entropy minimization, which\nsharpens the model’s confidence distribution. This approach can improve inference performance by encouraging the\nmodel to commit to consistent and high-confidence solution paths (Gao et al., 2025; Agarwal et al., 2025). Our work\nbuilds upon a similar insight, leveraging the model’s intrinsic confidence to distinguish between valuable _reasoning_\n_sparks_ and irrelevant noise within this low-probability range.\n\n\n**3** **Preliminaries**\n\n\n**3.1** **Reinforcement Learning with Verifiable Rewards**\n\n\nReinforcement learning (RL) has played a critical role in LLMs (Murphy, 2024). Formally,\n\n\n_J_ RL ( _**θ**_ ) = **E** ( _q_, _a_ ) _∼_ _D_, _o_ _∼_ _π_ _θ_ ( _·|_ _q_ ) � _r_ ( _o_, _a_ ) �, (1)\n\n\nwhere _r_ ( _o_, _a_ ) denotes the reward assigned to an output _o_ given a reference answer _a_ . In reinforcement learning with\nverifiable rewards (RLVR), this reward is computed through rule-based functions, such as Math-Verify [1] . Recent\nstudies have demonstrated that large-scale RLVR encourages models to perform more deliberative reasoning by\nproducing extended chains of thought prior to the final prediction, thereby substantially improving their capacity\nto solve complex problems (DeepSeek-AI et al., 2025). In practice, Eq. 1 is typically optimized using policy\ngradient methods, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Group Relative Policy\nOptimization (GRPO) (Shao et al., 2024b).\n\n\n**3.2** **Group-Relative Policy Optimization**\n\n\nGRPO is a representative actor-only policy gradient method for optimizing LLMs. It directly estimates the advantage\nof each token by leveraging multiple samples drawn from the same prompt. Formally, the advantage is defined as\n\n_A_ _i_, _t_ = _[R]_ [(] _[o]_ _[i]_ [)] _[ −]_ std [mean] ( _G_ ) [(] _[G]_ [)], (2)\n\n\nwhere _{_ _o_ 1, . . ., _o_ _G_ _}_ are independent outputs sampled from the same prompt, with group size _G_, _G_ = _{_ _R_ ( _o_ 1 ), . . ., _R_ ( _o_ _G_ ) _}_\ndenotes their associated rewards, and _R_ ( _o_ _i_ ) is the reward of output _o_ _i_ . In this formulation, _A_ _i_, _t_ represents the\nadvantage of the _t_ -th token in _o_ _i_ . The policy is then optimized on the basis of these advantages using the PPO\n\n\n1 [https://github.com/huggingface/Math-Verify](https://github.com/huggingface/Math-Verify)\n\n\n3\n\n\nsurrogate objective:\n\n\n_J_ GRPO ( _**θ**_ ) = **E** ( _q_, _a_ ) _∼_ _D_, _{_ _o_ _i_ _}_ _iG_ = 1 _[∼]_ _[π]_ _**[θ]**_ old [(] _[·|]_ _[q]_ [)]\n\n\n\n_|_ _o_ _i_ _|_\n### ∑\n\n_t_ = 1\n\n\n\n(3)\n�min� _r_ _i_, _t_ _A_ _i_, _t_, clip� _r_ _i_, _t_, 1 _−_ _ϵ_, 1 + _ϵ_ � _A_ _i_, _t_ � _−_ _β D_ KL ( _π_ _**θ**_ _∥_ _π_ ref ) �,\n\n\n\n1\n\n_G_\n\n\n\n_G_\n### ∑\n\n_i_ = 1\n\n\n\n1\n\n_|_ _o_ _i_ _|_\n\n\n\nwhere _β_ controls the strength of KL regularization between the current policy _π_ _θ_ and the reference policy _π_ ref . The\nprobability ratio\n\n\n\n_π_ _θ_ � _o_ _i_, _t_ _|_ _q_, _o_ _i_, _<_ _t_ �\n_r_ _i_, _t_ = _π_ _θ_ old ~~�~~ _o_ _i_, _t_ _|_ _q_, _o_ _i_, _<_ _t_ ~~�~~ (4)\n\n\n\nserves as the importance sampling weight for off-policy training, where _π_ _θ_ old denotes the behavior policy. The\nhyperparameter _ϵ_ specifies the clipping ratio, which constrains the updated policy from deviating excessively from\nthe behavior policy, thereby ensuring stability during optimization.\n\n\n**4** **Low-probability Regularization**\n\n\nTo address the premature elimination of valuable _reasoning sparks_, we propose a regularization method termed **Low-**\n**probability Regularization (Lp-Reg)** . This method is designed to be integrated into policy gradient algorithms to\ncreate a more stable exploratory environment. The central idea is to leverage the model’s own predictive distribution\nto construct a less-noisy proxy for regularization, preserving low-probability tokens.\n\n\n**4.1** **Proxy Distribution** _π_ **proxy**\n\n\nThe foundation of Lp-Reg is the construction of a proxy distribution, which represents a filtered variant of the\ncurrent policy _π_ _**θ**_ . It is constructed in two steps:\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||= 0.|02|||||||||\n\n\n|(b) Renormalized Policy pro|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|oxy|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|pr|pr|pr|pr|pr|pr|pr|pr|pr|pr|xy|xy|xy|\n||||||||||||||\n|||||||Reno|rmalize|sum|= 1||||\n||||||||||||||\n||||||||||||||\n||||||||||||||\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|0<br>2<br>4<br>6<br>8<br>10<br>12<br>14<br>16<br>18<br>Token Index|\n\n\n\nFigure 2: An example of probability renormalization. _π_ proxy assigns zero probability to tokens with _π_ _**θ**_ _≤_ _τ_ and\nrenormalizes the probability mass to tokens with _π_ _**θ**_ _>_ _τ_ .\n\n\n1. **Filtering Noise Tokens:** We first filter out a set of low-confidence tokens, which are presumed to be noise,\ndefined as those whose probability _π_ _**θ**_ ( _o_ _|·_ ) is under a threshold _τ_ . This threshold controls the filtering strategy,\nfor which we explore two primary choices:\n\n\n     - **Fixed threshold:** A simple approach where _τ_ is a constant hyperparameter, e.g., 0.02.\n\n     - **Min-p threshold:** Following (Nguyen et al., 2025), _τ_ is defined relative to the peak probability: _τ_ =\n_κ_ _·_ max _o_ _′_ _∈_ _V_ _π_ _**θ**_ ( _o_ _[′]_ _|·_ ), where _κ_ _∈_ ( 0, 1 ) is a hyperparameter. This makes the filtering adaptive to the\ndistribution’s sharpness.\n\nOur primary experiments employ the min-p strategy for its adaptiveness, though fixed thresholds are also\nshown to be effective in our ablation studies.\n\n\n2. **Probability Renormalization:** As shown in Figure 2, the proxy distribution _π_ proxy assigns zero probability to\ntokens filtered out in the previous step and renormalizes the probability mass across the remaining tokens:\n\n\n\n_π_ proxy ( _o_ _|·_ ) =\n\n\n\n_π_ _**θ**_ ( _o_ _|·_ )\n∑ _o_ _′_ s.t. _π_ _**θ**_ ( _o_ _′|·_ ) _>_ _τ_ _π_ _**θ**_ ( _o_ ~~_[′]_~~ _|·_ ) if _π_ _**θ**_ ( _o_ _|·_ ) _>_ _τ_ . (5)\n�0 otherwise\n\n\n\nThis process effectively treats tokens with low relative probabilities as potential noise, while preserving all\nothers to form a high-confidence reference.\n\n\n4\n\n\n**4.2** **Low-probability Regularization Objective**\n\n\nThe Low-probability Regularization (Lp-Reg) penalty is integrated into the GRPO framework as a selective\nregularization term. The final objective function is:\n\n\n\n_|_ _o_ _i_ _|_\n### ∑\n\n_t_ = 1\n\n\n\n_J_ ( _**θ**_ ) = **E** _B∼D_, ( _q_, _a_ ) _∼B_, _{_ _o_ _i_ _}_ _iG_ = 1 _[∼]_ _[π]_ _**[θ]**_ old [(] _[·|]_ _[q]_ [)]\n\n\n\n1\n� ∑ _i_ _[G]_ = 1 _[|]_ _[o]_ _[i]_ _[|]_\n\n\n\n_G_\n### ∑\n\n_i_ = 1\n\n\n\nclip ( _r_ _i_, _t_ ( _**θ**_ ), 0, _U_ ) _·_ _A_ _i_, _t_\n�\n\n\n\n_−_ _β_ _·_ _I_ � _π_ _**θ**_ ( _o_ _i_, _t_ _|_ _q_, _o_ _i_, _<_ _t_ ) _<_ _δ_ _ρ_ _[B]_ _[∧]_ _[π]_ [proxy] [(] _[o]_ _i_, _t_ _[|]_ _[q]_ [,] _[ o]_ _i_, _<_ _t_ [)] _[ >]_ [ 0] _[ ∧]_ _[A]_ _i_, _t_ _[<]_ [ 0] � (6)\n\n\n_· D_ KL ( _π_ proxy ( _·|_ _q_, _o_ _i_, _<_ _t_ ) _∥_ _π_ _**θ**_ ( _·|_ _q_, _o_ _i_, _<_ _t_ ))\n� [�]\n\n\n\nThe first term is the policy gradient objective from GRPO. We modify its clipping by removing the lower bound to\navoid clipping low-probability exploratory actions and adding a large upper bound _U_ for numerical stability.\n\n\nThe second term is the Lp-Reg penalty, designed to protect _reasoning sparks_ . It is activated by the indicator function\n_I_ [ _·_ ] only for tokens that satisfy three conditions simultaneously: first, their sampling probability _π_ _**θ**_ is below a\ndynamic low-percentile threshold _δ_ _ρ_ _[B]_ [, which is calculated as the lowest] _[ ρ]_ [-th percentile of the sampling probabilities]\nof all tokens within the current training batch _B_ ; second, their probability in the proxy distribution _π_ proxy is greater\nthan zero; and third, the token receives a negative advantage signal ( _A_ _i_, _t_ _<_ 0 ). This final condition ensures\nthe regularization applies exclusively to tokens receiving a negative learning signal, preventing their potential\nover-penalization while leaving updates from positive experiences unaffected.\n\n\nWe use the forward KL divergence, _D_ KL ( _π_ proxy _∥_ _π_ _**θ**_ ) as the regularization function. It imposes a significant penalty\nwhen _π_ _**θ**_ ( _o_ _|·_ ) approaches zero for a token _o_ with non-zero probability in _π_ proxy, providing a targeted penalty against\ntoken elimination without forcing the policy to strictly match the heuristic proxy distribution.\n\n\n**5** **Experiments**\n\n\n**5.1** **Experimental Setup**\n\n\n**Baselines** We compare Lp-Reg against a suite of strong baselines, including a foundational algorithm and\nseveral state-of-the-art methods designed to enhance exploration through entropy control. Our primary baseline\nis **GRPO** (Shao et al., 2024a), a value-free policy optimization algorithm that employs group-relative advantage\nestimation, making it a common choice for RLVR. To represent classical entropy regularization methods, we\nimplement **GRPO + Entropy Loss**, which directly incorporates the principles of Maximum Entropy RL by adding\na policy entropy bonus to the GRPO objective function. We also compare against several advanced methods:\n**Clip-Higher** (Yu et al., 2025), a core component of DAPO that encourages higher entropy by using an asymmetric\nclipping range in the PPO objective; **Selective High-Entropy Training (80/20)** (Wang et al., 2025), a method that\nrestricts policy gradient updates to only the top 20% of tokens with the highest generation entropy; **KL-Cov** (Cui\net al., 2025), which prevents entropy collapse by applying a selective KL-divergence penalty to tokens with the\nhighest covariance between their log probabilities and advantages; and **GSPO** (Zheng et al., 2025), which modifies\nthe clipping mechanism to operate at the sequence level to promote higher training entropy.\n\n\n**Training Settings** All experiments are conducted within the verl (Sheng et al., 2024) framework to ensure a\nstandardized and fair comparison. We use two base models for our experiments: Qwen3-14B-Base and Qwen2.532B. The 14B models are trained on 32 NVIDIA H20 GPUs, while the 32B model is trained on 64 NVIDIA H20\nGPUs, with an average training step time of 600 _−_ 1400 seconds. For the reinforcement learning from verifier\nrewards (RLVR) phase, models are trained on the Dapo-Math-17K Yu et al. (2025) dataset with a maximum response\nlength of 8, 192 tokens. We use a global batch size of 256. For off-policy methods, we use a mini-batch size of\n32, resulting in 8 gradient updates per rollout. To ensure a fair comparison, a “step” in our experimental results\nconsistently refers to a single rollout for all methods. Consequently, each reported step for off-policy training\ncorresponds to 8 gradient updates. A constant learning rate of 1 _×_ 10 _[−]_ [6] is applied without a warmup schedule. We\nset the group number as 8 for all GRPO-based methods. To ensure numerical stability, we set the policy gradient’s\nclipping by setting the upper bound of the importance sampling ratio to _U_ = 10 . For our proposed Lp-Reg, which\nuses the min-p threshold, we set the probability percentile threshold _ρ_ to 0.5% for Qwen2.5-32B-Base and 1% for\nQwen3-14B-Base, the KL regularization coefficient _β_ to 1.0, and the min-p ratio _κ_ to 0.02 . The proxy distribution,\n_π_ proxy, is constructed from the data-generating policy ( _π_ _θ_ old in the off-policy setting and the current policy _π_ _**θ**_ in\nthe on-policy setting). For all baseline methods, we adopt the hyperparameters specified in their original public\nimplementations to ensure a faithful reproduction. Specifically for the GRPO + Entropy Loss baseline, we set the\nentropy coefficient to 0.002 within the verl framework.\n\n\n5\n\n\n**Evaluation** For evaluation, we assess model performance across five diverse mathematical reasoning benchmarks:\nAIME24 (MAA), AIME25 (MAA), MATH-500 (Hendrycks et al., 2021), OlympiadBench (He et al., 2024a), and\nMinerva Math (Lewkowycz et al., 2022). Following (Cui et al., 2025), we employ distinct decoding strategies\nbased on the benchmark. For AIME24 and AIME25, which have smaller test sets, we use sampled decoding with a\ntemperature of 0.6 and generate 16 independent responses per problem to obtain a robust performance estimate. For\nthe remaining benchmarks, including MATH-500, OlympiadBench, and Minerva, we utilize greedy decoding to\nevaluate performance.\n\n\n**5.2** **Results**\n\n\nAs shown in Figure 3 and Table 1, Lp-Reg achieves state-of-the-art performance across five challenging mathematical\nreasoning benchmarks on both 14B and 32B model scales. On the Qwen3-14B model, on-policy Lp-Reg sets a\nnew benchmark with an average accuracy of 60.17%, surpassing the next best method, 80/20, by 2.66% . Notably,\nLp-Reg’s advantage is more pronounced on the newer Qwen3-14B base model compared to the older Qwen2.5-32B.\nWe hypothesize that as base models improve, their capacity for nuanced, low-probability reasoning increases,\ncreating a richer substrate for the emergence of valuable _reasoning sparks_, which Lp-Reg can then effectively\nprotect and leverage.\n\n\nOur experiments consistently show the superiority of on-policy training over off-policy methods across 14B and\n32B scales. This is due to the inherent stability of on-policy updates, which avoid distribution shifts caused by\nmismatched data-sampling and training policies. Off-policy methods, such as Clip-Higher, often rely on importance\nsampling clipping, leading to instability. While competitive on Qwen2.5-32B, Clip-Higher’s performance drops on\nQwen3-14B, highlighting its fragility. In contrast, Lp-Reg’s self-contained, policy-intrinsic regularization ensures\nits effectiveness in both on-policy and off-policy settings, unlike competing methods that are heavily reliant on\noff-policy importance sampling.\n\n\nBeyond raw performance, Lp-Reg demonstrates a distinct entropy signature indicative of a healthy explorationexploitation balance. As shown in Figure 3, methods like Clip-Higher induce a continuous, often artificial increase in\npolicy entropy. Lp-Reg, however, facilitates a dynamic, multi-phase entropy trajectory: entropy initially decreases as\nthe model learns core reasoning patterns, then gradually increases to foster exploration as performance improves, and\nfinally stabilizes within a healthy range as accuracy converges. This adaptive behavior stems from our confidenceaware regularization, which selectively protects _reasoning sparks_ without amplifying low-probability out-of-context\nirrelevant noise.\n\n\nMethod AIME24 AIME25 Math-500 Minerva Olympiad Bench Avg.\n\n\n**Qwen2.5-32B-Base**\n\n\n**Qwen3-14B-Base**\n\n\nTable 1: Main results on five mathematical reasoning benchmarks across two model scales. Our method, Lp-Reg,\nachieves the best average performance on both models. On-policy (on.) and off-policy (off.) training methods are\nhighlighted with distinct colors.\n\n\n6\n\n\n|(off.) GRPO(|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n\n\n|Col1|Col2|\n|---|---|\n|5<br>0||\n|5<br>0||\n\n\n\n\n\n\n\n\n\nFigure 3: Training dynamics on the Qwen3-14B-Base model. To best illustrate the performance differences, we\ncompare the top-performing methods. Lp-Reg demonstrates superior and more stable performance throughout\ntraining. Full training dynamics for the Qwen2.5-32B model are available in Appendix A.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Ablation studies for core components of Lp-Reg on the Qwen3-14B-Base model. The results confirm that\ntargeting our noise filtering threshold _τ_ is critical for stable performance. The adaptiveness of the min-p threshold is\nalso shown to be beneficial over a fixed one.\n\n\n**5.3** **Ablation Study**\n\n\nWe conduct a series of ablation studies to analyze the core components of Lp-Reg and validate our key design\nchoices.\n\n\n**Importance of Noise Filtering.** Lp-Reg only protects tokens deemed meaningful by the proxy distribution\n( _π_ proxy _>_ 0 ). To test this, we remove the filter and fork all tokens below the noise threshold _τ_ from contributing\nto gradient updating (Lp-Reg w/o _τ_ ). Figure 4 shows that this leads to a catastrophic performance collapse and\nentropy explosion. This confirms that filtering is critical to ignore the extreme tail of the distribution, which consists\nof irrelevant noise that destabilizes training if regularized.\n\n\n**Dynamic vs. Fixed Threshold.** We conduct a comparison between the dynamic min-p noise threshold (Lp-Reg\nw/ dynamic _τ_ ) and the fixed noise threshold (Lp-Reg w/ fixed _τ_ ) in Section 4.1. As shown in Figure 4, the fixed\nthreshold underperforms compared to the dynamic threshold, which we adopt as the default. However, it still\nsignificantly surpasses the standard GRPO. This indicates that while the core filtering principle is effective, the\ndynamic nature of min-p provides a more robust estimate of the model’s confidence across different contexts, better\npreserving genuine _reasoning sparks_ .\n\n\nWe conduct further ablation studies on the high-entropy token regularization and reverse KL regularization. For\ndetailed results and analysis, please refer to Appendix A.2.\n\n\n**6** **Analysis**\n\n\nTo understand the mechanisms behind Lp-Reg’s performance, we conduct a series of analyses focusing on how it\novercomes the exploration bottleneck by targeting and preserving valuable reasoning tokens.\n\n\n**6.1** **Probability-Entropy Distribution of Exploratory Tokens**\n\n\nWe begin by exploring the distinction between low-probability tokens and high-entropy tokens. Figure 5 highlights\nthis contrast by comparing tokens from the top 1% lowest probability with those from the top 1% highest entropy.\nThe difference is striking: low-probability tokens frequently include semantically meaningful exploratory markers\nsuch as _“But,” “Wait,” “Perhaps,”_ and _“Alternatively,”_ which often signal a shift in the reasoning trajectory. In\n\n\n7\n\n\nFigure 5: The word cloud statistics.\n\n\ncontrast, high-entropy tokens are dominated by common functional terms (e.g., _“sqrt,” “times”_ ) or formatting\nsymbols (e.g., _\\_ n ), which carry little exploratory intent. This explains why entropy-based regularization often fails\nto enhance exploration: it confuses noise with exploration.\n\n\nHowever, the set of low-probability tokens is also not uniformly useful. It also includes noisy artifacts such as\nspurious newline characters ( _\\_ n ) or formatting debris, whose regularization can destabilize training rather than\nenhance reasoning. To mitigate this, Lp-Reg applies a threshold _τ_ that filters out such noise. Ablation studies in\nSection 5.3 confirm the necessity of this step: removing the threshold results in unstable training dynamics and\ndegraded reasoning performance. Thus, Lp-Reg’s effectiveness stems not only from targeting low-probability tokens\nbut also from selectively excluding irrelevant noise.\n\n\n**6.2** **Sampling Dynamics of Exploratory Tokens**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|3.0<br>2.5|Col2|Col3|Mea<br>Mean E|n Prob: 0.<br>ntropy: 0.|9099 ± 0.2<br>1576 ± 0.2|036<br>717|\n|---|---|---|---|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>0.0<br>0.5<br>1.0<br>1.5<br>2.0<br><br>|||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>0.0<br>0.5<br>1.0<br>1.5<br>2.0<br><br>|||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>0.0<br>0.5<br>1.0<br>1.5<br>2.0<br><br>|||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>0.0<br>0.5<br>1.0<br>1.5<br>2.0<br><br>|||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>0.0<br>0.5<br>1.0<br>1.5<br>2.0<br><br>|||||||\n\n\n|0|Col2|Mea<br>Mean E|n Prob: 0.<br>ntropy: 1.|4565 ± 0.3<br>5960 ± 1.4|172<br>706|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>|\n\n\n|4<br>3|Col2|Mea<br>Mean E|n Prob: 0.<br>ntropy: 0.|7169 ± 0.3<br>6183 ± 0.5|100<br>867|\n|---|---|---|---|---|---|\n|3<br>4<br>||||||\n|1<br>2<br>||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Sampled Probability<br>0||||||\n\n\n\nFigure 6: Probability–Entropy scatter plots of explorative tokens, displaying a random sample of 5% of all data\npoints.\n\n\nFigure 6 shows the probability–entropy distributions of key explorative tokens (“but”, “wait”, “perhaps”, “alternatively”, and “however”) under three methods: GRPO, GRPO + Entropy Loss, and our Lp-Reg.\n\n\nWith the baseline GRPO, these tokens are concentrated in low-entropy, high-probability regions. In this case, tokens\nlike “wait” tend to appear only when the model is already confident, turning them into deterministic patterns rather\nthan initiating a new exploration path with uncertainty.\n\n\nAdding an entropy loss changes this behavior, but in an uncontrolled way. Some sampled “wait” tokens appear at\nextremely high entropy levels (sometimes exceeding 10), which superficially boosts diversity but produces little\nuseful exploratory signal. These scattered occurrences do not integrate meaningfully into the reasoning process.\n\n\nOur Lp-Reg method yields a more balanced dynamic. Explorative tokens are sampled across a broad range of\nentropy values, from high probability to low probability states. This balance prevents their probabilities from\ncollapsing under negative feedback while keeping them informative for reasoning. As a result, tokens like “wait”\nremain viable options throughout training, allowing the model to explore alternative reasoning paths rather than\noverfitting to fixed usage patterns.\n\n\nFigure 7 further compares the frequency of explorative tokens (“but”, “wait”, “perhaps”, “alternatively”, and\n“however”) under GRPO and Lp-Reg. Our method consistently maintains a higher fraction of these tokens,\ndemonstrating that Lp-Reg not only broadens their probability–entropy distribution but also sustains their practical\nuse throughout training.\n\n\n8\n\n\n|Step 256|Step 256|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>|||||||\n|but<br>wait<br>perhaps<br>alternatively<br>however<br>0.0000<br>0.0005<br>0.0010<br>0.0015<br>0.0020<br>0.0025<br>0.0030<br>0.0035<br>||ut<br>w|ait<br>perh|aps<br>alterna|tively<br>ho|ever|\n\n\n|Step 768|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n|but<br>w|ait<br>perh|aps<br>alterna|ively<br>ho|ever|\n\n\n\nFigure 7: Frequency of explorative tokens during training.\n\n\n**6.3** **Probabilistic Distinction between Reasoning Sparks and Noise**\n\n\n\nOur introduction established a challenge for a successful exploration strategy: it must protect valuable, lowprobability _reasoning sparks_ without simultaneously\namplifying the destructive effects of irrelevant noise.\nThis raises a critical question: is there a systemic, observable difference between these two classes of tokens\nwithin the low-probability range that our method can\nexploit?\n\n\nTo investigate this, we analyze the next-token prediction distribution throughout the training process. Due to\nstorage limitations, we focus our analysis on the top-64\nmost probable tokens, but specifically examine those\nwithin a low-probability range ( 0 to 0.1 ) to isolate the\nphenomenon from high-probability tokens. Figure 8\nplots the average probability of two distinct classes of\ntokens over time: a group of meaningful exploratory tokens (e.g., “wait”, “perhaps”) and a group of irrelevant\ntokens (e.g., “cost”, “fine”).\n\n\n\n\n\n\n\n|Ex|plorator|y Token|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|~~Irr~~|~~levant~~|~~Token~~|||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\nFigure 8: Probabilistic distinction between exploratory\nand irrelevant tokens across training steps in standard\nGRPO training.\n\n\n\nThe results reveal a clear and consistent statistical distinction: across all training stages, the average next-token probability of meaningful exploratory tokens is persistently\nhigher than that of irrelevant tokens. It can be attributed to the intrinsic confidence of LLMs (Nguyen et al., 2025;\nXu et al., 2025; Fu et al., 2025). This persistent probabilistic gap provides the foundational justification for our\nLp-Reg design. It suggests that while a perfect separation is not possible, a probability threshold _τ_, as defined for our\nproxy distribution in Section 4.1, can serve as a principled filtering mechanism. By setting such a threshold, we can\neffectively filter out a substantial portion of the lowest-probability irrelevant tokens, which constitute destabilizing\nnoise, while simultaneously retaining a majority of the valuable exploratory tokens that give rise to _reasoning sparks_ .\nThis allows Lp-Reg to focus its regularization on tokens that are more likely to be meaningful, providing a targeted\nand robust approach to preserving high-quality exploration.\n\n\n**7** **Conclusion**\n\n\nIn this work, we investigated the exploration collapse in Reinforcement Learning with Verifiable Rewards. We\nidentified a key mechanism driving this failure: the systematic elimination of a class of valuable, low-probability\nexploratory tokens we term _reasoning sparks_ . To address this, we introduced Low-probability Regularization\n(Lp-Reg), a method designed to selectively preserve these crucial exploratory pathways. Lp-Reg is founded on\nthe key insight that within the low-probability range, meaningful exploratory tokens consistently exhibit higher\naverage probabilities than semantically irrelevant tokens, whose low-probability appearances act as noise. By\nleveraging this statistical distinction to filter out irrelevant noise and regularizing the policy towards the remainder,\nour method effectively protects valuable low-probability tokens from being extinguished. This focus on exploration\nquality over quantity enables stable on-policy training for around 1,000 steps, resulting in a 2.66% test accuracy\nimprovement over baselines and underscoring the importance of preserving the _reasoning sparks_ within the policy’s\nlow-probability tail.\n\n\n9\n\n\n**References**\n\n\nShivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy\n[minimization in llm reasoning, 2025. URL https://arxiv.org/abs/2505.15134.](https://arxiv.org/abs/2505.15134)\n\n\nTaubenfeld Amir, Sheffer Tom, Ofek Eran, Feder Amir, Goldstein Ariel, Gekhman Zorik, and Yona Gal. Confidence\nimproves self-consistency in llms. _arXiv preprint arXiv:2502.06233_, 2025. URL [https://www.arxiv.](https://www.arxiv.org/abs/2502.06233)\n[org/abs/2502.06233.](https://www.arxiv.org/abs/2502.06233)\n\n\nGhasemabadi Amirhosein, Mills Keith, G., Li Baochun, and Niu Di. Guided by gut: Efficient test-time scaling\nwith reinforced intrinsic confidence. _arXiv preprint arXiv:2505.20325v1_, 2025. URL [https://www.arxiv.](https://www.arxiv.org/abs/2505.20325v1)\n[org/abs/2505.20325v1.](https://www.arxiv.org/abs/2505.20325v1)\n\n\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei.\nReasoning with exploration: An entropy perspective on reinforcement learning for llms, 2025. URL [https:](https://arxiv.org/abs/2506.14758)\n[//arxiv.org/abs/2506.14758.](https://arxiv.org/abs/2506.14758)\n\n\nGanqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu\nChen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning\nDing. The entropy mechanism of reinforcement learning for reasoning language models, 2025. URL [https:](https://arxiv.org/abs/2505.22617)\n[//arxiv.org/abs/2505.22617.](https://arxiv.org/abs/2505.22617)\n\n\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao,\nZhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin,\nFucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng\nWang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang,\nJingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong,\nKai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang,\nLiyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang,\nMingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge,\nRuisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou,\nShanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou,\nShaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu,\nWenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang,\nXiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng\nSu, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan\nSong, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao\nLi, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao,\nYisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng\nZou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu,\nYanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren,\nZehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang\nYan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang,\nZhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via\n[reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948.](https://arxiv.org/abs/2501.12948)\n\n\nYichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence, 2025. URL [https:](https://arxiv.org/abs/2508.15260)\n[//arxiv.org/abs/2508.15260.](https://arxiv.org/abs/2508.15260)\n\n\nZitian Gao, Lynx Chen, Haoming Luo, Joey Zhou, and Bryan Dai. One-shot entropy minimization, 2025. URL\n\n[https://arxiv.org/abs/2505.20282.](https://arxiv.org/abs/2505.20282)\n\n\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang,\nYuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: A challenging benchmark for\npromoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins,\nand Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational_\n_Linguistics (Volume 1: Long Papers)_, pp. 3828–3850, Bangkok, Thailand, August 2024a. Association for\nComputational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL [https://aclanthology.org/](https://aclanthology.org/2024.acl-long.211/)\n[2024.acl-long.211/.](https://aclanthology.org/2024.acl-long.211/)\n\n\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie\nHuang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: A challenging\nbenchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024b.\n\n\n10\n\n\nJujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng\nXu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork\n[open reasoner 1 technical report, 2025. URL https://arxiv.org/abs/2505.22312.](https://arxiv.org/abs/2505.22312)\n\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring mathematical problem solving with the MATH dataset. In _Thirty-fifth Conference_\n_on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021. URL [https:](https://openreview.net/forum?id=7Bywt2mQsCe)\n[//openreview.net/forum?id=7Bywt2mQsCe.](https://openreview.net/forum?id=7Bywt2mQsCe)\n\n\nAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur,\nGuy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing_\n_Systems_ [, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7.](https://openreview.net/forum?id=IFXTZERXdM7)\n\n\nLi Loka, Chen Zhenhao, Chen Guangyi, Zhang Yixuan, Su Yusheng, Xing Eric, and Zhang Kun. Confidence matters:\nRevisiting intrinsic self-correction capabilities of large language models. _arXiv preprint arXiv:2402.12563_, 2024.\n[URL https://www.arxiv.org/abs/2402.12563.](https://www.arxiv.org/abs/2402.12563)\n\n\nMAA. American invitational mathematics examination (AIME). Mathematics Competition Series, n.d. URL\n\n[https://maa.org/math-competitions/aime.](https://maa.org/math-competitions/aime)\n\n\nMiniMax, :, Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu,\nChao Wang, Cheng Zhu, Chengjun Xiao, Chengyu Du, Chi Zhang, Chu Qiao, Chunhao Zhang, Chunhui Du,\nCongchao Guo, Da Chen, Deming Ding, Dianjun Sun, Dong Li, Enwei Jiao, Haigang Zhou, Haimo Zhang,\nHan Ding, Haohai Sun, Haoyu Feng, Huaiguang Cai, Haichao Zhu, Jian Sun, Jiaqi Zhuang, Jiaren Cai, Jiayuan\nSong, Jin Zhu, Jingyang Li, Jinhao Tian, Jinli Liu, Junhao Xu, Junjie Yan, Junteng Liu, Junxian He, Kaiyi Feng,\nKe Yang, Kecheng Xiao, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Li, Lin Zheng, Linge Du, Lingyu\nYang, Lunbin Zeng, Minghui Yu, Mingliang Tao, Mingyuan Chi, Mozhi Zhang, Mujie Lin, Nan Hu, Nongyu\nDi, Peng Gao, Pengfei Li, Pengyu Zhao, Qibing Ren, Qidi Xu, Qile Li, Qin Wang, Rong Tian, Ruitao Leng,\nShaoxiang Chen, Shaoyu Chen, Shengmin Shi, Shitong Weng, Shuchang Guan, Shuqi Yu, Sichen Li, Songquan\nZhu, Tengfei Li, Tianchi Cai, Tianrun Liang, Weiyu Cheng, Weize Kong, Wenkai Li, Xiancai Chen, Xiangjun\nSong, Xiao Luo, Xiao Su, Xiaobo Li, Xiaodong Han, Xinzhu Hou, Xuan Lu, Xun Zou, Xuyang Shen, Yan Gong,\nYan Ma, Yang Wang, Yiqi Shi, Yiran Zhong, Yonghong Duan, Yongxiang Fu, Yongyi Hu, Yu Gao, Yuanxiang\nFan, Yufeng Yang, Yuhao Li, Yulin Hu, Yunan Huang, Yunji Li, Yunzhi Xu, Yuxin Mao, Yuxuan Shi, Yuze\nWenren, Zehan Li, Zelin Li, Zhanxu Tian, Zhengmao Zhu, Zhenhua Fan, Zhenzhen Wu, Zhichao Xu, Zhihang\nYu, Zhiheng Lyu, Zhuo Jiang, Zibo Gao, Zijia Wu, Zijian Song, and Zijun Sun. Minimax-m1: Scaling test-time\n[compute efficiently with lightning attention, 2025. URL https://arxiv.org/abs/2506.13585.](https://arxiv.org/abs/2506.13585)\n\n\nKevin Murphy. Reinforcement learning: an overview. _arXiv preprint arXiv:2412.05265_, 2024.\n\n\nMinh Nhat Nguyen, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, and Ravid Shwartz-Ziv. Turning\nup the heat: Min-p sampling for creative and coherent llm outputs, 2025. URL [https://arxiv.org/abs/](https://arxiv.org/abs/2407.01082)\n[2407.01082.](https://arxiv.org/abs/2407.01082)\n\n\nOpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander\nNeitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea\nVallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin\nNair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys\nMinaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi,\nCary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris\nKoch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel\nLevy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li,\nDragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell,\nEric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos\nTsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas\nChabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin,\nHessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian\nOsband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki,\nJames Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu,\nJoaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan ˜\nGordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal,\nKarina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin\nLu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian\nWeng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz,\n\n\n11\n\n\nMadelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt\nKaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia\nGlaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail\nPavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil\nChowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk,\nOlivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall\nLin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown,\nRoshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel\nMiserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao,\nShengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph\nLin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted\nSanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov,\nTom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie\nMonaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann\nDubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao,\n[and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720.](https://arxiv.org/abs/2412.16720)\n\n\nKadavath Saurav, Conerly Tom, Askell Amanda, Henighan Tom, Drain Dawn, Perez Ethan, Schiefer Nicholas,\nHatfield-Dodds Zac, DasSarma Nova, Tran-Johnson Eli, Johnston Scott, El-Showk Sheer, Jones Andy, Elhage\nNelson, Hume Tristan, Chen Anna, Bai Yuntao, Bowman Sam, Fort Stanislav, Ganguli Deep, Hernandez Danny,\nJacobson Josh, Kernion Jackson, Kravec Shauna, Lovitt Liane, Ndousse Kamal, Olsson Catherine, Ringer\nSam, Amodei Dario, Brown Tom, Clark Jack, Joseph Nicholas, Mann Ben, McCandlish Sam, Olah Chris, and\nKaplan Jared. Language models (mostly) know what they know. _arXiv preprint arXiv:2207.05221_, 2022. URL\n[https://www.arxiv.org/abs/2207.05221.](https://www.arxiv.org/abs/2207.05221)\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization\nalgorithms. _arXiv preprint arXiv:1707.06347_, 2017.\n\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nY. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language\n[models, 2024a. URL https://arxiv.org/abs/2402.03300.](https://arxiv.org/abs/2402.03300)\n\n\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\n_arXiv preprint arXiv:2402.03300_, 2024b.\n\n\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and\nChuan Wu. Hybridflow: A flexible and efficient rlhf framework. _arXiv preprint arXiv: 2409.19256_, 2024.\n\n\nZhenpeng Su, Leiyu Pan, Xue Bai, Dening Liu, Guanting Dong, Jiaming Huang, Wenping Hu, Fuzheng Zhang,\nKun Gai, and Guorui Zhou. Klear-reasoner: Advancing reasoning capability via gradient-preserving clipping\n[policy optimization, 2025. URL https://arxiv.org/abs/2508.07629.](https://arxiv.org/abs/2508.07629)\n\n\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang\nTang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao\nZhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan\nYuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan\nWu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong,\nShaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong,\nWeiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu,\nXinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru\nChen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang,\nZaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao\nHuang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025.\n[URL https://arxiv.org/abs/2501.12599.](https://arxiv.org/abs/2501.12599)\n\n\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang,\nZhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and\nJunyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm\n[reasoning, 2025. URL https://arxiv.org/abs/2506.01939.](https://arxiv.org/abs/2506.01939)\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL [https:](https://arxiv.org/abs/2201.11903)\n[//arxiv.org/abs/2201.11903.](https://arxiv.org/abs/2201.11903)\n\n\n12\n\n\nZenan Xu, Zexuan Qiu, Guanhua Huang, Kun Li, Siheng Li, Chenchen Zhang, Kejiao Li, Qi Yi, Yuhao Jiang,\nBo Zhou, Fengzong Lian, and Zhanhui Kang. Adaptive termination for multi-round parallel reasoning: An\n[universal semantic entropy-guided framework, 2025. URL https://arxiv.org/abs/2507.06829.](https://arxiv.org/abs/2507.06829)\n\n\nZhao Xuandong, Kang Zhewei, Feng Aosong, Levine Sergey, and Song Dawn. Learning to reason without\nexternal rewards. _arXiv preprint arXiv:2505.19590v2_, 2025. URL [https://www.arxiv.org/abs/2505.](https://www.arxiv.org/abs/2505.19590v2)\n[19590v2.](https://www.arxiv.org/abs/2505.19590v2)\n\n\nWang Xuezhi and Zhou Denny. Chain-of-thought reasoning without prompting. _arXiv preprint arXiv:2402.10200v2_,\n[2024. URL https://www.arxiv.org/abs/2402.10200v2.](https://www.arxiv.org/abs/2402.10200v2)\n\n\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan\nLin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou,\nJunyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei\nZhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao\nYin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3\n[technical report, 2025. URL https://arxiv.org/abs/2505.09388.](https://arxiv.org/abs/2505.09388)\n\n\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong\nLiu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan\nZhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan\nSong, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui\nWu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL\n[https://arxiv.org/abs/2503.14476.](https://arxiv.org/abs/2503.14476)\n\n\nYu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian\nFan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin,\nBole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui\nWu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. URL\n[https://arxiv.org/abs/2504.05118.](https://arxiv.org/abs/2504.05118)\n\n\nYuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui,\nQixiang Ye, Fang Wan, and Furu Wei. Geometric-mean policy optimization, 2025. URL [https://arxiv.](https://arxiv.org/abs/2507.20673)\n[org/abs/2507.20673.](https://arxiv.org/abs/2507.20673)\n\n\nChujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui\nMen, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL [https:](https://arxiv.org/abs/2507.18071)\n[//arxiv.org/abs/2507.18071.](https://arxiv.org/abs/2507.18071)\n\n\nOmer Veysel ¨ C¸ agatan and Barı ˘ s¸ Akgun. Failure modes of maximum entropy rlhf, 2025. URL ¨ [https://arxiv.](https://arxiv.org/abs/2509.20265)\n[org/abs/2509.20265.](https://arxiv.org/abs/2509.20265)\n\n\n13\n\n\n## **Appendix**\n\n**A** **Details of Experiments**\n\n\n**A.1** **Further Training Dynamics**\n\n\nThe training dynamics of Lp-Reg and other RLVR methods on the Qwen2.5-32B base model are presented in\nFigure 9. The results show that Lp-Reg maintains a comparable performance in test accuracy throughout the training\nprocess, underscoring the benefits of our low-probability token regularization strategy for preventing exploration\ncollapse.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Training dynamics on the Qwen2.5-32B-Base model. To best illustrate the performance differences, we\ncompare the top-performing methods.\n\n\n**A.2** **Further Ablation Study**\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Ablation study comparing low-probability token regularization versus high-entropy token regularization\nfor Lp-Reg (on-policy) on the Qwen3-14B-Base model. The test datasets used are AIME-24 and AIME-25, with the\naverage score reported over 16 test runs.\n\n\n**Low Probability vs. High Entropy.** To verify that targeting low-probability tokens is superior to the conventional\nwisdom of targeting high entropy, we conduct a comparison between the high-entropy token regularization (w/\nhighest _H_ regularization) and the low-probability regularization (w/ lowest _π_ _**θ**_ regularization, vanilla Lp-Reg).\nInstead of applying Lp-Reg to the lowest 1% probability tokens, we apply an identical regularization mechanism to\nthe tokens with the highest 1% entropy. As shown in Figure 10, this approach not only fails to improve performance\nbut also fails to sustain high entropy, which collapses after an initial spike. This result reinforces our claim from\nthe Introduction: high entropy is a poor proxy for valuable exploration. As our analysis in Section 6.1 further\ncorroborates, high-entropy tokens are often common function words or formatting characters, not the meaningful,\nlow-probability exploratory tokens we term _reasoning sparks_ . Regularizing them pollutes the learning signal without\nprotecting the structured, low-probability reasoning paths necessary for progress.\n\n\n**Forward KL vs. Reverse KL.** We compare the performance of our chosen “forward KL” formulation, which is\n_D_ KL ( _π_ proxy _∥_ _π_ _**θ**_ ), against the ”reverse KL” formulation, _D_ KL ( _π_ _**θ**_ _∥_ _π_ proxy ), in Equation 6. As shown in Figure 11,\nour forward KL significantly outperforms the reverse KL. This result stems from the fact that our proxy distribution,\n_π_ proxy, is a heuristic reference derived from the current policy, not an ideal target distribution. The reverse KL,\n\n\n14\n\n\n_D_ KL ( _π_ _**θ**_ _∥_ _π_ proxy ), penalizes any deviation of _π_ _**θ**_ from _π_ proxy, effectively forcing the policy to strictly imitate this\nnon-ideal, heuristic target. This aggressive imitation constrains the protection of potentially valuable exploratory\ntokens. In contrast, the forward KL, _D_ KL ( _π_ proxy _∥_ _π_ _**θ**_ ), provides a much softer regularization: it only penalizes the\npolicy for completely discarding tokens that _π_ proxy considers plausible, without forcing a strict match. This allows\nthe policy to use _π_ proxy as a stabilizing guide while retaining the freedom to explore beyond it, which empirically\nleads to better performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|0.60<br>0.55<br>0.50<br>0.45<br>0.40<br>0.35<br>0.30<br>0 200<br>Trai|Col2|\n|---|---|\n|0<br>200<br><br>Trai<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>||\n|0<br>200<br><br>Trai<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>||\n|0<br>200<br><br>Trai<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>||\n|0<br>200<br><br>Trai<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>||\n|0<br>200<br><br>Trai<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>||\n|0<br>200<br><br>Trai<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>|400<br>600<br>800<br>ning Steps|\n\n\n\nFigure 11: Ablation study comparing the forward and reverse KL formulations for the Lp-Reg penalty on the\nQwen3-14B-Base model. The results demonstrate the superiority of the forward KL, which uses the heuristic proxy\ndistribution as a soft guide, over the reverse KL, which forces a strict imitation.\n\n\n\n\n\n\n\n\n|15|Col2|\n|---|---|\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)|300<br>400<br>500<br>600<br>ining Steps|\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Ablation study on _ρ_ which defined the low-probability percentile threshold _δ_ _ρ_ _[B]_ .\n\n\n\n\n\n\n\n\n\n\n\n\n|entile threshold δBρ . 3|Col2|\n|---|---|\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)||\n|0<br>100<br>200<br>Tra<br>0.30<br>0.35<br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Avg. Test Accuracy (%)|300<br>400<br>500<br>600<br>ining Steps|\n\n\n\n(b) Ablation study on _κ_ which defined the noise threshold _τ_ = _κ_ _·_ max _o_ _′_ _∈_ _V_ _π_ _**θ**_ ( _o_ _[′]_ _|·_ )\n\n\nFigure 12: Training dynamics of Lp-Reg method with different hyperparameters.\n\n\n**A.3** **Hyperparameter Sensitivity Analysis**\n\n\nIn this section, we analyze the sensitivity of two core hyperparameters in Lp-Reg to demonstrate the robustness of\nour method: the low-probability percentile _ρ_ and the min-p ratio _κ_ . The results are presented in Figure 12.\n\n\nThe parameter _ρ_, as defined in our objective function (Equation 6), determines the percentile threshold for identifying\nlow-probability tokens that are candidates for regularization. A higher _ρ_ means a wider range of tokens are protected.\nAs shown in the top panel of Figure 12, we evaluated _ρ_ with values of 0.005, 0.010, and 0.015 . The training\ntrajectories for average test accuracy are comparable, and the final performance across all three settings is highly\ncomparable. This indicates that Lp-Reg is not overly sensitive to the precise scope of tokens being protected within\nthis reasonable range.\n\n\nThe hyperparameter _κ_ controls the adaptiveness of the min-p filtering threshold, which defines the boundary for\nwhat is treated as noise. A smaller _κ_ results in a more conservative filtering strategy, removing fewer tokens. Our\n\n\n15\n\n\nsensitivity analysis for _κ_, presented in the bottom panel of Figure 12, shows a similar trend of stability. Across the\ntested values of 0.01, 0.02, and 0.03, the training curves and final performance remain consistently high and tightly\nclustered. Taken together, these results demonstrate the robustness of Lp-Reg. The method’s effectiveness is not\ncontingent on extensive, fine-grained hyperparameter tuning, highlighting its practical applicability.\n\n\n**B** **Further Analysis**\n\n\n**B.1** **Details of Sampling Probability Density**\n\n\nThis section provides a detailed, token-by-token breakdown of the aggregated distributions presented in Figure 1c\nand Figure 1d of the main paper, reinforcing the conclusions drawn from our analysis.\n\n\nFigure 14 exhibits the individual distribution of observed sampling probabilities for a class of meaningful lowprobability exploratory tokens we term _reasoning sparks_ : “but”, “wait”, “perhaps”, “alternatively”, and “however”.\nA consistent trend is observable across all five tokens, validating our claims in the introduction. With standard\nGRPO training, the ability to sample these tokens at low probabilities is systematically eliminated, causing their\ndistributions to collapse and shift towards higher probabilities. The indiscriminate entropy bonus (GRPO + Entropy\nLoss) is largely ineffective at restoring this crucial low-probability tail. In stark contrast, our proposed method,\nLp-Reg, consistently maintains a healthy, wide distribution for each of these tokens, demonstrating its effectiveness\nin preserving the model’s capacity for exploration.\n\n\nConversely, Figure 15 details the behavior of a class of what we term irrelevant tokens (e.g. “cost”, “fine”,\n“balanced”, “ere”, and “trans”). We refer to the low-probability sampling of these tokens as irrelevant noise, which\ncan be destructive to the training process. These individual plots clearly illustrate the detrimental side effect of\na simple entropy bonus. For nearly every token, the GRPO + Entropy Loss baseline significantly amplifies the\nsampling of this irrelevant noise, which, as shown in our main analysis, contributes to a faster performance collapse.\nLp-Reg, by design, avoids this amplification and maintains a suppressed probability distribution for these tokens,\ncomparable to or even more constrained than the standard GRPO baseline.\n\n\nThese detailed visualizations confirm that the elimination of _reasoning sparks_ and the amplification of irrelevant\nnoise are not artifacts of aggregation but are consistent patterns at the individual token level. This provides\nstrong, granular evidence for the central challenge our paper addresses and highlights the necessity of a selective\npreservation mechanism like Lp-Reg.\n\n\n**B.2** **Details of Probability-Entropy Distribution**\n\n\nTo supplement the aggregated analysis presented in Figure 6 of the main text, this section provides a detailed\nbreakdown of the probability-entropy distributions for individual meaningful exploratory tokens. Figure 16 shows\na consistent pattern across all representative tokens, ranging from “but” (Figure 16a) to “however” (Figure 16e).\nFor frequently occurring tokens such as “but”, “wait”, and “perhaps”, we randomly subsample one out of every 20\ninstances for visualization. Under the baseline GRPO, these sparks are consistently confined to a low-entropy, highprobability region, indicating a collapse into deterministic usage. In contrast, the addition of an entropy loss pushes\nthese tokens into highly scattered, often excessively high-entropy states, suggesting an uncontrolled and potentially\nnoisy form of exploration. Our method, Lp-Reg, strikes a crucial balance, maintaining a structured and broad\ndistribution across a healthy range of entropy values. This consistent behavior demonstrates that the trends identified\nin the aggregated data are not artifacts of averaging. The individual plots offer strong, disaggregated evidence for\nour central claim: Lp-Reg effectively preserves the exploratory potential of reasoning sparks by preventing both the\ndeterministic collapse seen in the baseline and the chaotic scattering induced by the indiscriminate entropy bonus.\n\n\n**B.3** **Training Dynamics of Regularized Token**\n\n\nTo better understand how Lp-Reg operates during training, we analyze the dynamics of the probability threshold\n\n_δ_ _ρ_ _[B]_ [and the proportion of low-probability tokens subjected to regularization] _|_ _π_ _**θ**_ ( _o_ _|·_ ) _|<_ _πδ_ _**θ**_ _ρ_ _[B]_ ( _∧_ _o_ _|·_ _π_ ) _<_ proxy _δ_ _ρ_ _[B]_ _|_ ( _o_ _|·_ ) _>_ 0 _|_ . As shown in\n\nFigure 13, the threshold _δ_ _ρ_ _[B]_ [gradually decreases with training steps. In parallel, the regularization ratio also declines]\nsteadily. This trend suggests that as training progresses, the extreme low-probability range becomes increasingly\ndominated by irrelevant tokens, constituting what we term irrelevant noise. In parallel, the semantically meaningful\nexploratory tokens are lifted into higher-probability regions, thus requiring less regularization.\n\n\n**B.4** **Case Study**\n\n\nTo further illustrate the effect of the filter applied on low-probability tokens, Figure 17 presents a case study of a\nmodel-generated response, where low-probability tokens are highlighted according to whether they were preserved\nor filtered. Tokens with probability greater than _τ_ are those retained by the filter, while tokens with probability\n\n\n16\n\n\n|Probability Threshold and Regularization Ratio|Col2|robability Thresho|Col4|old and Regulariz|Col6|zation Ra|atio|\n|---|---|---|---|---|---|---|---|\n|128<br>256<br>384<br>512<br>640<br>768<br>Training Steps<br>0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>Probability Threshold and Regularization Ratio<br>Probability Threshold<br>Regularization Ratio<br>7.5<br>10.0<br>12.5<br>15.0<br>17.5<br>20.0<br>22.5<br>25.0<br>27.5<br>Regularization Ratio|128<br>256<br>384<br>512<br>640<br>768<br>Training Steps<br>0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>Probability Threshold and Regularization Ratio<br>Probability Threshold<br>Regularization Ratio<br>7.5<br>10.0<br>12.5<br>15.0<br>17.5<br>20.0<br>22.5<br>25.0<br>27.5<br>Regularization Ratio|obability Thresho|obability Thresho|ld and Regulariz<br>Probabili<br>Regulari|ld and Regulariz<br>Probabili<br>Regulari|ation R<br>ty Thresho<br>zation Rati|tio<br>ld<br>o<br>17.5<br>20.0<br>22.5<br>25.0<br>27.5<br>ization Ratio|\n|128<br>256<br>384<br>512<br>640<br>768<br>Training Steps<br>0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>Probability Threshold and Regularization Ratio<br>Probability Threshold<br>Regularization Ratio<br>7.5<br>10.0<br>12.5<br>15.0<br>17.5<br>20.0<br>22.5<br>25.0<br>27.5<br>Regularization Ratio||||||||\n|128<br>256<br>384<br>512<br>640<br>768<br>Training Steps<br>0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>Probability Threshold and Regularization Ratio<br>Probability Threshold<br>Regularization Ratio<br>7.5<br>10.0<br>12.5<br>15.0<br>17.5<br>20.0<br>22.5<br>25.0<br>27.5<br>Regularization Ratio||||||||\n|128<br>256<br>384<br>512<br>640<br>768<br>Training Steps<br>0.00<br>0.02<br>0.04<br>0.06<br>0.08<br>Probability Threshold and Regularization Ratio<br>Probability Threshold<br>Regularization Ratio<br>7.5<br>10.0<br>12.5<br>15.0<br>17.5<br>20.0<br>22.5<br>25.0<br>27.5<br>Regularization Ratio||||||||\n\n\nFigure 13: Training dynamics of the probability threshold and regularization ratio.\n\n\nsmaller than _τ_ are suppressed. The preserved tokens include meaningful exploratory markers such as ”Then”,\n”Wait”, which guide the reasoning trajectory, whereas the discarded set largely consists of relatively irrelevant tokens\nsuch as ”We”, ”also”, ”that”. This qualitative evidence complements our quantitative analysis, demonstrating that\nLp-Reg effectively leverages the min-p transformation to distinguish between valuable low-probability exploratory\ntokens ( _reasoning sparks_ ) and tokens that constitute destabilizing, irrelevant noise.\n\n\n17\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n=|656<br>n=|56<br>n=|656<br>n=2|239<br>n=11|311<br>n=28|73<br>|~~=2172~~8<br>n=35|21|\n||||||||||\n||||||||||\n||||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n|31<br>n|31<br>n|31<br>n=1|454<br>n=15|102<br>n=17|53<br>|~~n=8388~~<br>n=17|57|\n||||||||||\n||||||||||\n||||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n=12<br>n|n=12<br>n|12<br>n=12<br>n=1|12<br>n=12<br>n=1|965<br>n=4|86<br>n=16|65<br>|~~n=6875~~<br>n=10|59|\n||||||||||\n||||||||||\n||||||||||\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n=14<br>n|n=14<br>n|14<br>n=14<br>n=4|14<br>n=14<br>n=4|891<br>n=1|19<br>n=7|55<br>|n=3125<br>n=61|50|\n||||||||||\n||||||||||\n||||||||||\n\n\n(d) Density of observed sampling probabilities for token “alternatively”.\n\n\n\n(a) Density of observed sampling probabilities for token “but”.\n\n\n\n\n\n(b) Density of observed sampling probabilities for token “wait”.\n\n\n\n\n\n(c) Density of observed sampling probabilities for token “perhaps”.\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n=|499<br>n=|99<br>n=|499<br>n=1|864<br>n=2|52<br>n=2|00<br>|~~n=1907~~<br>n=40|92|\n||||||||||\n||||||||||\n||||||||||\n\n\n(e) Density of observed sampling probabilities for token “however”.\n\n\nFigure 14: Individual density of observed sampling probabilities for meaningful exploratory tokens: “but”, “wait”,\n“perhaps”, “alternatively”, and “however”.\n\n\n18\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n=|156<br>n=|56<br>n=|156<br>~~n=60~~|n=5|1<br>~~n=89~~|n=107|n=246<br>n|96|\n||||||||||\n||||||||||\n||||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|n=3<br>n|3<br>n=3<br>n=13|n=4|1<br>n=32|n=23|n=130<br>n|23|\n||||||||\n||||||||\n||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|n=1<br>n|1<br>n=1<br>n=43|n=2|1<br>n=30|n=38|n=95<br>n|30|\n||||||||\n||||||||\n||||||||\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|n|27<br>n|=27<br>|n=27<br>n=0<br>n=17|n=0<br>n=0<br>n=|7<br>n=0|\n|||||||\n|||||||\n|||||||\n\n\n(d) Density of observed sampling probabilities for token “ere”.\n\n\n\n(a) Density of observed sampling probabilities for token “cost”.\n\n\n\n\n\n(b) Density of observed sampling probabilities for token “fine”.\n\n\n\n\n\n(c) Density of observed sampling probabilities for token “balanced”.\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n|n|5<br>n|5<br>n|5<br>n=208|n=6|4<br>n=79|n=166|n=446<br>n|91|\n||||||||||\n||||||||||\n||||||||||\n\n\n(e) Density of observed sampling probabilities for token “trans”.\n\n\nFigure 15: Individual density of observed sampling probabilities for irrelevant tokens: “cost”, “fine”, “balanced”,\n“ere”, and “trans”.\n\n\n19\n\n\n|3.5|Mean Prob:|n = 21728 : 0.9315 ± 0.1801|Col4|Mean Prob: 0.|\n|---|---|---|---|---|\n|.5<br>3.0<br>.5|~~Mean Prob~~<br>Mean Entropy|.8<br>1.0<br><br>~~: 0.9315 ± 0.1801~~<br>: 0.1189 ± 0.2440<br>0.0<br>0.2<br>0<br><br>0<br>2<br>4<br>6<br>8<br>10<br>Entropy||Mean Prob: 0.<br>Mean Entropy: 1.|\n|.5<br>3.0<br>.5|||||\n|2.0<br>|||||\n|1.0<br>1.5|||||\n|1.0<br>1.5|||||\n|1.0<br>1.5|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0<br><br>0.0<br>0.5|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0<br><br>0.0<br>0.5||||.4<br>0.6<br>0.8<br>|\n\n\n|M|n = 3582 Mean Prob: 0.7323 ± 0.303|\n|---|---|\n|Me|Mean Prob: 0.7323 ± 0.30<br>an Entropy: 0.5976 ± 0.58|\n|||\n|||\n|||\n|||\n||6<br>0.8<br>1.0<br>|\n\n\n\n(a) Scattered probability–entropy plot of observed sampling instances for the token\n“but”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Mean Prob:|n = 8388 : 0.8975 ± 0.2159|Col4|Mean Prob: 0.|\n|---|---|---|---|---|\n|.5<br>2.0|Mean Prob<br>Mean Entropy|.8<br>1.0<br><br>: 0.8975 ± 0.2159<br>: 0.1747 ± 0.2816<br>0.0<br>0.2<br>0<br><br>0<br>2<br>4<br>6<br>8<br>10<br>Entropy||Mean Prob: 0.<br>Mean Entropy: 1.|\n|.5<br>1.0<br>|||||\n|.5<br>1.0<br>|||||\n|.5<br>1.0<br>|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0<br><br>0.0<br>|||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0<br><br>0.0<br>||||.4<br>0.6<br>0.8<br>|\n\n\n|M|n = 1715 Mean Prob: 0.7356 ± 0.307|\n|---|---|\n|Me|Mean Prob: 0.7356 ± 0.30<br>an Entropy: 0.5430 ± 0.53|\n|||\n|||\n|||\n||6<br>0.8<br>1.0<br>|\n\n\n\n(b) Scattered probability–entropy plot of observed sampling instances for the token\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|ait”. 2.00|Mean Prob GRPO|\n|---|---|\n|1.25<br>1.50<br>1.75|Mean Pro<br>Mean Entropy|\n|1.25<br>1.50<br>1.75||\n|.75<br>1.00||\n|.75<br>1.00||\n|0.50<br>||\n|0.50<br>||\n|0.0<br>0.2<br>0.4<br>0.6<br><br><br>0.00<br>.25||\n\n\n|Col1|MeaMne Eannt Proropby:: 20|..13175327 ±± 10..92695351 4.0 Me<br>3.5<br>3.0<br>2.5 Entropy<br>2.0<br>1.5<br>1.0<br>0.5<br>1.0 0.00.0 0.2 0.4 0.|Me|aMne Eannt Proropby:: 00..67603499 ±± 00..6311|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n||0.4<br>0.6<br>0.8<br>|0.4<br>0.6<br>0.8<br>|0.4<br>0.6<br>0.8<br>|6<br>0.8<br>1.0<br>|\n\n\n\n(c) Scattered probability–entropy plot of observed sampling instances for the token\n“perhaps”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|1.6|Mean Prob: GRPO|\n|---|---|\n|1.2<br>1.4<br>|Mean Prob:<br>Mean Entropy:|\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.0<br>0.2<br>0.4<br>0.6<br>0<br><br>0.0<br>.2||\n\n\n|Col1|Mean Prob: 0.3|n = 1419 3093 ± 0.2816|M|n = 615 Mean Prob: 0.6740 ± 0.315|\n|---|---|---|---|---|\n||Mean Prob: 0.<br>Mean Entropy: 2.|3093 ± 0.2816<br>0874 ± 2.0776<br>2.5<br>3.0<br>3.5<br>4.0<br>ropy|Me|Mean Prob: 0.6740 ± 0.31<br>an Entropy: 0.7196 ± 0.59|\n||Mean Prob: 0.<br>Mean Entropy: 2.|3093 ± 0.2816<br>0874 ± 2.0776<br>2.5<br>3.0<br>3.5<br>4.0<br>ropy|||\n||||||\n|||1.0<br>0.0<br>0.2<br>0.4<br>0.<br><br>0.0<br>0.5<br>1.0<br>1.5<br>2.0<br>Ent|||\n||||||\n||||||\n||||||\n||.4<br>0.6<br>0.8<br>|.4<br>0.6<br>0.8<br>|.4<br>0.6<br>0.8<br>|6<br>0.8<br>1.0<br>|\n\n\n\n(d) Scattered probability–entropy plot of observed sampling instances for the token\n“alternatively”.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Mean Prob:|\n|---|---|\n|1.2<br>1.4<br>1.6|Mean Prob:<br>~~Mean Entropy:~~|\n|1.2<br>1.4<br>1.6||\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.4<br>0.6<br>0.8<br>1.0<br>||\n|0.0<br>0.2<br>0.4<br>0.6<br>0<br><br>0.0<br>.2||\n\n\n|Col1|MeaMne Eannt Proropby:: 20..|03075811 ±± 20..13311360<br>3.5<br>3.0<br>2.5<br>tropy<br>2.0|Me|aMne Eannt Proropby:: 00..86402011 ±± 00..6312|\n|---|---|---|---|---|\n||Mean Prob: 0.<br>Mean Entropy: 2.|3781 ± 0.3130<br>0051 ± 2.1316<br>2.0<br>2.5<br>3.0<br>3.5<br>tropy|||\n||||||\n|||1.0<br>0.0<br>0.2<br>0.4<br>0.<br><br>0.0<br>0.5<br>1.0<br>1.5<br>En|||\n|||1.0<br>0.0<br>0.2<br>0.4<br>0.<br><br>0.0<br>0.5<br>1.0<br>1.5<br>En|||\n||||||\n||.4<br>0.6<br>0.8<br>|.4<br>0.6<br>0.8<br>|.4<br>0.6<br>0.8<br>|6<br>0.8<br>1.0<br>|\n\n\n\n(e) Scattered probability–entropy plot of observed sampling instances for the token\n“however”.\n\n\nFigure 16: Individual scattered probability–entropy plot of observed sampling instances for meaningful tokens:\n“but”, “wait”, “perhaps”, “alternatively”, and “however”.\n\n\n20\n\n\nFigure 17: An Example generated by Qwen3-14B-Base model trained by Lp-Reg from math reasoning.\n\n\n21\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03217v1",
      "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair",
      "authors": [
        "José Cambronero",
        "Michele Tufano",
        "Sherry Shi",
        "Renyao Wei",
        "Grant Uy",
        "Runxiang Cheng",
        "Chin-Jung Liu",
        "Shiying Pan",
        "Satish Chandra",
        "Pat Rondon"
      ],
      "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.",
      "published": "2025-10-03T17:53:28Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03217v1"
    },
    "text": "## **Abstain and Validate: A Dual-LLM Policy for Reducing Noise in** **Agentic Program Repair**\n\nJosé Cambronero [1], Michele Tufano [1], Sherry Shi [1], Renyao Wei [1], Grant Uy [1], Runxiang Cheng [1],\nChin-Jung Liu [1], Shiying Pan [1], Pat Rondon [1], Satish Chandra [2 ∗]\n\n1 Google, USA\n2 Meta, USA\n{jcambronero,tufanomichele,sherryyshi,renyaow,grantuy,chengsam,chinjungliu,shiyingp,rondon}@google.com\nschandra@acm.org\n\n\n\n**Abstract**\n\n\nAgentic Automated Program Repair (APR) is increasingly tackling\ncomplex, repository-level bugs in industry, but ultimately agentgenerated patches still need to be reviewed by a human before\ncommitting them to ensure they address the bug. Showing unlikely\npatches to developers can lead to substantial noise, wasting valuable\ndeveloper time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such\nnoise: bug abstention and patch validation policies. Bug abstention\nexcludes bugs that the agentic APR system is unlikely to fix. Patch\nvalidation rejects patches that are unlikely to be a good fix for the\ngiven bug. We evaluate both policies on three sets of bugs from\nGoogle’s codebase, and their candidate patches generated by an\ninternal agentic APR system. On a set of 174 human-reported bugs,\nremoving bugs and patch trajectories rejected by our policies can\nraise success rates by up to 13 percentage points and 15 percentage\npoints, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with\nmachine-generated bug reports, patch validation also improves\naverage single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of\nagentic APR systems.\n\n\n**1** **Introduction**\n\n\nA substantial portion of software development involves diagnosing\nand repairing bugs. To mitigate this cost, the research community\nhas long invested in Automated Program Repair (APR) [ 16 – 18, 23 ].\nMore recently, advanced language models have enabled more complex, autonomous “repository-level” program repair, where a system manages diagnosis, localization, and repair. Systems like SWEAgent [ 35 ], AutoCodeRover [ 36 ], SpecRover [ 28 ], and RepairAgent [ 3 ] show that agent-based APR can tackle such challenge,\nachieving pass@1 bug resolution rates up to 75.20% on benchmarks\nlike SWE-Bench-Verified (TRAE as of 09/02/2025 [11]).\nIn an industry setting, however, maximizing the rate of successful\npatches shown to a developer from an APR system is as important as\nmaximizing the system’s overall bug resolution rate. While pass@1\nrates are improving, deploying these agents in production still\nrequires a human developer to review and validate the generated\npatches. Because developer time is limited, reviewing patches for\nintractable bugs or unlikely fixes wastes valuable time. Reducing\nsuch noise from unproductive APR attempts is thus crucial, not only\nto save developer time but also to build developer trust in automated\n\n\n∗ Work done while at Google.\n\n\n\ncode changes, which is essential for AI tool adoption [ 4, 24 ]. This\nwork aims to tackle this challenge by proposing two policies.\nFirst, inspired by abstention in classification [ 6, 12, 13 ], we introduce bug abstention (Section 3.1). We use an LLM to predict if our\nAPR system is likely to produce an acceptable patch for a given bug,\nusing only the bug report information. This check is performed\n_before_ the agent attempts a repair. If the predicted probability of\nsuccess is below a threshold, we abstain from running the repair\nagent. This avoids showing developers unlikely patches and saves\ncompute costs from futile repair attempts.\nSecond, for any patches produced, we employ a multi-stage patch\nvalidation policy to predict if the patch is correct (Section 3.2). This\npolicy composes deterministic checks (e.g., running tests) and an\nLLM-based filter. The LLM filter first generates a “fix specification”\n—a guideline for a correct fix—based on the bug description and original source files, without knowledge of any ground-truth solution.\nA second LLM call uses this specification and the agent’s trajectory\nto produce a boolean (fixed or not) outcome, an explanation, and a\nconfidence score derived from token-level log probabilities.\nWe evaluate the bug abstention and patch validation policies\nboth independently and in combination (Section 4). Our combined,\ntwo-policy approach first employs bug abstention to screen out bugs\nunlikely to be fixed, then runs the APR system on the remaining\nbugs, and finally uses patch validation to filter the generated patches.\nOur evaluation shows these policies are complementary, improving\nsuccess rates most effectively when combined.\nWe evaluate our policies on three sets of bugs from Google’s\ncodebase (Section 5). Similar to recent work [ 14, 20, 27 ], the first set\nhas 174 human-reported bugs with known ground-truth fixes and\nfail-to-pass reproduction tests. We evaluate both policies on repair\ntrajectories generated by our ReAct-style APR system for this set.\nWe further study patch validation on 198 machine-reported Java\nnull pointer exception (NPE) bugs from a live deployment and 50\nhistorical, sanitizer-reported bugs with known reproduction tests.\nBecause our goal is to reduce developer noise, we introduce\n“filtered-success@k”. This metric computes success (e.g., fail-to-pass\nor human acceptance) only on the subset of patches and bugs that\nare “allowed” by our policies, rather than over the entire population.\nOn our human-reported bug set, the baseline fail-to-pass@1\nrate is 0.11. Applying bug abstention or patch validation alone\nraises this rate to 0.21 or 0.29, respectively. When combined, these\npolicies form a configurable funnel that significantly improves the\nquality of patches shown to developers. A moderately permissive\nsetting raises the filtered fail-to-pass@1 to 0.35. A stricter funnel\nconfiguration raises this rate to 0.53 — a 1-in-2 success rate — at\n\n\nGoogle et al.\n\n\n**Figure 1: Bug abstention rejects issues that are unlikely to be fixed by our APR system. After the agent generates patches,**\n**validation decides whether these are likely to have addressed the issue. Jointly they reduce developer noise from APR.**\n\n\n\nthe cost of showing patches for fewer bugs. On machine-reported\nbug sets, patch validation alone also shows significant gains: for\nNPE bugs, it raises the filtered-accept@1 from 0.38 to 0.62, and for\nsanitizer bugs, it improves the rate by up to 15 percentage points\nover a baseline using known reproduction tests.\nTo summarize, our contributions in this work are:\n\n\n  - A bug abstention policy to select bugs suitable for agentic\nAPR.\n\n  - A multi-stage patch validation policy to assess candidate\npatch correctness.\n\n   - An extensive evaluation on three industrial bug sets.\n\n\n**2** **Background: ReAct-style Agentic APR**\n\n\nOur agentic APR system consists of an LLM-driven ReAct-style loop\nwith coding tools designed to interact with an isolated Google environment [ 26 ]. At the start of the process, the agent is provided with\na bug report, which may contain varying degrees of information\nabout the underlying bug. The agent then proceeds for a sequence\nof up to _𝑁_ steps, where each step is defined as one LLM output and\nan associated tool execution. The tools are: code search, viewing a\nfile, editing a file, running tests, and finishing the repair trajectory.\nThe LLM output consists of two portions: a thought, describing\nthe agent’s intentions in natural language, and an action, a simple\nUnix-style function calling format which we parse and then use\nto execute the associated tool [∗] . The LLM output and environment\nresponses are added to an append-only history. For more details on\nthe underlying agent, we refer the interested reader to [27].\n\n\n**3** **Abstention and Validation Policies**\n\n\nWe introduce two key components into an agentic APR system\nto reduce developer noise. In Figure 1, we augment the core APR\nAgent module (in blue) with a component before the repair loop\n(Bug Abstention) and a component after the repair loop (Patch\nValidation). The bug abstention policy filters out bugs that the APR\nagent is unlikely to fix, while the patch validation policy filters out\npatches that are unlikely to be a correct fix for a given bug. We now\ndescribe these policies in more detail.\n\n\n**3.1** **Bug Abstention Policy**\n\nAbstention is a well-known technique in classification [ 8 ], and\namounts to allowing a _𝐾_ -label classifier to produce an extra label\n\n\n∗ One of our experiments relies on Gemini function calling instead of custom\nparsing.\n\n\n\ncorresponding to “unknown” (i.e. abstaining from making a decision). Analogously, in the repair setting, we say a system abstains\nfrom a bug if it decides that it will not attempt repair.\nImportantly, the abstention policy can only access the bug, which\ncomprises textual bug title, description, and accompanying metadata, to make its decision; it does not receive any additional human\nfeedback. We also choose not to provide codebase access to the\nabstention policy, which may otherwise require making the policy\nagentic and losing some efficiency gains.\nWe choose to model this problem in a probabilistic fashion to\nproduce a score, rather than a boolean decision. Intuitively, we\nwant this score to approximate the inherent probability of a single\nrepair attempt succeeding. We then convert this into a decision by\ncomparing it to a configurable threshold, _𝜏_ .\n\n\n_3.1.1_ _Formalization._ Let _𝑏_ ∈B be a bug in a set of bugs that has\nbeen provided to our repair system as they require code changes for\ntheir resolution. We assume that knowing that these bugs require\nexplicit code changes is already determined in a pre-filtering phase\nor because a user has informed the system. Let there be _𝑛_ independent Bernoulli trials reflecting _𝑛_ independent repair attempts by\nthe repair agent on _𝑏_ . The outcome of the _𝑖_ -th trial for bug _𝑏_, _𝑋_ _𝑏,𝑖_, is\n1 if the agent generates a successful patch in trial _𝑖_ and 0 otherwise.\nAccordingly, we define the **empirical probability of the agent’s**\n**success on bug** _𝑏_ as ˆ _𝜋_ _𝑏_ = _𝑛_ [1] � _𝑛𝑖_ =1 _[𝑋]_ _[𝑏,𝑖]_ [, or the expected value of a]\n\nrepair trial.\nThis empirical value ˆ _𝜋_ _𝑏_ is an observation-based estimate of the\n**unknown, true probability of success for bug** _𝑏_, with respect\nto a particular repair system, which we denote as _𝜋_ _𝑏_ = _𝑃_ ( _𝑋_ _𝑏,𝑖_ = 1 ) .\nEstimating ˆ _𝜋_ _𝑏_ requires a success criterion, such as a test oracle or\nhuman reviewer. During deployment, however, such criterion is\neither unavailable (no test), or renders the estimation unnecessary\n(manual review on all patches). Thus, for a given bug _𝑏_, we leverage\nan abstention model _𝑃_ _𝑎_ to produce _𝑃_ _𝑎_ ( _𝑏_ ), which then is used to\nestimate ˆ _𝜋_ _𝑏_, _a priori_ . We aim to turn _𝑃_ _𝑎_ ( _𝑏_ ) into a decision policy\nthat allows us to filter down the original bug set B to a new set B [′],\nsuch that B [′] ⊆B and E _𝑏_ ′ ∈B ′ [ _𝜋_ _𝑏_ ′ ] _>_ E _𝑏_ ∈B [ _𝜋_ _𝑏_ ], demonstrating a\nreduced probability of wasted repair attempts.\nExplicitly, the abstention model’s prediction can be turned into\na decision policy through a threshold. Let this be expressed as a\nfunction, _𝐷_ _𝑎_, which maps the model’s predicted success likelihood\nto an action from the set {AttemptRepair _,_ Abstain} . Given a predefined threshold, _𝜏_ ∈[0 _,_ 1], the decision rule is formalized as:\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\n\n_𝐷_ _𝑎_ ( _𝑃_ _𝑎_ ( _𝑏_ ) _,𝜏_ ) =\n\n\n\nAttemptRepair if _𝑃_ _𝑎_ ( _𝑏_ ) ≥ _𝜏_\n�Abstain if _𝑃_ _𝑎_ ( _𝑏_ ) _< 𝜏_\n\n\n\nOur abstention policy queries an LLM to produce a prediction of\nthe (unobserved) probability of an attempt successfully addressing\na given bug. We use an instruction-style prompt and instruct the\nmodel to produce one of two possible outcomes, success (attempt\nrepair) or failure (abstain), given a prompt with some representation _𝑟_ ( _𝑏_ ) of the given bug _𝑏_ . We define _𝑃_ _𝑎_ (outcome) as the model\npredicted probability of that token. Since success and failure are\nthe only two possible outputs based on the instructions, the LLM’s\ndecoding should place relatively little weight on other tokens and\nso _𝑃_ _𝑎_ (success| _𝑟_ ( _𝑏_ )) + _𝑃_ _𝑎_ (failure| _𝑟_ ( _𝑏_ )) ≈ 1. We confirmed this\nproperty on a held-out set of bugs during our development.\n\n\n_3.1.2_ _Bug representations._ As is well known, the prompt used to\nquery an LLM can have a substantial impact on its performance [ 19,\n31]. We describe two possible bug representations for our prompt.\n\n\n  - _𝑟_ 1 ( _𝑏_ ) **Instruction Prompting with Title and Descrip-**\n**tion.** The LLM is provided with the title and description\nof from the bug report, along with the capabilities of the\nagent used in our experiments. Specifically, we provide information on the commands available to the agent, as well\nas the execution budget for the expected agent runs. The\nassociated prompt (shown in the Appendix as Listing 1) instructs the model to analyze the bug’s title and description\nand output only success or failure.\n\n  - _𝑟_ 2 ( _𝑏_ ) **Instruction Prompting Adding Guidelines.** This\nrepresentation builds on top of _𝑟_ 1 ( _𝑏_ ) by incorporating guidelines that can be used to determine the likelihood of success\nful bug resolution. We derived these guidelines —detailed\nin Listing 2 (Appendix)— through a manual process on a\nheld-out set of 50 human-filed bugs. Four authors inspected\nthe bug titles and descriptions, determined subjective difficulty levels, and provided short guideline that would help\nidentifying bugs that could be resolved by reasonable attempts from an agent. One author collated the inspection\noutcomes into 5 basic guideline categories, such as “Clear\nProblem & Action” and “Precise Code Localization”. These\n\nguidelines are added to the prompt.\n\n\n**3.2** **Patch Validation Policy**\n\n\nPatch validation performs an analogous accept/reject decision at\nthe patch-level. Specifically, it predicts whether a **particular** repair\ntrajectory produced a successful patch for a specific bug.\n\n\n_3.2.1_ _Formalization._ Let _𝑡_ be a particular trajectory from the repair\nagent when attempting to repair bug _𝑏_ . Given some representation of the trajectory, _𝑟_ ( _𝑡_ ), the policy predicts a triplet _𝑃_ _𝑣_ ( _𝑏,𝑟_ ( _𝑡_ )) ∈\n{false _,_ true}×Σ [∗] ×R, where the first element is a binary judgment\nfor patch correctness, the second element is a natural language explanation of the judgment, and the third element is an optional\nconfidence score for the patch’s correctness. We then define a decision function over this triplet, which can use the binary judgment\nor the patch correctness confidence score to filter out patches and\nallow for the intelligent allocation of human effort, filtering out\nless likely patches as noise and reducing review time.\n\n\n\nIn our implementation, we compute _𝑃_ _𝑣_ using a multi-stage approach. We first perform build and test regression checks to remove\npatches that have known issues. Specifically, we use Google’s unified build system to extract targets that have any modified files as\na direct dependency. Alternatively, if the bug we are tackling has a\nknown reproduction test, we replace this regression check with a\nfilter that rejects any patches that fail to pass the reproduction test.\nNext, our reviewer applies a simple heuristic which removes\ntrajectories that have a final test command issued by the agent\nthat results in a failure. Note that this may capture different test\ntargets than the build/test check as the agent is not restricted to\nexecuting only direct dependencies. Finally, our validation employs\na two-staged LLM prediction, which we now describe in detail.\n\n\n_3.2.2_ _Fix specification._ Past literature in the area of LLMs-as-judges\nhas explored the use of generated rubrics to perform more accurate\nscoring of an answer [ 32 ]. However, this rubric is often based on\nadditional information such as the reference answer [ 37 ], which is\nnaturally unavailable in our setting, where we are attempting to\nrepair a program without knowing the fix ahead of time.\nHowever, we do take inspiration from this line of research, and\nexplore the LLM’s ability to generate a set of criteria for determining if an agent attempt has successfully addressed a bug _without_\naccess to any ground truth—we refer to this as a “fix specification.” We simply provide this specification as part of the LLM judge\nprompt to produce a score for the patch. To generate the fix specification, we prompt an LLM with the bug information and the _original_\nsource code contents of the files that were edited in the agent’s\ncandidate patch. Importantly, this connects the fix specification and\nthe agent’s attempt by re-using the localization information from\nthe agent’s trajectory to generate the fix specification. Note that\nthis agent trajectory _does not_ contain any information from the\nground-truth fix—that is, the specification can only be derived from\ninformation known at the start of the repair attempt. We do not\nimpose any format or structure on the generated specification and\ninstead allow the LLM to freely generate this content. In practice,\nthe model often generates clear Markdown with a diagnosis and\nlist of expected properties of a correct change.\n\n\n_3.2.3_ _Reviewing with a specification._ Once the LLM has generated\na fix specification, a second LLM call judges the agent’s attempt.\nThis prompt includes the bug information, the unified-diff-format\npatch produced from the agent’s trajectory, test result information\n(extracted from the trajectory) for any test targets run by the agent,\nand the generated fix specification.\nA test result consists of a numbered test attempt for a particular\ntest suite target, and the associated environment response (including logs for a failure). If no test result information is found, we\nsimply state “No testing information is available”.\n\n\n_3.2.4_ _Reviewing without a specification._ Besides our specificationbased validation, we experiment with a variant of validation that\nconsumes the bug description and the concatenation of all prompt\nmessages and responses that were produced during the agent’s\nexecution. Recent work [ 25 ] has used this representation to judge\nan agent’s attempts to solve SWE-Bench tasks.\n\n\n_3.2.5_ _Patch validation outputs._ As described previously, the output\nof the validation policy is a triplet consisting of a judgment, a\n\n\nnatural-language explanation, and a real-valued confidence score.\nTo enforce this format, we employ constrained decoding [10, 29].\nWe define the patch correctness confidence score for the response\nheuristically. If validation returns a negative binary judgment, this\nscore is set to zero. If the validation returns a positive binary judgment, we set this score to the exponential of the average token log\nprobabilities. While not a well-defined probability, our evaluation\nshows this score can improve performance over binary labels.\n\n\n_3.2.6_ _Decisions over patch validation outputs._ We can make decisions based on the binary judgment or the patch correctness\nconfidence score extracted from the validation outputs. We consider two variants (1) **binary**, where we use the binary judgment\nand accept patches with a positive label; (2) **percentile of patch**\n**correctness confidence score**, where we estimate a particular\npercentile cutoff across all patches for all bugs in the given set and\nthen accept patches with a score at or above that cutoff. While percentile thresholding needs a set of patches to perform a judgment,\nit mitigates the risks of score drift, e.g. due to population or model\nchanges, and closely aligns with the idea of budget-based reviewing\n(i.e., a human reviews the top ≤ _𝑘_ patches).\n\n\n**4** **Composing Abstention and Validation**\n\n\nWhile abstention and validation policies can be used independently,\nthese two policies can also be combined. We present and evaluate\na simple sequential combination of the two policies.\nLet _𝑅_ represent our repair system, which, given a _𝑏_ bug, generates multiple trajectories independently. Each trajectory _𝑡_ has a\n_._ patch property representing all changes made by the agent. Let\n_𝑃_ _𝑎_ ( _𝑏_ ) be the probability predicted by the abstention policy for\nsampling a successful fix for bug _𝑏_ . Let _𝑃_ _𝑣_ ( _𝑏,𝑡_ ) be the patch validation policy output. Let _𝜏_ _𝑎_ be the abstention threshold and _𝐷_ _𝑣_ be\nthe decision function over the validation’s output. Then the set of\npatches ultimately shown for human review is: { _𝑡._ patch| _𝑃_ _𝑎_ ( _𝑏_ ) ≥\n_𝜏_ _𝑎_ _,𝑡_ ∈ _𝑅_ ( _𝑏_ ) _, 𝐷_ _𝑣_ ( _𝑃_ _𝑣_ ( _𝑏,𝑡_ ))} . We leave alternate compositions (e.g.,\nprobability-based) or further optimizations (e.g., accounting for\nbug/review cost and fix value) for future work.\n\n\n**5** **Evaluation**\n\n\nWe now describe our datasets, metrics, and research questions.\n\n\n**5.1** **Datasets**\n\n\nWe use three industrial bug datasets (Table 1) for evaluation. Bug\nabstention only applies to human-reported bugs. Machine-reported\nbugs (i.e., NPE and sanitizer bugs) are all good candidates for APR\nby construction, as they contain reproduction tests or stack traces.\n\n\n_5.1.1_ _Human-reported bugs:_ From Google’s monorepo, we collected a set of 174 human-reported bugs filed within a year by\nfollowing a curation process similar to that for popular benchmarks\nlike SWE-Bench and recent agentic APR work in industry [ 20, 27 ].\nSpecifically, we adhere to the curation process detailed in [ 27 ] and\nadditionally restrict examples to those with at most 31 test targets\nthat depend on the associated ground truth files. Bug curation steps\ndetailed in Appendix A.\n\n\n\nGoogle et al.\n\n\nWe then generated 20 trajectories with a 25-step limit for each\nof the 174 bugs using our agentic APR system (Section 2) using\nGemini 2.5 Pro [7] with temperature 0.2 and top_p 0.95.\nWe consider the success criterion to be whether the held-out fail\nto-pass test succeeds. We have found this criterion closely matches\ncorrectness in human-reported bugs from our codebase [27].\n\n\n_5.1.2_ _Null pointer exception (NPE) bugs._ We collected a set of 198\nmachine-reported NPE bugs that our agent attempted to repair\nduring a live deployment. Every bug report includes a stack trace of\nthe exception. The report may also include a section ( message blob )\nthat provides additional machine-generated debugging information.\nDuring deployment, our system used patch validation with fix\nspecification to filter patches, normalize patches, and choose a\nsingle patch per bug based on majority voting. The normalization\nstep includes normalizing identifier strings (mapping each unique\nlikely identifier string to a unique index) and post-patch least-linecount tie-breakers. A patch was then sent for human review, and\nsubmitted to the codebase if a code owner accepted it as fix.\nDuring live deployment, our APR system setup differed in the\nfollowing: function-calling is through Gemini instead of custom\nReAct parsing, asynchronous step execution to allow long-running\ncalls without blocking concurrent repairs nor overloading services,\nat most 5 trajectories were generated with 25-step limit per bug.\n\n\n_5.1.3_ _Sanitizer bugs._ We collected a set of 50 sanitizer-reported\nbugs with a known ground-truth fix following the curation procedure described in our past work [ 27 ]. These bugs cover issues\nsuch as memory errors, data races, datatype overflows, typecasting\nerrors, and fuzzer-detected errors. We use the same APR agent\nconfiguration as for our human-reported bug set.\n\n\n**5.2** **Metrics**\n\n\nWe report precision and recall for both policies. For repair performance, we focus on a standard pass@k metric, and a variant\nwhere bugs and patches are filtered by the corresponding policies.\nLet B [′] be the set of bugs with at least one repair run/trajectory\navailable after bug abstention has filtered bugs and patch validation\nhas filtered patches, _𝑛_ [′] be the total number of runs remaining after\napplying patch validation, and _𝑐_ [′] be the successful count among\nthose. Then the filtered-success@k metric is defined as:\n\n\n\nThe success criterion differs by bug types. For human-reported\nbugs, we use the held-out ground-truth test to compute fail-topass and report filtered-fail-to-pass@k. For NPE bugs, which were\nopen at the time of collection and may not have any tests, we use\nhuman acceptance annotations and report filtered-accept@k. The\nannotations are from real code owners, or authors if the patch had\nnot been reviewed by owners during live deployment, e.g., a patch\nnot picked by majority voting (§5.1.2). For sanitizer bugs, which\nhave known ground-truth fixes and a reproduction test at reporting\ntime for filtering patches, we manually compare patches to groundtruth fixes for semantic equivalence, and report filtered-accept@k.\nFor both bug policies, we also include comparisons to a no-skill\npolicy which always accepts bugs (or patches, respectively). Under\n\n\n\nfiltered-success@ _𝑘_ = E B [′] [1 −\n\n\n\n� _𝑛_ ′ _𝑘_ − _𝑐_ ′ �\n\n~~�~~ _𝑛𝑘_ ′ ~~�~~ []]\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\n**Set** **Description** **#** **Policies** **Success Cri-**\n**teria**\n\n\n\n\n\n\n\nHuman Human\nreported bugs\nwith known\nfix.\n\n\nNPE Open bugs\nfrom a live\n\ndeployment\nof a repair\nsystem.\n\n\nSanitizer Sanitizer\nreported bugs\nwith known\nfix.\n\n\n\n174 Abstention Passes held\n+Validation out reproduc\ntion test.\n\n\n198 Validation Confirmed\ncorrect by\ncode owner\n\nor manual\n\ninspection.\n\n\n50 Validation Confirmed\ncorrect by\nmanual in\nspection.\n\n\n|1.2<br>1.0<br>0.8<br>Density<br>0.6<br>0.4<br>0.2<br>0.0|Distribution of E|mpirical vs. Model|Probabilities<br>Source<br>R1 - Abstention<br>R2 - Abstention w/ Guidelines<br>Empirical Success Rate|Col5|Col6|\n|---|---|---|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br>Density|||R1~~-~~ Absten<br>R2~~-~~ Absten<br>~~Empirical S~~|tion<br>tion w/ G<br>~~ccess R~~|uidelines<br>~~te~~|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br>Density||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br>Density||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br>Density||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br>Density||||||\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br>Density||||||\n|0%<br>20%<br>40%<br>|0%<br>20%<br>40%<br>|60%<br>80%<br>100%<br>Probability|60%<br>80%<br>100%<br>Probability|60%<br>80%<br>100%<br>Probability|60%<br>80%<br>100%<br>Probability|\n\n\n\n**Table 1: Benchmarks, policies applied, and success criteria.**\n\n\nthis no-skill policy, the precision of predictions corresponds to the\nprevalence successful cases in the population.\n\n\n**5.3** **Research Questions**\n\n\nWe aim to answer the following four research questions:\n\n\n**RQ1** : Can bug abstention successfully abstain on our human-reported\nbugs? Does this improve filtered-fail-to-pass@k?\n\n\n**RQ2** : Can patch validation successfully predict if a patch should be\nrejected on our human-reported bugs? Does this improve filteredfail-to-pass@k?\n\n\n**RQ3** : Does combining bug abstention and patch validation further\nimprove filtered-fail-to-pass@k on our human-reported bugs?\n\n**RQ4** : How does patch validation improve filtered-accept@k on bugs\nwith systematically-generated reports (NPE and sanitizer bugs) and\nknown reproduction tests (sanitizer bugs)?\n\n\n**6** **Results**\n\n\nWe now present our experimental results. At the time of these\nexperiments, our abstention policy uses Gemini 2.0 Flash (newer\nmodels are now available) patch validation policy uses Gemini 2.5\nPro, and both perform greedy decoding (temperature=0). We expect\nto use a smaller model for abstention for efficiency as it runs on a\nlarger number of inputs than patch validation.\n\n\n**6.1** **RQ1: Bug Abstention Policy**\n\n\nWe evaluate bug abstention on the two bug representation variants:\nwith and without the use of general guidelines (Section 3.1.2).\nFigure 2 shows a comparison of model-predicted probabilities\nand the empirical success rate (Section 3.1.1) of repair runs for the\n174 human-reported bugs. The empirical success rate (green) is the\nfraction of trajectories that have a passing outcome on the held-out\nreproduction test, which indicates the inherent difficulty of APR. As\nshown, it is heavily skewed, with the vast majority of bugs having\na very low true success rate, concentrated in the 0–15% range. The\nskewness confirms that an agentic APR system can benefit from an\nabstention mechanism to avoid wasting human review effort.\n\n\n\n**Figure 2: Using abstention with guidelines better aligns**\n**model probabilities with observed (left-skewed) rates.**\n\n\nAbstention with representation _𝑟_ 1 (i.e., without guidelines) produces a more uniform distribution of repair success probability. In\ncontrast, abstention with _𝑟_ 2 (i.e., with guidelines) produces a distribution that much more aligns to that of the empirical success rate.\nWe further measured the Wasserstein distance [ 1 ] between each\npredicted distribution and the empirical success rate distribution:\nthe distance from _𝑟_ 2 (0.126) was nearly three times smaller than\nthat from _𝑟_ 1 (0.363), indicating that _𝑟_ 2 is a much better overall fit for\nbug abstention. Like the empirical data, abstention predictions with\n_𝑟_ 2 are correctly concentrated at lower probability values, while still\nassigning a higher repair success probability to easier bugs.\nFigure 3 presents precision-recall curves for both abstention policy variants, a horizontal line for the prevalence of a success, and a\nsummary average precision (AP) score. For purposes of this classification, we define success as any bug that has at least one repair\ntrajectory that passes the held-out reproduction test among the 20\ntrajectories of the bug. The prevalence of success corresponds to\nthe precision that could be expected from a no-skill policy, which\njust accepts all bugs, and all patches for these bugs. For our humanreported bug set, this prevalence corresponds to 27% of all bugs\nhaving at least one successful repair trajectory without any abstention filtering. We observe that both abstention policy variants\ncan improve over prevalence; adding guidelines further provides\nimprovements across a range of recall values.\nTo turn the abstention predictions into a decision policy, we\nselect _𝜏_ = 0 _._ 5 as a sensible default for the abstention policy threshold\nbased on a separate development bug set. Using this threshold\nproduces a precision/recall of 0.44/0.41 in that set. This threshold\nis also intuitive, as it prioritizes bugs with a predicted success\nprobability greater than failure. More importantly, it achieves a\nstrong balance between precision and recall, effectively filtering\nout a large number of bugs unlikely to be fixed while retaining a\nsignificant portion of the fixable ones.\nFigure 4 shows the pass@k (when abstension is not applied)\nand the corresponding filtered-fail-to-pass@k (when abstention\nis applied) of our agentic APR system on the set of 174 humanreported bugs. We use _𝜏_ = 0 _._ 5 when applying abstention. Applying\nabstention improves the system’s efficacy, and the _𝑟_ 2 abstention\nvariant demonstrates the most significant gain. The baseline (no\n\n\nGoogle et al.\n\n\n\n\n\n\n\n\n\n|1.0 No-Skill Classifier (Prevalence = 0.27)<br>R1 - Abstention (AP = 0.389)<br>R2 - Abstention w/ Guidelines (AP = 0.43<br>0.8<br>0.6<br>Precision<br>0.4<br>0.2<br>0.0<br>0.0 0.2 0.4 0.6 0.8|No-Skill Classifier (Prevalence = 0.27)<br>R1 - Abstention (AP = 0.389)<br>R2 - Abstention w/ Guidelines (AP = 0.43|4)<br>1.0|\n|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br><br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>No~~-~~Skill Classifier (Prevalence = 0.27)<br>R1~~-~~ Abstention (AP = 0.389)<br>R2~~-~~ Abstention w/ Guidelines (AP = 0.43|||\n\n\n**Figure 3: Both abstention variants outperform randomly sam-**\n**pling to identify bugs with at least one successful repair.**\n**Adding guidelines helps. (AP=average precision)**\n\n\n\n\n\n\n\n\n\n**Figure 4: Abstention can raise fail-to-pass@k by removing**\n**bugs that are less likely to succeed with our APR system.**\n\n\nabstention) achieves pass@1 of 11.29%. By filtering bugs with the\n_𝑟_ 1 abstention policy, the filtered-fail-to-pass@1 rate increases to\n16.81%. The _𝑟_ 2 abstention achieves an even higher filtered-fail-topass@1 of 21.05%. This trend of improvement continues as more\nrepair attempts are made: when _𝑘_ = 20, (filtered-fail-to-)pass@k\nclimbs from 27.01% (no abstention) to 44.74% ( _𝑟_ 2 abstention). If we\nconsider having at least one passing trajectory in 20 as a groundtruth positive label for a bug and an abstention probability above\nour threshold as a predicted positive label, our abstention policy\nresults in a 16.5% false positive rate, defined as a ratio of false\npositives to ground-truth negatives. These results strongly suggest\nthat abstention effectively discards bugs with a low fix probability,\nallowing the APR system to concentrate on more-promising bugs.\n\n\n**6.2** **RQ2: Patch Validation Policy**\n\n\nWe compare three patch validation policy variants: (1) _no-skill policy_,\ni.e., prevalence, which reports the fraction of successful trajectories\nafter filtering patches that fail build and test regression checks, (2)\n\n\n\n**Figure 5: Patch correctness score allows more aggressive val-**\n**idation, with precision up to 3** × **the success prevalence.**\n\n\n_validation_ that uses fix specification (Section 3.2.3), and (3) _no-spec-_\n_validation_ that does not use fix specification and reviews the agent’s\nwhole trajectory (Section 3.2.4).\nTo determine the build and test regression checks, we extract\nfrom Google’s unified build system any targets that have a modified\nfile as a direct dependency. We retrieve this direct dependency by\nidentifying the build system rules that include the modified file in\ntheir definitions. Given Google’s monorepo, transitive dependencies\ncan quickly grow, and so we use this restricted set of rules as a\npractical approximation to identifying and removing regressions.\nOur experiments show that while using fix specification improves both precision and recall over the ablation, the absolute\nprecision remains modest at 0.3. The no-spec validation’s binary\njudgment is heavily biased towards positive judgment, providing\nlow recall of failing patches and prevalence-level precision for successful patches. The no-spec validation’s bias towards positive judgment results in a high false positive rate (ratio of false positives to\nground-truth negatives) of 0.64 compared to 0.28 for the variant\nthat uses the fix specification.\nWe show that despite the modest predictive power of binary\njudgment, the validation’s patch correctness score can be used to\ndistinguish between predictions. As a reminder, we define this score\nas zero for negative binary judgments and as the exponential of\nthe average log probabilities of the response tokens for positive\nbinary judgments. Figure 5 shows precision-recall curves for both\nthe validation and the no-spec validation, as well as a horizontal line\nfor the no-skill policy after regression checks. At stricter thresholds,\nour spec-based validation (in blue) can yield a precision up to three\ntimes higher (at the cost of recall) than that of the prevalence rate,\noffering a substantial noise reduction for a subset of bugs. we find\nthat the no-spec validation can improve over the no-skill policy\nbut achieves lower precision at the same recall than the spec-based\nvalidation. With the 75th and 90th percentile cutoffs, the specbased validation achieves a false positive rate (FPR) of 0.13 and 0.04,\nrespectively, while the no-spec validation achieves an FPR of 0.15\nand 0.06, respectively.\nWe now study the impact of different decision procedures for\npatch validation with filtered-fail-to-pass@k. We evaluate two decision procedures described in Section 3.2.6 for validation. Specifically, one procedure uses binary judgment: validation-binary and\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\nno-spec-validation-binary . The other uses the 75th and 90th percentile thresholds of all patches’ correctness scores: validation\n-p75 and validation-p90 (and similarly for no-spec-validation ).\nFigure 6 compares the filtered fail-to-pass@k of these patch validation variants against build-and-test, which is a baseline that\nsamples patches that pass build and test regression checks. The\nfiltered fail-to-pass@k of build-and-test are 0.14 and 0.19 when\n_𝑘_ equals to 1 and 3, respectively. On no-spec validation, using binary judgment provides no improvement. However, using the 75th\nand 90th percentile thresholds of patch correctness scores, no-spec\nvalidation raises filtered fail-to-pass@1 to 0.16 and 0.22, respectively. Similarly, on spec-based validation, using 75th and 90th\npercentile thresholds of patch correctness scores better improve filtered fail-to-pass@1 than using binary judgment. Binary judgment\nonly modestly improves filtered fail-to-pass@1 (0.16), while the\n75th and 90th percentile thresholds improve filtered fail-to-pass@1\nmore substantially to 0.21 (+7 percentage points, +50% relative\nimprovement) and 0.29 (+15 percentage points, +100% relative improvement), respectively. Overall, using an aggressive validation\npolicy results in fewer bugs that have at least one patch that can be\nsurfaced. However, given that our goal is to reduce developer noise,\nwe prioritize increasing the likelihood of success for any patches\nthat remain.\nIn Figure 6, filtered-fail-to-pass@k improves further as a validation variant with more aggressive filtering is applied. While this\nimprovement trend matches our goal on reducing developer noise,\nit also presents a tradeoff occurring in patch validation between\nreducing developer noise (by discarding more agent trajectories)\nand increasing bug resolution rate.\nTo better understand this tradeoff, we compute “(pass & validation)@k”, where at least one patch must both pass the reproduction\ntest and satisfy the validation, and present the results in Figure 7.\nFigure 7 shows that: more aggressive filtering in validation reduces\noverall bug resolution rate. For example, (pass & validation)@1\ndecreases by 2, 4, and 7 percentage points for binary judgment,\n75th percentile threshold judgment, and 90th percentile threshold\njudgment, respectively. In summary, when employing a validation\npolicy, there is this tradeoff between wasted compute and developer\nattention to consider. The validation policy ultimately rejects many\ngenerated patches, which lowers overall bug resolution rates and\nleads to early flattening in Figure 6, but ensures the patches shown\nto developers are more likely to be plausible.\n\n\n**6.3** **RQ3: Combined Abstention-and-Validation**\n\n\nWe now evaluate a dual-policy framework that combines abstention and validation: it filters bugs via the abstention policy, then\nfilters patches via the validation policy (Section 4). For evaluation,\nwe set the abstention threshold to 0.5, rejecting any bugs with\na predicted value below that. We use spec-based validation, and\nconsider both binary and percentile-threshold judgments. Figure 8\ncompares (filtered-)fail-to-pass@k between the combined variants\n( combined-* ), a policy that only applies abstention and excludes\ntrajectories that produce no patches ( abstention ), and a baseline\nthat applies neither abstention and validation ( no-filter ).\nCombined abstention and binary validation can raise filtered failto-pass@1 to 0.29 over abstention alone (+5 percentage points) or\n\n\n\n**Figure 6: Validation can raise filtered-fail-to-pass@1 up to**\n**15 percentage points compared to using only build and test**\n**regression checks. We show the same plot zoomed in for easy**\n**comparison. Parentheses show number of bugs with at least**\n**one patch after validation.**\n\n\nvalidation alone (+13 percentage points). More aggressive filtering,\nsuch as 75th percentile can increase filtered-fail-to-pass@1 to 0.35\n(+11 percentage points relative to abstention alone and +14 to the\nsame validation alone). These percentile thresholds create a configurable funnel for developer review. A strict policy ( combined-p90 )\nnarrows the intake to just 12 bugs, but its output is high-confidence,\nwith a filtered fail-to-pass@1 of 0.53 (a 1-in-2 success rate). A more\npermissive policy ( combined-p75 ) widens the intake, doubling the\nnumber of bugs for inspection while still yielding a high success\nrate of roughly 1-in-3.\nWhile it is not surprising that using validation can improve on\nbug-abstention-only success rates, as validation has access to more\ninformation (e.g., the agent-generated patch) and is judging a single\ninstance, we also observe improvements over the validation-only\nversion. This is interesting because patch validation—just like the\nbug abstention policy—has access to the bug report. We hypothesize,\nthat given a patch, validation is more likely to be persuaded of the\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**Figure 7: More aggressive filtering tradeoffs between reduc-**\n**ing developer noise and increasing bug resolution rate.**\n\n\n\n\n\n\n\n**Figure 8: Abstention and validation are complementary,**\n**jointly they can raise filtered fail-to-pass@k.**\n\n\nsuccess of the attempt, while the abstention policy is more likely\nto reflect overall quality of bug report.\nFigure 9 visualizes the process of our dual-policy framework as a\nconfigurable funnel. The funnel presents the filtered fail-to-pass@1\nrate on human-reported bugs at each stage. Starting with the full\nset of 174 bugs, where our APR system has a baseline pass@1\nrate of 11.29%, the abstention policy first filters this down to 39\nmore promising bugs, increasing the filtered fail-to-pass@1 rate\nto 21.5%. The combined validation policies ( combined-* ) further\nrefine this set. The combined-binary policy leaves 33 bugs with a\nrate of 29%. More aggressive filtering with combined-p75 and the\nstrictest combined-p90 variant narrow the funnel to 25 and 12 bugs,\nrespectively, boosting the quality of patches to achieve high filtered\nfail-to-pass@1 rates of 35% and 53%. Overall, Figure 9 demonstrates\nthe trade-off between the volume of bugs attempted and the precision of the patches ultimately shown to developers.\n\n\n\nGoogle et al.\n\n\n**Figure 9: Abstention and Validation Funnel. Our combined**\n**policies filter an initial set of 174 bugs, progressively nar-**\n**rowing the set to increase the quality of patches shown to**\n**a developer. Each stage reduces the number of bugs but in-**\n**creases the filtered fail-to-pass@1 rate, from a baseline of**\n**11.29% to 53% for the most selective configuration.**\n\n\n**6.4** **RQ4: Validation on NPE and sanitizer bugs**\n\n\nWe further evaluate patch validation in bug categories that have\nmore systematic information in their bug reports, and so are assumed good candidates for APR by construction (i.e., these bugs\npass abstention by construction). We consider the machine-reported\nNPE and sanitizer bugs, whose bug reports always have a stack\ntrace and reproduction test, respectively.\nFigure 10 shows filtered-accept@k by manually inspecting the\ncorrectness of patches for NPE bugs, whose bug reports have stack\ntraces. The baseline ( build-and-test ) filters patches that did not\npass build and test regression checks. We observe that validation\nusing binary judgment provide little improvement over these regression checks. However, validation using the more-aggressive\npercentile-threshold judgment can result in improvements to filteredaccept@1 of +8 percentage points and +24 percentage points, with\n75th and 90th percentile thresholds, respectively. No-spec validation at 90th percentile produces a bigger filtered-accept@1 improvement than that at 75th percentile (+20 percentage points vs +24\npercentage points relative to regression checks), but both of them\nperform worse than the spec-based variant at 90th percentile (i.e.,\nvalidation-p90 ).\nFigure 11 shows mean filtered-accept@k (based on manual inspection of patch correctness) for sanitizer bugs, whose bug reports\nhave reproduction tests. As a result, we filter any patches first by\nthe reproduction test and then apply the validation policy. The\nbaseline ( repro-test ) filters patches that did not pass reproduction\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\n\nSanitizer # Bugs repro-test validation-binary validation-p75 validation-p90\n\n\ndata_race 17 3.11 2.93 2.43 3.04\nFuzz+property testing 3 NA NA NA NA\nheap_buffer_overflow 1 1.43 1.43 1.54 1.54\ninvalid_bool_load 2 1.49 1.65 1.25 NA\ninvalid_enum_load 1 NA NA NA NA\nleak_detected 2 1.52 1.72 4.29 3.81\nmisaligned_pointer_use 6 1.34 1.33 2.14 1.43\nnull_pointer_use 1 2.86 3.33 NA NA\nsigned_integer_overflow 2 1.2 1.27 1.56 2.5\nstack_use_after_scope 2 1.85 1.85 1.67 3.33\nuse_of_uninitialized_value 13 2.05 1.93 2.05 1.95\n\n**Table 2: Lift (filtered-accept@1 / original accept@1) varies**\n**for different sanitizer bug types. The higher the better.**\n\n\nMetric Abstain AttemptRepair\n\n\nAverage # code-like identifiers 3.03 5.28\nAverage character length 771.67 1239.26\nFraction with repository root 0.12 0.31\nFraction possible build rule 0.01 0.03\nFraction Markdown code ticks 0.17 0.20\n\n\n**Table 3: Abstention-accepted bugs more frequently have the**\n**presence of the monorepo root directory.**\n\n\nto reproduction test filtering slightly reduces lift, while adding the\n70th percentile validation keeps the same lift. For data races, adding\nvalidation actually reduces lift compared to just using reproduction\ntests. We hypothesize that: for these bugs, a patch that adds locking mechanism indeed resolves the data race and can pass patch\nvalidation, but still may not be an acceptable solution for a Google\ndeveloper if it introduces downsides such as larger critical sections.\nFor some categories, e.g. leaks and misaligned pointer usage, a less\naggressive validation (75th percentile) achieves better lift than a\nmore aggressive one (90th percentile).\nWe hypothesize that a true bug reproduction test, such as that\nincluded in the sanitizer’s bug report, results in a set of plausible\npatches that are increasingly difficult for validation to assess, compared to the patches produced for the less-constrained bugs such as\nthe human-reported ones. This points to a few future research directions. First, generating additional executable tests to help assess\npatch correctness is a particularly promising direction. For example, AutoCodeSherpa [15] generates property-based tests to assist\nreviewing and filtering agent-generated patches. As pointed out,\nafter filtering with the reproduction test, sampling _𝑘_ = 4 produces\na mean filtered accept rate of 0.74. Supporting developers when\ninspecting more patches may help maintain a high success rate.\n\n\n**7** **Discussion**\n\n\nThe abstention policy has access to the bug report when making a\njudgment. We compute basic properties on our human-reported bug\nset to characterize their influence on policy outcomes. As shown\nin Figure 12 and Table 3, bugs accepted by abstention tend to have\nlonger titles and descriptions (combined), and higher count of words\nthat are possible code identifiers (based on underscore or capitalization changes). Similarly, accepted bugs tend to have a slightlyhigher presence of Google’s source repository root directory and\npossible code Markdown, indicated by triple backticks.\n\n\n\n\n\n**Figure 10: More aggressive validation can increase filtered**\n**accept@k for NPE bugs.**\n\n\n\n\n\n**Figure 11: Aggressive validation increases filtered accept@1**\n**for sanitizer bugs. But just reviewing more patches can lead**\n**to higher filtered-accept@k, with 4 patches sufficing.**\n\n\ntests. For sanitizer bugs, validation can lift filtered-accept@1 but\nincreasing _𝑘_ quickly outstrips the validation policy. At _𝑘_ = 4, sampling patches from the set that passes the reproduction test provides\nbetter results (0.74) than using the validation’s filtering. When we\nconsider _𝑘_ = 1 and using 90th percentile threshold, no-spec variant\nno-spec-validation-p90 (0.65) outperforms the spec-based variant\nvalidation-p90 (0.62) and baseline repro-test (0.5).\nWe inspected whether patch validation performs differently\nbased on the sanitizer bug type. Table 2 shows the lift for each\nsanitizer bug category, defined as the ratio of filtered-accept@1 to\nthe raw accept@1, where per-bug accept@1 is averaged by sanitizer type. If a policy removes all patches associated with a sanitizer\nbug type or there are no patches (in the unfiltered set) that are\ncorrect, we enter NA in the table. We find that patch validation\nperformance indeed varies by bug type. For the two main categories\nof bugs in our evaluation set—data races and use of uninitialized values—validation does not provide a meaningful improvement over\njust filtering with reproduction tests. For use of uninitialized value\nerrors, adding the binary validation and 90th percentile validation\n\n\nGoogle et al.\n\n\n\n\n\n|0.8 tributi|Col2|Col3|Col4|\n|---|---|---|---|\n|0<br>2<br><br>0.0<br>0.2<br>0.4<br>0.6<br>||Abstention policy accepts<br>False<br>True|Abstention policy accepts<br>False<br>True|\n|0<br>2<br><br>0.0<br>0.2<br>0.4<br>0.6<br>||Abstention policy accepts<br>False<br>True|False<br>True|\n|0<br>2<br><br>0.0<br>0.2<br>0.4<br>0.6<br>||000<br>4000<br>6000<br>8000<br>10000<br>1200<br>haracter length of title and description|000<br>4000<br>6000<br>8000<br>10000<br>1200<br>haracter length of title and description|\n\n\n**(a) Description length**\n\n\n\n|0.8 tributi|Col2|\n|---|---|\n|0<br>10<br>Number of code~~-~~like i<br>0.0<br>0.2<br>0.4<br>0.6<br>|Abstention policy accep<br>False<br>True|\n|0<br>10<br>Number of code~~-~~like i<br>0.0<br>0.2<br>0.4<br>0.6<br>|20<br>30<br>40<br>dentifiers in title and descrip|\n\n\n**(b) Code-like identifiers**\n\n\n\n\n\n\n\n**Figure 12: Bugs accepted by the abstention policy tend to**\n**have slightly longer title and descriptions and more words**\n**that could be possible code identifiers.**\n\n\nSimilarly, we considered the size of the patches accepted by the\n75th percentile-based patch validation for our human-reported bug\nset. We only consider patches that underwent LLM policy judgment.\nWe count the number of line-level operations (addition/deletions)\nper patch. We find that validation-accepted patches had on average\n18.38 operations, compared to 22.69 for rejected patches.\nWe now look at an example of a patch that validation (at 90th\npercentile) incorrectly accepted. Figure 13 shows the associated\nreport indicating cases for a change in number formatting (when\n_“points are chosen to be burnt”_ ). The report leaves unspecified what\nshould happen in other cases, but a human would likely take this\nto mean that other cases should still render the decimal point.\nThe ground-truth patch makes decimal rendering a function\nargument and modifies necessary call sites (not shown for brevity).\nIn contrast, the agent patch modifies the constant true to false,\nwhich will impact all call sites. The generated specification identifies\nthe correct method to change, but does not scope to particular call\nsites. The validation policy then incorrectly accepts.\n\n\n**8** **Threats to Validity**\n\n\nHuman and sanitizer-reported bugs were drawn from a set that\nhas a known ground-truth patch, and some of our filtering relies\non ground-truth patch properties (e.g., existence of a test). These\nconstraints allow us to focus on patches within scope for an agent\nand employ ground-truth tests as an evaluation oracle. For sanitizerreported bugs, where reproduction tests are already available at the\ntime of reporting, having the ground-truth patch allows us to perform a manual comparison to determine correctness. This curation\napproach closely mirrors that used to create popular benchmarks\nlike SWE-Bench, as well as industry practices [20, 27].\nManual correctness assessment may diverge from the decisions\nthat would have been made by real code owners for patches that\nthey did not inspect. To mitigate this risk, we employed multiple\nannotators, consulting others in cases of ambiguity.\nLike other large-scale industrial software engineering studies [ 2,\n12, 21 ], the bugs and code in this work are drawn from a single company (Google) and reflect internal practices. Projects or companies\nthat write substantially different bug reports, or adhere to different\ndevelopment practices, may observe different policy impacts.\n\n\n\n**9** **Related Work**\n\n\nWe used Passerine, our existing agentic APR system [ 27 ], to evaluate\nboth bug abstention and patch validation. SpecRover [ 28 ] produces\nfunction-level specifications to guide the repair agent. Similarly, our\nvalidation policy employs a specification generation step to guide\nthe patch judgment but this specification is at the file level, can be\nderived from multiple files, and is used only after the repair agent\nhas concluded. Pan et al [ 25 ] showed that a fine-tuned LLM can be\nused to pick the best patch per bug based on the agent’s trajectory.\nOur no-spec validation employs a similar concatenation of trajectory prompt messages. However, our LLM patch validation relies\non prompting only and is used to filter out patches rather than pick\na patch per bug. Our patch validation filters on build/test regression\nchecks, or a reproduction test if available. Similarly, Agentless [ 34 ]\nuses (generated) tests to filter out unlikely candidates.\nAbstention [ 5 ] has long been employed in classification to reduce\nmispredictions resulting from ambiguity or lack of information.\nPast work has explored this topic extensively, learning abstention\npolicies in boosting algorithms [ 8 ], fairness guarantees [ 30 ], and,\nmore recently, in the context of generative models [33].\nConcurrent to our work, BouncerBench [ 22 ] introduces the concept of “bouncers” to reduce the overconfidence of an agentic software engineer. Bouncers reject underspecified tasks and unlikely\npatches, closely matching our abstention and validation policies.\nThe benchmark BouncerBench opens up this challenge to the community and represents a complementary analysis to our own, which\nfocuses on evaluating these policies in an industrial context.\n\n\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\nMaddila et al. [ 20 ] concurrently introduced a patch critic in an\nindustrial agentic APR system. Our work shows that abstention\ncan reduce developer noise further and can be complementary\nto a validation policy. Our evaluation also covers additional bug\ncategories (with and without known reproduction tests).\nWorks in LLM-as-judge for code, such as CodeJudge [ 32 ] and\nCrupi et al. [ 9 ], are closely related to our patch validation policy.\nIn particular, reference-free judging is a close analogue to patch\nvalidation, but we focus exclusively on program repair and use\njudgments to filter patches to reduce noise (at the expense of recall).\n\n\n**10** **Conclusion**\n\n\nWe present a dual-policy framework to reduce developer noise\nfrom an agentic APR system. A bug abstention policy rejects bugs\nunlikely to be solved by the APR system, while a patch validation\npolicy rejects unlikely candidate fixes. We evaluate these policies\non three sets of bugs drawn from Google’s codebase and on patches\nproduced by our agentic APR system. On a set of human-reported\nbugs with known fixes, abstention and validation can raise success rates up to 13 percentage points and 15 percentage points,\nrespectively, and up to 39 percentage points in combination.\nIn practical terms, this improvement transforms the developer\nexperience: instead of expecting a successful patch roughly 1 in 10\ntimes, developers can now expect a correct patch in 1 in 2 cases\npresented for their review ( combined-p90 ). A more relaxed policy\nconfiguration ( combined-p75 ) doubles the number of patches for\ninspection while still yielding a high success rate of roughly 1 in 3.\nThe patch validation policy can also be used to improve success\nrates for null pointer exceptions and sanitizer-reported bugs.\n\n\n**References**\n\n\n[1] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative\nadversarial networks. In _International conference on machine learning_, pages\n214–223. PMLR, 2017.\n\n[2] Christian Bird, Nachiappan Nagappan, Brendan Murphy, Harald Gall, and\nPremkumar Devanbu. Don’t touch my code! examining the effects of ownership on software quality. In _Proceedings of the 19th ACM SIGSOFT symposium_\n_and the 13th European conference on Foundations of software engineering_, pages\n4–14, 2011.\n\n[3] Islem Bouzenia, Premkumar Devanbu, and Michael Pradel. RepairAgent: An\nautonomous, LLM-based agent for program repair. In _International Conference_\n_on Software Engineering (ICSE)_, 2025.\n\n[4] Adam Brown, Sarah D’Angelo, Ambar Murillo, Ciera Jaspan, and Collin Green.\nIdentifying the factors that influence trust in ai code completion. In _Proceedings_\n_of the 1st ACM International Conference on AI-Powered Software_, pages 1–9, 2024.\n\n[5] C. Chow. On optimum recognition error and reject tradeoff. _IEEE Transactions_\n_on Information Theory_, 16(1):41–46, 1970.\n\n[6] Chi-Keung Chow. An optimum character recognition system using decision\nfunctions. _IRE Transactions on Electronic Computers_, (4):247–254, 2009.\n\n[7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen\nSachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen,\net al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. _arXiv preprint_\n_arXiv:2507.06261_, 2025.\n\n[8] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention.\n_Advances in neural information processing systems_, 29, 2016.\n\n[9] Giuseppe Crupi, Rosalia Tufano, Alejandro Velasco, Antonio Mastropaolo, Denys\nPoshyvanyk, and Gabriele Bavota. On the effectiveness of llm-as-a-judge for\ncode generation and summarization. _IEEE Transactions on Software Engineering_,\n51(8):2329–2345, 2025.\n\n[10] Daniel Deutsch, Shyam Upadhyay, and Dan Roth. A general-purpose algorithm\nfor constrained sequential inference. In _Proceedings of the 23rd Conference on_\n_Computational Natural Language Learning (CoNLL)_, pages 482–492, 2019.\n\n[11] Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan\nXiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, et al. Trae agent: An\n\n\n\nllm-based agent for software engineering with test-time scaling. _arXiv preprint_\n_arXiv:2507.23370_, 2025.\n\n[12] Philip J Guo, Thomas Zimmermann, Nachiappan Nagappan, and Brendan Murphy. Characterizing and predicting which bugs get fixed: an empirical study of\nmicrosoft windows. In _Proceedings of the 32nd ACM/IEEE International Conference_\n_on Software Engineering-Volume 1_, pages 495–504, 2010.\n\n[13] Radu Herbei and Marten H Wegkamp. Classification with reject option. _The_\n_Canadian Journal of Statistics/La Revue Canadienne de Statistique_, pages 709–721,\n2006.\n\n[14] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press,\nand Karthik Narasimhan. Swe-bench: Can language models resolve real-world\ngithub issues? _arXiv preprint arXiv:2310.06770_, 2023.\n\n[15] Sungmin Kang, Haifeng Ruan, and Abhik Roychoudhury. Autocodesherpa:\nSymbolic explanations in ai coding agents. _arXiv preprint arXiv:2507.22414_, 2025.\n\n[16] Claire Le Goues, ThanhVu Nguyen, Stephanie Forrest, and Westley Weimer.\nGenprog: A generic method for automatic software repair. _Ieee transactions on_\n_software engineering_, 38(1):54–72, 2011.\n\n[17] Claire Le Goues, Michael Pradel, and Abhik Roychoudhury. Automated program\nrepair. _Communications of the ACM_, 62(12):56–65, 2019.\n\n[18] Fan Long and Martin Rinard. Staged program repair with condition synthesis. In\n_Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering_,\npages 166–178, 2015.\n\n[19] Sheng Lu, Hendrik Schuff, and Iryna Gurevych. How are prompts different in\nterms of sensitivity? In _Proceedings of the 2024 Conference of the North Ameri-_\n_can Chapter of the Association for Computational Linguistics: Human Language_\n_Technologies (Volume 1: Long Papers)_, pages 5833–5856, 2024.\n\n[20] Chandra Maddila, Adam Tait, Claire Chang, Daniel Cheng, Nauman Ahmad,\nVijayaraghavan Murali, Marshall Roch, Arnaud Avondet, Aaron Meltzer, Victor\nMontalvao, et al. Agentic program repair from test failures at scale: A neurosymbolic approach with static analysis and test execution feedback. _arXiv_\n_preprint arXiv:2507.18755_, 2025.\n\n[21] Alexandru Marginean, Johannes Bader, Satish Chandra, Mark Harman, Yue Jia,\nKe Mao, Alexander Mols, and Andrew Scott. Sapfix: Automated end-to-end repair\nat scale. In _2019 IEEE/ACM 41st International Conference on Software Engineering:_\n_Software Engineering in Practice (ICSE-SEIP)_, pages 269–278. IEEE, 2019.\n\n[22] Noble Saji Mathews and Meiyappan Nagappan. Is your automated software\nengineer trustworthy? _arXiv preprint arXiv:2506.17812_, 2025.\n\n[23] Hoang Duong Thien Nguyen, Dawei Qi, Abhik Roychoudhury, and Satish Chandra. Semfix: Program repair via semantic analysis. In _2013 35th International_\n_Conference on Software Engineering (ICSE)_, pages 772–781. IEEE, 2013.\n\n[24] Yannic Noller, Ridwan Shariffdeen, Xiang Gao, and Abhik Roychoudhury. Trust\nenhancement issues in program repair, 2022.\n\n[25] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr,\nand Yizhe Zhang. Training software engineering agents and verifiers with\nSWE-gym. In _Forty-second International Conference on Machine Learning_, 2025.\n\n[26] Rachel Potvin and Josh Levenberg. Why google stores billions of lines of code in\n[a single repository. https://cacm.acm.org/research/why-google-stores-billions-](https://cacm.acm.org/research/why-google-stores-billions-of-lines-of-code-in-a-single-repository/)\n[of-lines-of-code-in-a-single-repository/, July 2016. Accessed on 2025-09-22.](https://cacm.acm.org/research/why-google-stores-billions-of-lines-of-code-in-a-single-repository/)\n\n[27] Pat Rondon, Renyao Wei, José Cambronero, Jürgen Cito, Aaron Sun, Siddhant\nSanyam, Michele Tufano, and Satish Chandra. Evaluating agent-based program\nrepair at google. _arXiv preprint arXiv:2501.07531_, 2025.\n\n[28] Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. SpecRover: Code\nIntent Extraction via LLMs . In _2025 IEEE/ACM 47th International Conference on_\n_Software Engineering (ICSE)_, pages 963–974, Los Alamitos, CA, USA, May 2025.\nIEEE Computer Society.\n\n[29] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing\nincrementally for constrained auto-regressive decoding from language models.\nIn Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih,\neditors, _Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-_\n_guage Processing_, pages 9895–9901, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics.\n\n[30] Nicolas Schreuder and Evgenii Chzhen. Classification with abstention but without disparities. In Cassio de Campos and Marloes H. Maathuis, editors, _Pro-_\n_ceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence_,\nvolume 161 of _Proceedings of Machine Learning Research_, pages 1227–1236. PMLR,\n27–30 Jul 2021.\n\n[31] Mahan Tafreshipour, Aaron Imani, Eric Huang, Eduardo Santana de Almeida,\nThomas Zimmermann, and Iftekhar Ahmed. Prompting in the wild: An empirical\nstudy of prompt evolution in software repositories. In _2025 IEEE/ACM 22nd_\n_International Conference on Mining Software Repositories (MSR)_, pages 686–698.\nIEEE, 2025.\n\n[32] Weixi Tong and Tianyi Zhang. CodeJudge: Evaluating code generation with\nlarge language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\nChen, editors, _Proceedings of the 2024 Conference on Empirical Methods in Natural_\n_Language Processing_, pages 20032–20051, Miami, Florida, USA, November 2024.\nAssociation for Computational Linguistics.\n\n[33] Bingbing Wen, Jihan Yao, Shangbin Feng, Chenjun Xu, Yulia Tsvetkov, Bill Howe,\nand Lucy Lu Wang. Know your limits: A survey of abstention in large language\n\n\nmodels. _Transactions of the Association for Computational Linguistics_, 13:529–556,\n2025.\n\n[34] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. _arXiv preprint_\n_arXiv:2407.01489_, 2024.\n\n[35] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik\nNarasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable\nautomated software engineering. _Advances in Neural Information Processing_\n_Systems_, 37:50528–50652, 2024.\n\n\n\nGoogle et al.\n\n\n[36] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In _Proceedings of the 33rd_\n_ACM SIGSOFT International Symposium on Software Testing and Analysis_, pages\n1592–1604, 2024.\n\n[37] Changsheng Zhuge, Mingchen Zhao, Dylan R. Ashley, Wenyi Wang, Dmitrii\nKhizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Krishnamoorthi Raghuraman, Yuandong Tian, Yangyang Shi, Vikas Chandra, and Jürgen Schmidhuber.\nAgent-as-a-judge: Evaluate agents with agents. _arXiv preprint arXiv:2410.10934v2_,\n2025.\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\n**A** **Human-Reported Bug Curation**\n\n\nIn this section, we describe the criteria for curating human-reported\nbugs that have a known ground truth patch for completeness. The\ncriteria closely follows [ 27 ], with the addition of the test target\ncriteria. Namely, each bug must satisfy the following:\n\n\n(1) Was reported during a period of a year and closed with\ncode changes after a model-based cutoff date in the recent\npast – we use the cutoff to prevent old code changes that\nare more likely to be stale.\n(2) Has a known ground-truth fix and an associated fail-topass test that could be used as a held-out test oracle. This\nfail-to-pass test must execute without any build errors and\nfail in the pre-patch code and execute and succeed in the\npost-patch code.\n(3) The bug and submitted code changes have a 1:1 relationship,\nmitigating risk of tangled commits\n(4) Did not have screenshots in their description (multimodal\nrepairs are out of scope for this work)\n(5) Affected files for any of the following languages: C++, C,\nDart, Java, Kotlin, Typescript, Javascript, Go, and Python\nand conversely excluded changes that affected SQL, HTML,\nCSS, configuration languages, binary data files, or multimedia data files\n(6) Had a unidiff ground-truth commit of less than 150 lines\n(7) Had <= 31 test targets that depended on the files changed\nin the ground-truth patch.\n\n\n**B** **Bug Abstention Prompts**\n\n\nThe section contains the full prompts for the bug abstention policies\ndescribed in Section 3.1.2.\n\n\n**Listing 1: Prompt for bug representation** _𝑟_ 1 ( _𝑏_ ) **.**\n\n\nYou are tasked to predict whether an LLM -based agentic\n\n\napproach will successfully fix a bug.\n\nThe agent can use a suite of tools by issuing commands,\n\n\nincluding:\n\n   - ``` code_search ``` to find relevant code.\n\n   - ``` cat ``` to read file contents.\n\n   - ``` ls ``` to list directory contents.\n\n   - ``` find_and_replace_text ``` to modify files.\n\n   - ``` bazel ``` to run tests.\n\nThe agent is presented with a bug report and can perform\n\n\na maximum of 25 steps.\n\n\nAnalyze the following bug and predict if the agent will\n\n\nsucceed. Your output must be a single word: either \"\n\nsuccess\" or \"failure \". Do not provide any other text\n\n, explanation, or punctuation.\n\n\n# Bug Title\n\n{bug.title}\n\n\n# Bug Description\n\n{bug.description}\n\n\n**Listing 2: Prompt for bug representation** _𝑟_ 2 ( _𝑏_ ) **.**\n\n\n# GUIDELINES\n\n\nThese guidelines provide criteria for determining if a\n\n\nbug is likely to be solved by the agent.\n\n - Clear Problem & Action: The bug report explicitly and\n\n\nclearly describes the incorrect behavior, and the\n\nrequired change is well -understood.\n\n\n\n\n - Precise Code Localization: The report points to the\n\n\nexact file, code line, function, or configuration\n\nthat needs attention, or it 's easily identifiable\n\nfrom provided information (e.g., stack traces).\n\n - Simple, Localized Fix Anticipated: The likely solution\n\n\ninvolves a small, straightforward, and localized\n\nchange.\n\n - Actionable Reproduction & Sufficient Context: The\n\n\nissue can be easily and reliably reproduced with\n\nthe information provided, and sufficient context is\n\n\navailable within the report for the agent to\n\nproceed effectively.\n\n - Self -Contained and Complete: All necessary information\n\n\nfor addressing the bug is provided and self \ncontained within the report, requiring minimal\n\nexternal look -up or inference.\n\n\n**C** **Patch Validation Prompts**\n\n\nThis section contains the full prompts for the patch validation\npolicy described in Section 3.2. Listing 3 shows the prompt for the\nspecification generation step.\n\n\n**Listing 3: Using the original bug description and file contents**\n**to generate a specification.**\n\n\nSYSTEM:\n\nYou are an expert software engineer that reviews a bug\n\n\nand lists out a specification for the desired\n\nbehavior.\n\nYou should cover properties desirable for a fix. Be\n\n\nspecific, do not provide general properties that\n\napply to all bugs. Do not provide properties that\n\nare about artifacts like documentation.\n\n\nUSER:\n\nHere is a bug, please create a fix specification:\n\n\n# Bug Title\n\n{bug.title}\n\n\n# Bug Description\n\n{bug.description}\n\n\n# Original source code\n\n$ cat {file_path}\n\n{file_content}\n\n\n# Specification\n\n\nListing 4 shows the LLM prompt used for validating a patch,\nalong with the use of the generated specification.\n\n\n**Listing 4: Using the original bug report, candidate patch, any**\n**test information, and the fix specification generated in the**\n**prior step, to decide if a patch is likely to fix a bug.**\n\n\nSYSTEM:\n\nSETTING: You are a software engineer reviewing code\n\n\nchanges to fix bugs. Your job is to review how a\n\nbug2code agent tried to fix a bug.\n\nReturn a ReviewResponse with whether the bug is fixed and\n\n\nexplaining why.\n\n\nUSER:\n\n\n**Figure 14: If we consider samples of size k of trajectories**\n**that pass the held-out reproduction test, more aggressive**\n**validation is more likely to incorrectly reject patches.**\n\n\n\n\n\n\n\n\n\n**Figure 15: Patches accepted by the patch validation policy**\n**have slightly fewer line-level operations compared to those**\n**it rejected.**\n\n\nApproach Binary Label Precision Recall\n\n\nPrevalence (build/test) TRUE 0.19   No-spec validation (binary) FALSE 0.99 0.07\nNo-spec validation (binary) TRUE 0.2 1\nValidation (binary) FALSE 0.91 0.58\nValidation (binary) TRUE 0.3 0.76\n\n\n**Table 4: Precision/Recall Table on the binary judgment: val-**\n**idation’s binary judgment provides modest predictive per-**\n**formance. Removing the specification usage reduces perfor-**\n**mance substantially.**\n\n\n\nGoogle et al.\n\n\nHere 's a log of an attempt by a bug2code agent to fix a\n\n\nbug:\n\n# Bug Title\n\n{bug.title}\n\n\n# Bug Description\n\n{bug.description}\n\n\n# Patch\n\n{patch}\n\n\n# Test information\n\n{test_information}\n\n\n# Specification:\n\nBased on the bug report and source code, here is a\n\n\nspecification for the desired behavior of the fix.\n\n\n### Fix Specification\n\n{specification}\n\n\n**C.1** **Additional Results**\n\n\nTable 4 presents the per-label precision/recall for binary validation\npolicies.\nTo focus on the validation’s ability to identify successful trajectories, we also consider a variant of recall. Specifically, we compute\nthe average (over bugs) probability that at least one trajectory is\naccepted by the validation, given a sample of k trajectories that\nsatisfy the held-out reproduction test. In effect, if our repair system\nproduced only good fixes, how often would we show at least one\nrepair to a user if we generated k trajectories. We denote this mean\n(validation recall)@k. To compute percentile-based thresholding\nwe determine cutoffs over all trajectories that pass build and test\nregressions first, before filtering.\nAs shown in Figure 14, percentile-based thresholding results in\nmore false negatives – rejecting patches that we should have shown\nto humans. When sampling a single successful patch, the binary\nvalidation will show a patch at a rate of 0.54, compared to 0.36 and\n0.16 for 75th and 90th percentile thresholding, respectively.\nFigure 15 shows that patches accepted by the validation policy\nhave slightly more line-level operations compared to those rejected.\n\n\n**C.2** **Patch Validation Decisions: False Positives**\n\n\nListing C.2 shows the generated specification and validation policy\noutput for the false positive example discussed in Section 7.\n\n\n\"Here is a fix specification for the bug.\n\n\n### Desired Behavior Specification\n\n\n1. ** Conditional Decimal Formatting for Amounts **\n\nThe amount displayed in the amount input field must\n\n\nbe formatted based on whether it is an integral\n\nvalue.\n\n   - If the ``` Money ``` object represents an integer (e.g\n\n\n., the fractional part is zero), it must be\n\nformatted as a string without decimal places (e.\n\ng., \"500\").\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle et al.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet _𝑉_ _𝑏_ be the value of successfully closing a bug _𝑏_ with a fix.\nLet _𝐶_ _𝑏_ be the cost on the code base of leaving the bug unrepaired.\nLet _𝑅_ _𝑐_ be the cost to produce a repair trajectory. Let _𝑝_ [∗] be the true,\nunknown, probability of a trajectory solving bug _𝑏_ .\nThe expected net value of the abstention policy is then:\n\n_𝑙𝑙𝑝_ [∗] ( _𝑉_ _𝑏_ ) + (1 − _𝑝_ [∗] )(− _𝐶_ _𝑏_ ) − _𝑅_ _𝑐_ if _𝑃_ _𝑎_ ( _𝑏_ ) ≥ _𝜏_ _𝑎_\n�− _𝐶_ _𝑏_ otherwise\n\n\n\nGoogle et al.\n\n\nLet _𝐻_ _𝑐_ be the cost of having a human review a patch from trajectory _𝑡_ . The analogous expected net value for the patch validation\npolicy is then:\n\n_𝑙𝑙𝑝_ [∗] ( _𝑉_ _𝑏_ ) + (1 − _𝑝_ [∗] )(− _𝐶_ _𝑏_ ) − _𝐻_ _𝑐_ if _𝐷_ _𝑣_ ( _𝑃_ _𝑣_ ( _𝑏,𝑡_ ))\n�− _𝐶_ _𝑏_ otherwise\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03215v1",
      "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
      "authors": [
        "Tianyu Fu",
        "Zihan Min",
        "Hanling Zhang",
        "Jichao Yan",
        "Guohao Dai",
        "Wanli Ouyang",
        "Yu Wang"
      ],
      "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
      "published": "2025-10-03T17:52:32Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03215v1"
    },
    "text": "Preprint version\n\n## - - C ACHE TO -C ACHE : D IRECT S EMANTIC C OMMUNICA TION B ETWEEN L ARGE L ANGUAGE M ODELS\n\n\n**Tianyu Fu** _[∗]_ [1] _[,]_ [2] **, Zihan Min** _[∗]_ [1] **, Hanling Zhang** _[∗]_ [3] **, Jichao Yan** [1] **,**\n**Guohao Dai** [5] _[,]_ [2], **Wanli Ouyang** [3] _[,]_ [4], **Yu Wang** _[†]_ [ 1]\n\n\n\n1 Tsinghua University 2 Infinigence AI 3 The Chinese University of Hong Kong\n4 Shanghai AI Laboratory 5 Shanghai Jiao Tong University\n\n\n\nA BSTRACT\n\n\nMulti-LLM systems harness the complementary strengths of diverse Large Language Models, achieving performance and efficiency gains unattainable by a single model. In existing designs, LLMs communicate through text, forcing internal\nrepresentations to be transformed into output token sequences. This process both\nloses rich semantic information and incurs token-by-token generation latency.\nMotivated by these limitations, we ask: _Can LLMs communicate beyond text?_\nOracle experiments show that enriching the KV-Cache semantics can improve response quality without increasing cache size, supporting KV-Cache as an effective medium for inter-model communication. Thus, we propose Cache-to-Cache\n(C2C), a new paradigm for direct semantic communication between LLMs. C2C\nuses a neural network to project and fuse the source model’s KV-cache with that of\nthe target model to enable direct semantic transfer. A learnable gating mechanism\nselects the target layers that benefit from cache communication. Compared with\ntext communication, C2C utilizes the deep, specialized semantics from both models, while avoiding explicit intermediate text generation. Experiments show that\nC2C achieves 8.5-10.5% higher average accuracy than individual models. It further outperforms the text communication paradigm by approximately 3.0-5.0%,\nwhile delivering an average 2.0× speedup in latency. Our code is available at\n[https://github.com/thu-nics/C2C.](https://github.com/thu-nics/C2C)\n\n\n1 I NTRODUCTION\n\n\n\n_context_ _s-prompt_ _t_ _0_ _t_ _n-1_ _context_ _t_ _0_ _… t_ _n_ _query_ _r_ _0_ _context_ _context_ _query_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n**R** …\n\n\n\n\n\n\n\n…\n\n\n\n\n\n\n\n\n\n\n\n\n\n…\n\n\n\n_communication text:_ _t_ _0_ _t_ _1_ _t_ _n_ _response text:_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: (a) Previous Text-to-Text (T2T) communication passes information through explicit text\ngeneration. (b) Our Cache-to-Cache (C2C) communication directly projects and merges KV-Cache\nwith rich semantics from different LLMs.\n\n\nWith the rapid progress of Large Language Models (LLMs) (Guo et al., 2025; Yang et al., 2025a;\nOpenAI, 2025), they are now applied across increasingly diverse domains and tasks. To meet versatile demands, LLMs are trained with distinct focuses, such as coding (Hui et al., 2024), mathematics (Yang et al., 2024a), visual understanding (Bai et al., 2025), edge computing (Zhang et al.,\n2024b), and so on. Meanwhile, general-purpose LLMs can also simulate specialized capabilities\nthrough prompt engineering, enabling flexible role adaptation across downstream applications.\n\n\nLeveraging the diversity of LLMs, many multi-LLM systems are proposed to further enhance overall\nperformance and efficiency (Guo et al., 2024; Tran et al., 2025). In **collaborative multi-LLM**\n\n\n_∗_ Equal contribution.\n\n_†_ Corresponding author: Yu Wang (yu-wang@tsinghua.edu.cn).\n\n\n1\n\n\nPreprint version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Conceptual comparison of T2T and C2C communication in a Coder-Writer collaboration\nexample. In T2T, the Coder’s ambiguous text instruction fails to convey the structural semantics\nof <p> as a paragraph separator, causing the Writer to misplace the content. C2C directly projects\nthe Coder’s KV-Cache into the Writer, transferring both the semantic understanding and precise\ninsertion location without intermediate text generation.\n\n\n**systems** (Li et al., 2023; Wu et al., 2023), LLMs are assigned distinct roles and proactively exchange\ntext messages. Mirroring human collaboration, these systems accumulate partial understandings\nor sub-solutions from different agents via verbal communication. They harnessing the collective\ncapabilities of multiple LLMs to solve complex problems that a single model cannot. By contrast,\n**routing-based** multi-LLM inference systems rely on passive context inheritance rather than active\nmessage exchange. These systems coordinate models of varying parameter sizes or reasoning depths\nfor more dynamic and efficient responses (Li et al., 2024; Fu et al., 2025; Ong et al., 2024; OpenAI,\n2025). Downstream models inherit the context from preceding models in multi-round conversations,\nthen generate follow-up responses to the new questions based on their own understanding of the\nconversation history.\n\n\nHowever, current text-to-text (T2T) interfaces restrict information exchange among LLMs, particularly when conveying rich or diverse semantic interpretations of a shared context. As illustrated in\nFigure 2, these limitations arise from several inherent constraints of T2T communication. First, as\na low-bandwidth medium, text introduces an information bottleneck. The high-dimensional internal representations must be repeatedly compressed into linear strings and then decompressed by the\nreceiver LLM. When models differ in knowledge or assigned roles, some signals may be irrecoverable (e.g., interpreting <p> as a section marker). Second, natural language is inherently ambiguous,\nwith idioms, underspecified references, and vague expressions. Although recent agent protocols\naim to standardize text messages (Anthropic, 2024; Surapaneni et al., 2025), rigid templates remain\ninsufficient for flexible, open-domain collaboration. Third, T2T communication incurs noticeable\nlatency. Every exchange requires exhaustive, token-by-token decoding of contextual explanations in\nsequence. These limitations motivate a key question:\n\n\n_Can LLMs communicate beyond text?_\n\n\nIn this work, we explore using KV-Cache as the medium for LLM communication. KV-Cache is a\nnaturally richer representation than text. It also enables fully parallel communication through direct\nprojection, avoiding the slow sequential decoding in text exchanges. Our oracle experiments show\nthat (1) Enriching KV-Cache under the same context length can lead to an increase in accuracy. (2)\nKV-Cache is convertible between LLMs. (3) Different LLMs encode distinct semantic understandings and contextual knowledge of the same input, reflecting their complementary strengths.\n\n\nEncouraged by these oracles, we propose Cache-to-Cache (C2C), a new paradigm for richer and\nfaster multi-LLM communication. As shown in Figure 1(b), C2C projects the KV-Cache from a\nsource model into the space of a target model and merges them through a neural Cache Fuser.\nExperiments show that C2C achieves 8.5-10.5% higher average accuracy than individual models.\nIt further outperforms the T2T paradigm by approximately 3.0-5.0%, while delivering an average\n2.0 _×_ speedup in latency.\n\n\n2 R ELATED W ORK\n\n\n2.1 KV-C ACHE S HARING AND R EUSE\n\n\nBased on the similarity of KV-Cache between layers, intra-model cache sharing methods (Yang\net al., 2024b; Wu & Tu, 2024; Sun et al., 2024; Brandon et al., 2024; Wu et al., 2025) are proposed\n\n\n2\n\n\nPreprint version\n\n\nto reuse shallow layers’ KV-Cache for deeper layers to accelerate single LLM inference. Another\nresearch focus is to reuse a portion of KV-Cache (e.g., common prefix, reference documents) for the\nsame model in multiple user queries (Bang, 2023; Ye et al., 2024; Yao et al., 2024; Qin et al., 2024;\nYang et al., 2025b). DroidSeek Liu et al. (2024a) extends cache reuse to models fine-tuned from\nthe same base model. Unlike existing works that focus on computational efficiency through cache\nreuse, our approach leverages the KV-Cache as a medium for semantic transfer between LLMs.\nFurthermore, unlike existing cache sharing methods that are restricted to only a single model or\nmodels with identical structure and size, our method supports sharing across different model families\nand varying model sizes.\n\n\n2.2 M ULTI -LLM S YSTEMS\n\n\n**Collaborative multi-LLM systems** . Collaborative systems treat multiple LLMs as peers that exchange information to improve collective performance. Chain-of-Agents (Zhang et al., 2024c) and\nMetaGPT (Hong et al., 2023) create sequential message flows where agents directly communicate\nusing natural language interfaces. Mixture-of-Agents Wang et al. (2024) and DyLAN (Liu et al.,\n2024b) introduce layered communication architectures. Target LLMs aggregate messages from multiple models using voting or summarization mechanisms. Multi-agent debate methods (Estornell &\nLiu, 2024; Liang et al., 2024; Du et al., 2023) involve iterative communication rounds, letting LLM\nagents discuss and refine responses. Recent works such as MCP Anthropic (2024) and A2A Surapaneni et al. (2025) establish formal text protocols beyond natural language, standardizing agent\ninteraction and tool usage in collaborative multi-LLM systems. These approaches rely on text-level\ninterfaces, where communication requires one model to generate text token-by-token and another to\ningest it as input. Our work explores a deeper and more efficient collaboration by directly sharing\ninternal KV-Cache representations.\n\n\n**Routing-based multi-LLM inference systems** . To accelerate LLM inference, several systems\nleverage multiple models with different capabilities and costs. Dynamic model selection methods (OpenAI, 2025; Ong et al., 2024; Feng et al., 2024) route queries to different models with\nvarying sizes and configurations to balance efficiency and performance. Token-level routing methods (Zhang et al., 2024a; Shen et al., 2024; Zheng et al., 2025; Fu et al., 2025) enable finer-grained\nselection, utilizing smaller models for simple token generation within the reasoning process of complex tasks. While these systems achieve efficiency through strategic model switching, they either\ncompletely drop context from other models, or simply rely on their own understandings of the context. Without understanding sharing, smaller models cannot benefit from the richer representations\nalready computed by larger models.\n\n\n3 M ETHOD\n\n\n3.1 P RELIMINARIES\n\n\n**LLM inference** . Autoregressive LLM inference involves two stages: _prefill_ and _decode_ . Prefill\nencodes the full input to produce the first output token; decode then generates subsequent tokens\niteratively using the last token and the cached key–value (KV) states. Formally, let _X_ [0: _n_ ] =\n\n[ _x_ 0 _, . . ., x_ _n−_ 1 ] be the input token sequence. After prefill, LLM produces a per-token KV-Cache\n_C_ ( _X_ [0: _n_ ] ) = [ _c_ 0 _, . . ., c_ _n−_ 1 ] _∈_ R _[n][×][d]_ . For notation brevity, _d_ denotes the KV dimensionality that is\nflattened from all layers into a single vector per token. The range subscripts are omitted when clear.\nDuring decoding, with current token _y_ _i_ and caches from the input and the generated prefix, the next\ntoken is predicted as\n_y_ _i_ +1 = _P_ � _y_ _i_ _| C_ ( _X_ ) _⊕C_ ( _Y_ [0: _i_ ] )� _,_ (1)\nwhere _⊕_ denotes sequence-wise concatenation. The cache updates as _C_ ( _Y_ [0: _i_ +1] ) = _C_ ( _Y_ [0: _i_ ] ) _⊕C_ ( _y_ _i_ ).\n\n\n**LLM communication** . In LLM communication scenarios, we define the LLM that provides contextual understanding or knowledge as _Sharer_, and the one that utilizes it as _Receiver_ .\n\n\n3.2 O RACLES FOR C ACHE - TO -C ACHE C OMMUNICATION\n\n\nWe aim to explore whether LLMs can have direct semantic communication through KV-Cache.\nSpecifically, we design two oracle experiments to answer the following questions: (1) _Benefit_ : can\n\n\n3\n\n\nPreprint version\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||||||||\n|||||||||\n|40<br>30<br>20<br>|40<br>30<br>20<br>|40<br>30<br>20<br>|40<br>30<br>20<br>|0<br>0<br>|0<br>0<br>|0<br>20|0<br>20|\n\n\n\nFigure 3: The t-SNE representations of source,\ntarget, and transformed KV-Cache.\n\n\n**Method** **Cache Len. Cache Augment Acc. (%)**\n\n\nDirect _|X|_ No 58.42\nFew-shot _|E|_ + _|X|_ Yes 63.39\nOracle _|X|_ Yes 62.34\n\n\nTable 1: Cache enrichment experiment. Oracle\nprefills on _E_ and _X_, then discard _E_ .\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||2<br>3<br>4<br>5<br>6<br>|2<br>3<br>4<br>5<br>6<br>|2<br>3<br>4<br>5<br>6<br>|2<br>3<br>4<br>5<br>6<br>|2<br>3<br>4<br>5<br>6<br>|2<br>3<br>4<br>5<br>6<br>|8<br>9<br>|8<br>9<br>|8<br>9<br>|0|\n\n\nFigure 4: Accuracy influence of accumulatively\naugmenting different number of layers.\n\n\n**Average Effective Rank**\n\n\n**Type** **Sharer** **Receiver** **C2C**\n\n\nK Cache 539 388 395\n\nV Cache 689 532 560\n\n\nTable 2: Average effective rank of KV-Cache\nfrom Sharer, Receiver, and the C2C fused one.\n\n\n\na model’s capabilities be improved through KV-Cache semantic enrichment without extending sequence length? (2) _Convertibility_ : can the KV-Cache of one model be effectively utilized by another\nmodel?\n\n\n3.2.1 C ACHE E NRICHMENT O RACLE\n\n\nTo validate the benefit of cache enrichment, we first explore whether the semantic quality of a _fixed-_\n_length_ question KV-Cache can be improved without increasing cache size. Few-shot prompting\nsuggests this might work: providing _exemplars E_ before the _question X_ often improves accuracy.\nBut does this arise from attending to more context tokens, or from _E_ enriching how _X_ is embedded\nin KV-Cache?\n\n\nWe evaluate this via three setups: (1) _Direct_ : prefill on _X_ only and decode with _C_ ( _X_ ); (2) _Few-shot_ :\nprefill on _E ⊕X_ and decode with _C_ ( _E ⊕X_ ) (longer cache); (3) _Oracle_ : prefill on _E ⊕X_ but _discard_\nthe exemplar segment and keep only the question-aligned slice\n\n\n_C_ _[∗]_ ( _X_ ) = _C_ [ _|E|_ : _|E|_ + _|X|_ ] ( _E ⊕_ _X_ ) _,_ (2)\n\n\nso that decoding uses a question-length cache with no extra tokens. Here, _| · |_ denotes sequence\nlength. In Equation 1, this corresponds to substituting _C_ ( _X_ ) with _C_ _[∗]_ ( _X_ ) before decoding.\n\n\nComparing _Direct_ and _Oracle_ isolates the effect of cache enrichment: any gain arises from the richer\nquestion embeddings induced by _E_, not from attending to additional token caches as in _Few-shot_ .\nAs shown in Table 1, the _Oracle_ setup improves response quality at the same cache length.\n\n\nAdditionally, we analyzed how cache enrichment affects different transformer layers. Our findings\nshow substantial variation across layers: while some layers benefit from cache enrichment, others\nexperience performance degradation (details in Appendix A.2.1). Furthermore, these layer-wise\neffects accumulate as more layers are augmented. As shown in Figure 4, selectively applying cache\nenrichment to the top-10 performing layers yields higher accuracy than enriching all layers, while\ntargeting the worst-performing layers leads to accuracy decline. This finding guides the gating\nmechanism of our cache Fuser (Section 3.3.2).\n\n\n3.2.2 C ACHE T RANSFORMATION O RACLE\n\n\nTo verify that one model’s KV-Cache can be utilized by another, we conducted cross-model transformation experiments. We trained a 3-layer MLP to map the KV-Cache from a source LLM (Qwen34B) to a target LLM (Qwen3-0.6B), with more setups detailed in Appendix A.3.2.\n\n\n4\n\n\nPreprint version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Receiver<br>cache reply|Col2|Col3|\n|---|---|---|\n|cache<br>**Receiver**<br>reply|cache|reply|\n\n\n\n\n\nFigure 5: C2C Fuser architecture and training scheme.\n\n\nT-SNE visualizations in Figure 3 reveal that the raw KV-Caches of the two LLMs are far apart in\nrepresentation space. After transformation, the mapped KV-Cache is inside the KV-Cache representation space of the target model. These results demonstrate that KV-Caches from different models\nare, in general, convertible as the transformed cache in the representation space of the target model.\n\n\nOne thing to note is that the transformed cache occupies only a smaller subset of the target’s space.\nIt indicates that the source model’s semantic information cannot fully cover the target’s, despite the\nsource being larger. This reflects inherent differences in how each model encodes context. Another\nobservation also supports this interpretation: the correct-answer sets of different models exhibit limited overlap (Figure 7), despite the comparable aggregated accuracy of respective models. These\nfindings suggest that if specialized contextual understanding from different models can be successfully projected and fused, it may harness the complementary strengths of respective models.\n\n\n3.3 C2C D ESIGN\n\n\n3.3.1 O VERVIEW\n\n\nBuilding on the oracle experiments, we propose the C2C Fuser architecture. Its core objective is to\nextract useful contextual understanding or knowledge from one model (the Sharer) and fuse it into\nanother model (the Receiver).\n\n\nIn general, the C2C paradigm contains a set of key/value cache fusers _F_ and a layer mapping strategy\n_G_ . During the prefill stage, fuser _F_ _n_ takes the _n_ th layer cache of the Receiver Model _C_ _n_ ( _X_ ) and the\ncorresponding _G_ ( _n_ )th layer cache of the Sharer Model _C_ _G_ _[S]_ ( _n_ ) [(] _[X]_ [)][ and and generate the corresponding]\nfused cache:\n_C_ _[F]_ = _{F_ _n_ ( _C_ _n_ ( _X_ ) _, C_ _G_ _[S]_ ( _n_ ) [(] _[X]_ [))] _[}]_ _n_ _[N]_ =1 (3)\n\n\nDuring decoding, with the current token _y_ _i_ and caches from the input and the generated prefix, the\nnext token is predicted as:\n\n\n_y_ _i_ +1 = _P_ � _y_ _i_ �� _C_ _F_ ( _X_ ) _⊕C_ ( _Y_ [0: _i_ ] )� (4)\n\n\n3.3.2 F USER S TRUCTURE\n\n\nTo enhance the Receiver’s KVCache without destructive overwriting of its information, the fuser is\ndesigned under a residual integration principle. As shown in Figure 5, it contains three key modules:\n\n\n(1) **Projection module** concatenates the Receiver’s KV-Cache with the Sharer’s KV-Cache, then\nprocesses the concatenated feature through a projection layer followed by a feature fusion layer.\n\n\n(2) **Dynamic weighting module** applies an input-aware head modulation layer to dynamically\nreweight the projected information.\n\n\n(3) **Learnable gate** introduces a trainable per-layer gate value that decides whether to inject the\nSharer’s context. The value applies a Gumbel-sigmoid with temperature annealing to smoothly\ntransition from differentiable during training to binary at inference.\n\n\n3.3.3 M ODEL A LIGNMENT\n\n\nFusing KV-Caches across model families and sizes requires alignment at two levels: tokens and layers. For _token alignment_, different tokenizers may produce slightly varied token sequences for the\n\n\n5\n\n\nPreprint version\n\n\nsame input. We align them by decoding each Receiver token into its string form and re-encoding it\nusing the Sharer’s tokenizer. When one-to-many mappings occasionally occur, we select the Sharer\ntoken with maximal string coverage to preserve information. For _layer_ alignment, we adopt a terminal alignment strategy: the final layers of both models are aligned first, then the penultimate layers,\nand so on in reverse order until reaching the shallower model’s first layer. Detailed specifications\nare provided in Appendix A.1.\n\n\n3.3.4 T RAINING S CHEME\n\n\nDuring training, we freeze both the Sharer and Receiver models, training only the C2C module\nfor KV-Cache fusion. We employ standard next-token prediction loss on the Receiver’s response\npredictions, similar to supervised fine-tuning (SFT). The key difference is that the Receiver predicts\nresponses conditioned on fused KV-Cache rather than its own.\n\n\nThe training procedure consists of three stages: (1) Forward: both models encode the input context\nto produce their respective KV-Caches. (2) Fusion: the C2C module fuses both KV-Caches and\nreplaces the Receiver’s cache. (3) Supervision: the Receiver prefills the response using the fused\ncache, and gradients backpropagate through C2C to minimize prediction loss.\n\n\n4 E XPERIMENT\n\n\n4.1 S ETUP\n\n\nWe highlight key setups here, with more details in Appendix A.3.\n\n\n**Models** . We evaluate C2C across various model families, including Qwen2.5 (Yang et al., 2024a;\nHui et al., 2024), Qwen3 (Yang et al., 2025a), Llama3.2 (Dubey et al., 2024), and Gemma3 (Team\net al., 2025). To test generalizability, we select different configurations for the Sharer-Receiver\nmodel combinations, including models of different generations (Qwen3 and Qwen2.5), different\nfamilies (Qwen, Llama, and Gemma), different sizes (0.6B to 14B), different specializations (general, code, and math model), and different training stages (pretrained and instruction fine-tuned\nmodels). For ablative and diagnostic analyses (scaling behavior, ablation study, behavior analysis), we fix the Receiver and Sharer to Qwen3 models unless otherwise specified. This consistency\neliminates confounders from model alignment and isolates the core impact of C2C.\n\n\n**Baselines** . We compare C2C over two LLM collaboration methods to contextualize performance:\n(1) Text-to-Text (T2T) communication: Collaborate by analyze-response hand-off for each query.\nThe Sharer generates analytical text of key information to solve the input question. This text is\nconcatenated with the original question and fed to the Receiver to mirror standard collaborative\npipelines. Corresponding prompts are in Appendix A.3.6. (2) Query-level routing (Ong et al., 2024):\nCollaborate by selecting the appropriate LLM for different queries. We also include individual\nmodel performance (Sharer or Receiver alone) to establish a lower bound for collaborative gains.\n\n\n**Benchmarks** . We evaluate on four widely used benchmarks spanning reasoning, knowledge,\nand language domains to ensure comprehensive coverage. OpenBookQA (Mihaylov et al., 2018)\nfor fact-based reasoning, MMLU-Redux (Gema et al., 2025) for knowledge in the general domain, ARC-Challenge (ARC-C) (Clark et al., 2018) for scientific and logistic reasoning, and CEval (Huang et al., 2023) for comprehensive knowledge in the Chinese domain.\n\n\n**Training dataset** . To ensure the generalizability of C2C, we utilize the first 500k samples of the\nOpenHermes2.5 Dataset (Teknium, 2023), a general finetuning dataset, to train C2C Fusers. For\nbudgeting training purposes, we utilize the MMLU as the trainset on scaling behavior and behavior\nanalysis, unless specified.\n\n\n**Evaluation settings** . We use average accuracy as the performance metric. We use text generation\nand answer extraction as the evaluation mode for C2C and baselines, with the max generation length\nset to 64 for multi-choice benchmarks. All experiments are conducted in the zero-shot setting with\nzero generation temperature to ensure reproducibility. We use average inference time as the efficient\nmetric, measured using a single NVIDIA A100 GPU (Choquette et al., 2021) with batch size = 1.\n\n\n6\n\n\nPreprint version\n\n\nTable 3: Comparison of communication methods across benchmarks. We use Qwen3-0.6B as the\nReceiver model.\n\n\n**Sharer** **Task** **Metric** **Receiver** **Sharer** **Routing** **Text-to-text** **Cache-to-cache**\n\n\nAcc 35.53 38.42 35.58 41.03 **42.92**\nMMLU-Redux\nTime 0.29 0.34 0.27 1.52 0.40\n\n\n\nQwen2.5-0.5B\n\n\nLlama3.2-1B\n\n\nQwen3-4B-Base\n\n\n\nAcc 39.20 45.60 40.80 44.00 **52.60**\nOpenBook\nTime 0.27 0.35 0.29 0.81 0.30\n\n\nAcc 41.04 42.09 40.70 49.48 **54.52**\nARC-C\nTime 0.29 0.39 0.29 1.00 0.36\n\n\nAcc 32.04 40.21 34.61 35.88 **41.77**\nC-Eval\nTime 0.26 0.31 0.26 1.51 0.34\n\n\nAcc 35.53 32.30 33.38 43.32 **44.42**\nMMLU-Redux\nTime 0.29 0.06 0.18 0.75 0.50\n\n\nAcc 39.20 32.60 36.40 41.20 **47.80**\nOpenBook\nTime 0.26 0.07 0.17 0.70 0.43\n\n\nAcc 41.04 33.57 37.22 50.00 **53.39**\nARC-C\nTime 0.28 0.07 0.18 0.70 0.47\n\n\nAcc 32.04 31.31 31.92 35.27 **40.77**\nC-Eval\nTime 0.25 0.04 0.15 0.71 0.49\n\n\nAcc 35.53 1.03 16.39 43.87 **43.95**\nMMLU-Redux\nTime 0.29 2.06 0.28 7.54 0.45\n\n\nAcc 39.20 2.20 22.20 46.40 **53.20**\nOpenBook\nTime 0.26 1.98 0.27 5.08 0.34\n\n\nAcc 41.04 1.48 19.65 53.91 **55.39**\nARC-C\nTime 0.28 2.06 0.28 6.56 0.40\n\n\nAcc 32.04 5.65 15.10 38.92 **42.79**\nC-Eval\nTime 0.25 2.02 0.26 3.59 0.39\n\n\n\n4.2 P ERFORMANCE AND E FFICIENCY\n\n\nAs shown in Table 8, C2C consistently improves the Receiver model performance across different\nsettings and benchmarks. After applying C2C, we see an average increase of accuracy by 11.00%,\n9.64%, and 11.88% across three different Sharers. Compared with text-to-text communication,\nC2C achieves an average accuracy increase of 5.36%, 4.15%, and 3.06%. It also achieves obvious\nspeedups of 3.46 _×_, 1.51 _×_, and 14.41 _×_, thanks to the waiving of intermediate text message generation. In contrast, query-level routing prioritizes efficiency but limits accuracy to the better of the\ntwo original models.\n\n\nNotably, when using Qwen3-4B Base as the Sharer, the generated text sometimes ignores instructions and exceeds the expected length. This results in extremely long text-to-text communication\ntime, while C2C bypasses this issue. The setup highlights an interesting use case of C2C, where a\nweak SFT model enables a strong pre-trained base model to follow instructions. We also observe\nthat Llama3.2-1B exhibits exceptionally fast inference, analyzed in Appendix A.4.3.\n\n\n4.3 S CALING B EHAVIOR\n\n\n**Scaling sequence lengths** . We evaluate how C2C scales with respect to sequence length on longcontext tasks from the LongBenchV1 benchmark. All C2C fusers are trained and tested on different\nsets of LongBenchV1. As shown in Table 4, C2C consistently outperforms text-to-text communication across all sequence-length intervals. It indicates C2C’s advantages across input length ranges.\nMore detailed setups and results are in Appendices A.2.2 and A.3.4.\n\n\n**Scaling model sizes** . We investigate how C2C scales with respect to the Sharer and Receiver model\nsizes. All C2C fusers are trained on MMLU’s auxiliary train split and evaluated on MMLU-Redux.\n\n\n7\n\n\nPreprint version\n\n\nseries, while the curves correspond to Receiver models from the Qwen3 series.\n\n\n\nLength Receiver Sharer T2T C2C\n\n\n0-4k 27.39 21.89 29.47 **36.64**\n\n4-8k 24.97 18.55 26.30 **31.71**\n\n8k+ 22.20 14.04 24.54 **25.37**\n\n\nTable 4: LongBenchV1 scores with\nQwen3-0.6B (Receiver) and Qwen2.50.5B (Sharer) across input lengths.\n\n\n\nSetting #Param. OpenBook ARC-C MMLU C-Eval\n\n\nSingle 596M 45.80 47.65 36.81 35.81\nIdentical 529M 50.60 52.52 42.17 40.34\n\nC2C 478M **52.60** **54.52** **42.92** **41.77**\n\n\nTable 5: Performance comparison of different training settings. _Single_ directly finetunes the Receiver model. _Iden-_\n_tical_ uses the same model for Sharer and Receiver.\n\n\n\nAs shown in Figure 6, the _x_ -axis denotes Sharer size (Qwen2.5-Instruct series), the _y_ -axis shows\naccuracy gains of C2C over Receiver-only baselines (∆ Accuracy), and each curve represents a\nReceiver from the Qwen3 series. We find that the accuracy improvements of C2C generally increase\nfaster than T2T. This trend shows that when the Sharer possesses richer knowledge, C2C is able to\nmore effectively transmit useful information to the Receiver. Note that the relative gains for larger\nReceivers are less pronounced due to their stronger baselines and higher overlap with the Sharer’s\nknowledge.\n\n\n**Different model combinations** . We test different Sharer-Receiver combinations, including different\nmodel families and different task-specific models. The result in Table 6 shows that C2C outperforms\ntext-to-text communication on all five combinations by an average increase of 8.59%. This supports\nthat by employing C2C, the Receiver model can effectively utilize contextual understanding from\ndifferent model to enhance performance. Notably, when using Qwen2.5-Math as the Sharer, the\ninference and communication time becomes substantially longer, analyzed in Appendix A.4.3. To\nfurther test the generalizability of C2C, we swap the Sharer and Receiver model. The result shows\nthat C2C robustly brings an 5.05% increase in accuracy while applying T2T results in a 6.3% decrease in performance.\n\n\nTogether, these experiments support the scalability of C2C as an effective and efficient new LLM\ncommunication paradigm.\n\n\n4.4 A BLATION S TUDY\n\n\n**Sources of improvement** . In Table 5, we ablate the source of C2C performance gain by fixing the\nReceiver(Qwen3-0.6B) and varying the Sharer. _Single_ denotes standard full fine-tuning of the Receiver without Sharer. _Identical_ denotes C2C where both Sharer and Receiver are Qwen3-0.6B. Our\ndefault C2C uses Qwen2.5-0.5B as the Sharer. Under the same training configuration, C2C consistently attains higher accuracy than both _Single_ and _Identical_ . This confirms that C2C improvements\ndo not purely come from added trainable capacity or overfitting to the training set. Instead, it points\nto complementary contextual understanding contributed by the heterogeneous Sharer. Identical still\noutperforms single, indicating that cache-level self-communication can provide useful auxiliary understanding, echoing effects observed in latent reasoning and recurrent transformers (Zeng et al.,\n2025; Saunshi et al., 2025).\n\n\n8\n\n\nPreprint version\n\n\nPair Type Receiver Sharer Metric Receiver Sharer T2T C2C\n\n\nAcc 39.20 31.75 41.35 **45.90**\nQwen3-0.6B Gemma3-1B\nTime 0.27 0.54 1.04 0.30\n\n\n\nHeterogeneous\n\n\n\nAcc 39.20 39.86 43.71 **46.13**\nQwen3-0.6B Qwen2.5-Math-1.5B\nTime 0.27 8.71 6.60 0.27\n\n\nAcc 39.20 25.09 39.74 **46.89**\nQwen3-0.6B Qwen2.5-Coder-0.5B\nTime 0.27 0.26 1.59 0.27\n\n\n\nAcc 38.42 39.20 32.12 **43.47**\nQwen2.5-0.5B Qwen3-0.6B\nTime 0.34 0.27 0.98 0.21\nSwap\n\nAcc 39.20 38.42 41.03 **46.50**\nQwen3-0.6B Qwen2.5-0.5B\nTime 0.27 0.34 1.52 0.26\n\n\nTable 6: Comparison of Receiver-only, Sharer-only, T2T, and C2C across accuracy and time. The\npairs are grouped into _Heterogeneous_ settings (where the Receiver is paired with Sharers of different\ncapabilities) and _Swap_ settings (where Receiver and Sharer roles are exchanged).\n\n\nMethod MMLU ARC-C OpenBook CEval Average\n\n\nProject 20.01 19.57 21.80 21.41 20.70\n+Fuse **43.36** 51.65 47.60 36.91 44.88\n+Gate (=C2C) 42.92 **54.52** **52.60** **41.77** **47.95**\n\n\nTable 7: Performance comparison on MMLU, ARC-C, OpenBook, and CEval benchmarks.\n\n\n**Fuser architecture** . In Table 7 we show the effect of different components in the C2C design. Compared with pure projection, keeping the Receiver’s original KV-Cache and fusing with the Sharer’s\nKV-Cache combines ability from both model and increases the accuracy by 24.18%. Adding a gate\nfor fused layer selection also helps increasing the average accuracy by 3.07%.\n\n\n4.5 B EHAVIOR A NALYSIS\n\n\n**Effective rank analysis** . We analyze the effective rank of KV-Cache before and after cache-tocache communication. Effective rank (Roy & Vetterli, 2007) is a common approach for measuring\nthe intrinsic dimension of model weight or activation value; a higher intrinsic dimension means\nricher semantic information, as formalized in Appendix A.4.1. As the Table. 2 shows, after cacheto-cache fusing, the K and V’s effective rank increased from 388 to 395 and from 532 to 560,\nrespectively. This indicates that C2C enriches the semantic space by successfully transforming in\nthe Sharer model and injecting knowledge into the Receiver model.\n\n\n**Progressive behavior** . We analyze the progressive behavior of C2C by gradually increasing the percentage of context KV-Cache being updated by C2C. When the percentage is above 50%, increasing\nthe percentage continuously yields better performance. Detailed setup and analysis can be found at\nAppendix A.2.4\n\n\n**Gate behavior** . We analyze the behavior of C2C ’s learnable gates under different training regimes\nas detailed in Appendix A.4.2.\n\n\n5 F UTURE W ORK\n\n\nAs a general LLM communication paradigm, C2C can be expanded to various fields. Some potential scenarios include: (1) Privacy-aware cloud–edge collaboration: a cloud-scale model can transmit\ncurated KV-Cache segments to an edge model to boost capability without emitting raw text, reducing bandwidth and limiting content exposure. (2) Integration with current inference acceleration\nmethod: use C2C to enhance speculative decoding and enable token-level routing across heterogeneous models for lower latency and cost. (3) Multimodal integration: align and fuse caches among\n\n\n9\n\n\nPreprint version\n\n\nlanguage reasoning LLMs, vision–language models (VLMs), and vision–language–action (VLA)\npolicies so that linguistic and visual context can drive more accurate actions.\n\n\n6 C ONCLUSION\n\n\nIn conclusion, we demonstrate that LLMs can communicate beyond text. We introduce Cache-toCache (C2C), a general paradigm that transforms and fuses key–value (KV) caches across models\nto enable direct semantic communication. Across diverse tasks and model configurations, C2C\nconsistently achieves higher task performance and better efficiency than text-to-text communication.\nThese results establish cache-to-cache as a practical alternative to token-based communication and\nhighlight its promise for scalable, low-latency multi-LLM systems.\n\n\n10\n\n\nPreprint version\n\n\nR EFERENCES\n\n\n[Anthropic. Introducing the model context protocol. Online; Nov. 25, 2024, 2024. URL https:](https://www.anthropic.com/news/model-context-protocol)\n[//www.anthropic.com/news/model-context-protocol. Accessed: 2025-09-08.](https://www.anthropic.com/news/model-context-protocol)\n\n\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. _arXiv preprint arXiv:2502.13923_,\n2025.\n\n\nFu Bang. Gptcache: An open-source semantic cache for llm applications enabling faster answers\nand cost savings. In _Proceedings of the 3rd Workshop for Natural Language Processing Open_\n_Source Software (NLP-OSS 2023)_, pp. 212–218, 2023.\n\n\nWilliam Brandon, Mayank Mishra, Aniruddha Nrusimha, Rameswar Panda, and Jonathan RaganKelley. Reducing transformer key-value cache size with cross-layer attention. _Advances in Neural_\n_Information Processing Systems_, 37:86927–86957, 2024.\n\n\nJack Choquette, Wishwesh Gandhi, Olivier Giroux, Nick Stam, and Ronny Krashinsky. Nvidia a100\ntensor core gpu: Performance and innovation. _IEEE Micro_, 41(2):29–35, 2021.\n\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n_arXiv preprint arXiv:1803.05457_, 2018.\n\n\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In _Forty-first International_\n_Conference on Machine Learning_, 2023.\n\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models.\n_arXiv e-prints_, pp. arXiv–2407, 2024.\n\n\nAndrew Estornell and Yang Liu. Multi-llm debate: Framework, principals, and interventions. _Ad-_\n_vances in Neural Information Processing Systems_, 37:28938–28964, 2024.\n\n\nTao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: A graph-based router for llm selections.\n_arXiv preprint arXiv:2410.03834_, 2024.\n\n\nTianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong\nYang, and Yu Wang. R2r: Efficiently navigating divergent reasoning paths with small-large model\ntoken routing. _arXiv preprint arXiv:2505.21600_, 2025.\n\n\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria\nMancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani,\net al. Are we done with mmlu? In _Proceedings of the 2025 Conference of the Nations of the Amer-_\n_icas Chapter of the Association for Computational Linguistics: Human Language Technologies_\n_(Volume 1: Long Papers)_, pp. 5069–5096, 2025.\n\n\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms\nvia reinforcement learning. _arXiv preprint arXiv:2501.12948_, 2025.\n\n\nTaicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest,\nand Xiangliang Zhang. Large language model based multi-agents: A survey of progress and\nchallenges. _arXiv preprint arXiv:2402.01680_, 2024.\n\n\nSirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven\nKa Shing Yau, Zi Hen Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu.\nMetagpt: Meta programming for multi-agent collaborative framework. _ArXiv_, abs/2308.00352,\n[2023. URL https://api.semanticscholar.org/CorpusID:260351380.](https://api.semanticscholar.org/CorpusID:260351380)\n\n\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. _Advances in Neural Information Processing Systems_, 36:\n62991–63010, 2023.\n\n\n11\n\n\nPreprint version\n\n\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang,\nBowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. _arXiv preprint arXiv:2409.12186_,\n2024.\n\n\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for” mind” exploration of large language model society. _Advances in Neural_\n_Information Processing Systems_, 36:51991–52008, 2023.\n\n\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires\nrethinking feature uncertainty. _arXiv preprint arXiv:2401.15077_, 2024.\n\n\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming\nShi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multiagent debate. In _Proceedings of the 2024 Conference on Empirical Methods in Natural Language_\n_Processing_, pp. 17889–17904, 2024.\n\n\nYuhan Liu, Yuyang Huang, Jiayi Yao, Shaoting Feng, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua\nCheng, Junchen Jiang, Shan Lu, et al. Droidspeak: Kv cache sharing for cross-llm communication\nand multi-llm serving. _arXiv preprint arXiv:2411.02820_, 2024a.\n\n\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. A dynamic llm-powered agent network\nfor task-oriented agent collaboration. In _First Conference on Language Modeling_, 2024b.\n\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _Proceedings of the 2018 Conference_\n_on Empirical Methods in Natural Language Processing_, pp. 2381–2391, 2018.\n\n\nIsaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E Gonzalez,\nM Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. _arXiv_\n_preprint arXiv:2406.18665_, 2024.\n\n\n[OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, Au-](https://openai.com/index/introducing-gpt-5/)\ngust 7 2025. Accessed: 2025-09-11.\n\n\nRuoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran\nXu. Mooncake: A kvcache-centric disaggregated architecture for llm serving. _arXiv preprint_\n_arXiv:2407.00079_, 2024.\n\n\nOlivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality. In _2007_\n_15th European signal processing conference_, pp. 606–610. IEEE, 2007.\n\n\nNikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank J Reddi. Reasoning\nwith latent thoughts: On the power of looped transformers. _arXiv preprint arXiv:2502.17416_,\n2025.\n\n\nZejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, and David Sontag. Learning to decode collaboratively with multiple language models. In _Proceedings of the 62nd Annual Meeting of the_\n_Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 12974–12990, 2024.\n\n\nYutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong\nWang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models.\n_Advances in Neural Information Processing Systems_, 37:7339–7361, 2024.\n\n\nRao Surapaneni, Miku Jha, Michael Vakoc, and Todd Segal. Announcing the agent2agent protocol\n[(a2a). Google Developers Blog, April 2025. URL https://developers.googleblog.](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)\n[com/en/a2a-a-new-era-of-agent-interoperability/. Accessed: 2025-09-08.](https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/)\n\n\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej,\nSarah Perrin, Tatiana Matejovicova, Alexandre Ram´e, Morgane Rivi`ere, et al. Gemma 3 technical\nreport. _arXiv preprint arXiv:2503.19786_, 2025.\n\n\nTeknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023.\n[URL https://huggingface.co/datasets/teknium/OpenHermes-2.5.](https://huggingface.co/datasets/teknium/OpenHermes-2.5)\n\n\n12\n\n\nPreprint version\n\n\nKhanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O’Sullivan, and\nHoang D Nguyen. Multi-agent collaboration mechanisms: A survey of llms. _arXiv preprint_\n_arXiv:2501.06322_, 2025.\n\n\nJunlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances\nlarge language model capabilities. _arXiv preprint arXiv:2406.04692_, 2024.\n\n\nHaoyi Wu and Kewei Tu. Layer-condensed kv cache for efficient inference of large language models.\nIn _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics_\n_(Volume 1: Long Papers)_, pp. 11175–11188, 2024.\n\n\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,\nLi Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multiagent conversation framework. _arXiv preprint arXiv:2308.08155_, 3(4), 2023.\n\n\nYou Wu, Haoyi Wu, and Kewei Tu. A systematic study of cross-layer kv sharing for efficient llm\ninference. In _Proceedings of the 2025 Conference of the Nations of the Americas Chapter of_\n_the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short_\n_Papers)_, pp. 396–403, 2025.\n\n\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical\nexpert model via self-improvement. _arXiv preprint arXiv:2409.12122_, 2024a.\n\n\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. _arXiv preprint_\n_arXiv:2505.09388_, 2025a.\n\n\nJingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, and Shiyu Chang. Kvlink: Accelerating large language models via efficient kv cache reuse. _arXiv preprint arXiv:2502.16002_, 2025b.\n\n\nYifei Yang, Zouying Cao, Qiguang Chen, Libo Qin, Dongjie Yang, Hai Zhao, and Zhi Chen.\nKvsharer: Efficient inference via layer-wise dissimilar kv cache sharing. _arXiv preprint_\n_arXiv:2410.18517_, 2024b.\n\n\nJiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan\nLu, and Junchen Jiang. Cacheblend: Fast large language model serving with cached knowledge\nfusion. _arXiv e-prints_, pp. arXiv–2405, 2024.\n\n\nLu Ye, Ze Tao, Yong Huang, and Yang Li. Chunkattention: Efficient self-attention with prefix-aware\nkv cache and two-phase partition. In _Proceedings of the 62nd Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Papers)_, pp. 11608–11620, 2024.\n\n\nBoyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu\nLi, and Zhouhan Lin. Pretraining language models to ponder in continuous space. _arXiv preprint_\n_arXiv:2505.20674_, 2025.\n\n\nKaiyan Zhang, Jianyu Wang, Ning Ding, Biqing Qi, Ermo Hua, Xingtai Lv, and Bowen Zhou.\nFast and slow generating: An empirical study on large and small language models collaborative\ndecoding. _CoRR_, 2024a.\n\n\nMingjin Zhang, Xiaoming Shen, Jiannong Cao, Zeyang Cui, and Shan Jiang. Edgeshard: Efficient\nllm inference via collaborative edge computing. _IEEE Internet of Things Journal_, 2024b.\n\n\nYusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. Chain of agents:\nLarge language models collaborating on long-context tasks. _Advances in Neural Information_\n_Processing Systems_, 37:132208–132237, 2024c.\n\n\nWenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P\nXing, Hongyi Wang, and Huaxiu Yao. Citer: Collaborative inference for efficient large language\nmodel decoding with token-level routing. _arXiv preprint arXiv:2502.01976_, 2025.\n\n\n13\n\n\nPreprint version\n\n\nA A PPENDIX\n\n\nA.1 D ESIGN C HOICE E XPLORATION\n\n\nWe detail the designs of C2C with discussions on alternative possible design choices in this section.\n\n\nA.1.1 L AYER A LIGNMENT\n\n\n**Terminal alignment** . In this strategy, the layers of the two models are aligned starting from the\noutput side. Specifically, the final layer of the smaller model is paired with the final layer of the\nlarger model, the penultimate layer with the penultimate layer, and so on. This scheme prioritizes\nalignment between the deeper layers across models, which typically captures higher-level semantic\nrepresentations.\n\n\n**Depth-normalized alignment** . In this strategy, both models’ layer indices are normalized to [0 _,_ 1]\nby dividing by ( _L_ _−_ 1), where _L_ is the total number of layers in the model. Let the model with fewer\nlayers ( _L_ min ) serve as the anchor. For each anchor layer _i_ (with normalized index _i/_ ( _L_ min _−_ 1)), we\nselect the layer _j_ in the other model ( _L_ max ) whose normalized index _j/_ ( _L_ max _−_ 1) is closest:\n\n\n_i_ _j_\n\n_j_ _[⋆]_ = arg min _j_ ��� _L_ min _−_ 1 _[−]_ _L_ max _−_ 1 ��� _._ (5)\n\n\nThis method produces an alignment that distributes correspondences approximately uniformly\nacross the model depth.\n\n\n**C2C Choice** . In our design, we adopt **terminal alignment**, as it provides a simpler and more direct\nlayer mapping strategy that empirically performs slightly better in our experiments.\n\n\nA.1.2 T OKENIZATION A LIGNMENT\n\n\nFor dialogue inputs, we first apply the chat template of each tokenizer, which produces a sequence\nconsisting of alternating sections of (1) _template tokens_ and (2) _message tokens_ . These two types of\nsections are handled differently during alignment.\n\n\n**Template sections** . Template tokens are structural markers (e.g., role delimiters, formatting tokens)\nthat differ across tokenizers and carry no semantic content. To preserve sequence consistency without introducing unnecessary distortions, these sections are aligned by simple length padding: the\nshorter side is padded with <pad> tokens until both tokenizers’ sequences are of equal length.\n\n\n**Message sections** . Message tokens correspond to the actual textual content of user or assistant\ndialogs. Each target model token in a message section is decoded into its string form and re-encoded\nusing the source model tokenizer. Special tokens (e.g., <pad>, <eos>) are mapped directly if\npossible; otherwise, the source model unknown token is used. For regular tokens, if the re-encoding\nproduces a single source model token, a direct one-to-one mapping is established. If multiple source\nmodel tokens are produced (a one-to-many case), one of the two selection strategies is applied: (1)\n_first-occurrence selection_ : choose the first source model token from the candidate set, yielding a\ndeterministic and computationally efficient mapping. (2) _Maximal-coverage selection_ : decode each\ncandidate token, compute its string length, and select the longest; this heuristic aims to preserve\nmaximal surface correspondence with the original target model token.\n\n\n**C2C choice** We observed that the two selection strategies generally produce very similar results,\nwith more than 80% of sequences yielding identical alignments across strategies. Based on this\nobservation, we empirically adopt **Maximal-coverage selection** as the default strategy to reduce the\nrisk of losing information in one-to-many tokenization cases.\n\n\nThrough this design, template sections are aligned structurally via padding, while message sections\nare aligned semantically at the token level, ensuring robust correspondences between target model\nand source model representations in chat-formatted inputs.\n\n\nA.1.3 F USER A RCHITECTURE\n\n\nBeyond the C2C Fuser, we also examined a more complex yet potentially more powerful variant,\nwhich we denote as **C2C-C (Complex)** . The main complexity comes from the introduction of an\n\n\n14\n\n\nPreprint version\n\n\nTable 8: Performance comparison across benchmarks with PGR and overall normalized time.\n\n\n**C-Eval** **ARC-C** **MMLU-Redux** **OpenBook**\n\n\n**Method** Acc PGR Time Acc PGR Time Acc PGR Time Acc PGR Time\n\n\nQ3-4B 68.09 100% 0.24 87.48 100% 0.24 71.38 100% 0.24 79.40 100% 0.25\nQ3-0.6B 32.04 0% 0.18 41.04 0% 0.19 35.53 0% 0.18 39.20 0% 0.21\nT2T 36.96 14% 0.92 52.00 24% 0.80 42.95 21% 0.99 46.40 18% 1.70\n\n\nC2C 44.40 34% 0.27 60.17 41% 0.27 45.92 29% 0.27 55.20 40% 0.28\n\nC2C-C 60.63 79% 0.21 80.96 86% 0.23 62.78 76% 0.15 70.40 78% 0.26\n\n\nadditional projection stage: instead of directly concatenating Sharer and Receiver caches as in C2C,\nSharer cache is first projected into the receiver’s dimensionality through a 3-layer MLP. The concatenated representation is then processed along two familiar routes—feature fusion and dynamic\nweighting—to yield the final S&R cache.\n\n\nThe main experiment results are presented in Table 8. Note that we fix the maximum response length\nto 8 tokens and the maximum communication length to 256 tokens in this experiment to reduce evaluation cost. C2C-C attains stronger performance than the default C2C, suggesting that increasing the\narchitectural sophistication of Fuser can further amplify the benefits of C2C communication. In this\ntable, we also report Performance Gap Recovered (PGR) (Ong et al., 2024) metric, which quantifies\nhow much of the performance gap between a weak and a strong model is recovered. Nevertheless,\nthe focus of this work is on introducing the C2C paradigm itself. For this purpose, we adopt a simple\nyet effective Fuser design, leaving systematic investigation of more elaborate architectures to future\nwork.\n\n\nA.2 A DDITIONAL E XPERIMENTAL R ESULTS\n\n\nA.2.1 C ACHE E NRICHMENT D ETAIL\n\n\nIn Table 9 we show the effect of single-layer cache enrichment. Layer 4 and 16 benefit from the\ncache enrichment approach by replacing the KV-Cache with the few-shot one, while cache enrichment on other layers shows performance degradation.\n\n\nTable 9: Accuracy of single-layer cache enrichment\n\n\nLayer Acc. Layer Acc.\n\n\n0 56.36 14 54.24\n\n1 56.36 15 58.06\n\n2 57.14 16 **58.45**\n\n3 57.53 17 57.88\n\n4 **58.52** 18 57.21\n\n5 56.45 19 56.71\n\n6 54.56 20 55.93\n\n7 56.82 21 57.74\n\n8 55.01 22 57.23\n\n9 56.78 23 55.22\n\n10 55.29 24 55.75\n\n11 57.05 25 56.16\n\n12 55.04 26 55.79\n\n13 54.83 27 55.01\n\n\nA.2.2 S TRONG - TO -W EAK C OMMUNICATION\n\n\nTable 10 reports the results on LongBenchV1 when pairing the weak receiver Qwen3-0.6B with a\nmuch stronger sharer, Qwen3-4B, under different input lengths. Across all length regimes, C2C\nconsistently outperforms both the receiver alone and the T2T baseline. On average, C2C achieves a\n40.45% PGR over the weak-to-strong gap. These results demonstrate that in strong-to-weak settings,\n\n\n15\n\n\nPreprint version\n\n\nTable 10: LongBenchV1 results with Qwen3-4B as the Sharer and Qwen3-0.6B as the Receiver\nacross different input lengths.\n\n\nLength Receiver Sharer T2T C2C\n\n\n0–4k 27.39 48.76 31.71 37.07\n\n4–8k 24.97 46.27 27.54 32.08\n\n8k+ 22.20 42.78 26.14 31.00\n\n\nAverage 25.01 46.12 28.62 33.55\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sharer: Q2.5-Math-1.5B, Receiver: Q3-0.6B\n\n\n\n(b) Sharer: Q3-4B, Receiver: Q3-0.6B\n\n\n\nFigure 7: Venn diagrams of correctly answered questions under different model pairings.\n\n\nC2C can effectively transfer the stronger model’s contextual understanding, yielding notable gains\nfor the weaker receiver.\n\n\nWe additionally evaluated the strong-to-weak setting (Qwen3-0.6B as receiver and Qwen3-4B as\nsharer) on other benchmarks beyond LongBenchV1. The detailed results are provided in Section A.1.3, Table 8.\n\n\nA.2.3 A CCURACY B REAKDOWN\n\n\nWe analyze where the accuracy gains of C2C come from by using Venn diagrams on the MMLURedux benchmark, as illustrated in Figure 7. For this analysis, we use the C2C-C variant introduced\nin Section A.1.3, as it has the potential to achieve stronger performance amd provides a clearer\nbreakdown of where C2C ’s accuracy originates.\n\n\n**Models with comparable capacity** . When the Receiver (Qwen3-0.6B, denoted as Q3-0.6B) and\nthe Sharer (Qwen2.5-Math-1.5B-Instruct, denoted as Q2.5-Math-1.5B) have comparable overall capacity but complementary strengths, C2C not only inherits part of the Sharer’s ability but also solves\nadditional questions by integrating understanding from both models.\n\n\n**Models with disparate capacity** . When the Sharer (Qwen3-4B, denoted as Q3-4B) is substantially\nstronger than the Receiver (Qwen3-0.6B), C2C tends to integrate more of the stronger model’s\nunderstanding. Quantitatively, in the disparate-capacity case (Figure 7b), among the questions that\nthe Sharer can answer correctly, C2C also answers 72.11% correctly. In contrast, in the comparablecapacity case (Figure 7a), C2C succeeds on only 50.97%.\n\n\n16\n\n\nPreprint version\n\n\nA.2.4 P ROGRESSIVE B EHAVIOR\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n|||F|rmer||\n|||L<br>S|atter<br>harer Mode|l|\n\n\n\nFigure 8: accuracy changes with C2C proportion increases\n\n\nTo investigate the impact of fused KV-Cache proportion on the accuracy of the receiver model, we\ngradually added the proportion of fused KV-Cache derived from the sharer to the receiver model\nbefore generating outputs. Specifically, former and latter refer to progressively replacing the receiver’s KV-Cache with the fused KV-Cache from front to back and back to front, respectively. We\nobserve that the overall accuracy first decreases and then increases as the replacement ratio grows.\nThe performance reduction may stem from the gap between training and testing, where only the full\nreceiver KV-Cache is used during training. When the fused proportion goes up to over 50%, the\nperformance of C2C continues to increase with respect to the proportion, reflecting the progressive\nbenefits of C2C. Note that projecting using latter cache generally has larger impact than projecting\nthe former, since it is closer to the final response.\n\n\nA.3 A DDITIONAL E XPERIMENT S ETUP\n\n\nA.3.1 C ACHE E NRICHMENT\n\n\nWe conducted Oracle experiments with Qwen3-0.6B and Qwen3-4B to examine how KV-Cache\nenrichment influences model performance. The evaluation was performed on MMLU-Redux. The\nfew-shot examples are selected from MMLU while excluding overlaps with MMLU-Redux to ensure\nfairness. To probe different ways of applying cache enrichment, we compared four cache enrichment\nstrategies: All-layer Cache Enrichment (apply cache enrichment on all layers), Single-Layer Cache\nEnrichment (apply cache enrichment only on single layers), Selective Cache Enrichment - Best (select n layers that have the highest accuracy according to Single-Layer Cache Enrichment), Selective\nCache Enrichment - Worst (select n layers that have the lowest accuracy according to Single-Layer\nCache Enrichment). All methods utilized Few-Shot–optimized KV-Caches while maintaining the\nsame cache length as Zero-Shot, enabling a controlled evaluation of cache enrichment and its layerspecific effects.\n\n\nA.3.2 C ACHE T RANSFORMATION\n\n\nWe employed the MMLU-Redux dataset to train a 3-layer MLP that maps the KV-Cache of a source\nLLM to that of a target LLM. For visualization, 300 samples were randomly selected from the\ndataset. The source, target, and transformed KV-Cache were all projected into two-dimensional\nspace using t-SNE, allowing us to examine the alignment of representations between the two models.\nFor t-SNE generation, we set perplexity to 50 and max iterations to 1000.\n\n\nA.3.3 Q UERY - LEVEL ROUTING\n\n\nQuery-level routing aims to improve the performance–efficiency trade-off by dynamically assigning\nharder queries to a stronger LLM. Following prior work, we adopt a matrix factorization framework.\nQuery embeddings are obtained from the OpenAI text-embedding-3-small encoder, while model\nembeddings are taken from pretrained vectors of gpt-4-1106-preview and mixtral-8x7b-instructv0.1. These embeddings are used to compute a strong win rate score for each query, which reflects\n\n\n17\n\n\nPreprint version\n\n\nits relative difficulty. Queries are then ranked by this score. For each evaluated model pair, we define\nthe strong model as the one achieving higher standalone benchmark accuracy and the weak model\nas the lower-performing one. Queries in the upper half of the ranking are routed to the strong model,\nwhile those in the lower half are routed to the weak model.\n\n\nA.3.4 E VALUATION M ETHOD\n\n\n**Main evaluation** . We evaluate C2C on four multiple-choice benchmarks: OpenBookQA, MMLURedux, ARC-Challenge, and C-Eval. For MMLU-Redux, we exclude questions annotated with the\nerror type _no correct answer_ . For all evaluations, we adopt a deterministic generation configuration\nwithout sampling, using greedy decoding to ensure reproducibility. Specifically, we use Non-CoT\nprompts, following the unified format described in Section A.3.6. Model outputs are then matched\nto the correct option labels to compute accuracy. To control evaluation cost, we set the maximum\nresponse length to 64 tokens unless otherwise specified, where the response refers to the final answer\ngenerated by the Receiver, since the base models do not always follow instructions and longer limits\nwould substantially increase inference time. For the T2T setting, we additionally set the maximum\ncommunication length to 256 tokens, where the communication refers to the messages passed from\nthe Sharer to the Receiver.\n\n\n**LongBench evaluation** . We evaluate C2C on the LongBench-E dataset, which comprises a total of\n13 individual datasets. We adopt the best-practice generation configuration of Qwen3, with temperature set to 0.6, topP to 0.95, topK to 20, minP to 0, repetition penalty of 1.2, and sampling enabled.\nThe prompts and evaluation procedures are strictly aligned with the official LongBench settings,\nwith a maximum output length of 2,048 tokens.\n\n\nA.3.5 C2C T RAINING\n\n\n**Training data** . (1) _Performance experiment._ The Fuser was trained on the OpenHermes-2.5 Dataset\nwith a maximum sequence length of 2,048 tokens. Training used 500,000 samples for one epoch\nwith a macro batch size of 256, corresponding to 1,929 total training steps.\n\n\n(2) _Scaling sequence lengths experiment._ The Fuser was trained on the LongBench-E benchmark\nwith a maximum sequence length of 12,000 tokens. The data was randomly split into 3/4 for training\nand 1/4 for evaluation to ensure independence between training and evaluation. Training used 1,896\nsamples for one epoch with a macro batch size of 16, corresponding to 118 total training steps.\n\n\n(3) _Scaling model sizes experiment._ The Fuser was trained on the auxiliary ~~t~~ rain split of the MMLU\ndataset with a maximum sequence length of 1,024 tokens. Training used 15,000 samples for one\nepoch with a macro batch size of 128, corresponding to 116 total training steps.\n\n\n**Training scheme** . All experiments were conducted with a fixed random seed of 42 to ensure reproducibility. Unless otherwise noted, the training configuration was as follows: optimization employed\na learning rate of 1 _×_ 10 _[−]_ [4] with a linear scheduler and a 10% warmup ratio, a weight decay of 0.01,\nand a maximum gradient norm of 1. The temperature was linearly annealed from 1.0 to 0.001 across\nthe total number of training steps. Layer alignment was configured with the last aligned scheme\nacross all experiments. Tokenization alignment was applied only when the paired models employed\ndifferent tokenizers, in which case the longest strategy was used. For data preparation, each dataset\nwas partitioned into a training split (99%) and a small held-out validation split (1%). The validation\nsplit was not used for model updates but was monitored during training to report evaluation loss.\n\n\nA.3.6 E VALUATION P ROMPTS\n\n\nTexts 1 and 2 present the exact prompts used for the main evaluation on multiple-choice datasets.\nText 3 provides the prompt for the Sharer model in the T2T evaluation, the Receiver model uses the\nsame prompt as in the C2C setting. Text 4 shows the prompt used in cache enrichment experiment.\nFor zero-shot method, no shots are included in the prompt. The few-shot method uses exactly\nthe same prompt as Text 4. For Oracle methods, we adopt the few-shot prompt but remove the\nKV-Cache associated with the shots after the forward pass. The prompt for LongBench evaluation\nstrictly follows its official configuration, which varies across the different sub-datasets.\n\n\n18\n\n\nPreprint version\n\n\n\n\n\n\n19\n\n\nPreprint version\n\n\n\n\n\nA.4 A DDITIONAL A NALYSIS\n\n\nA.4.1 E FFECTIVE R ANK\n\n\nWe list the definition of effective rank that was proposed by Roy & Vetterli (2007) here as a reference. For a matrix W that has size _M × N_, the singular value decomposition of it can be expressed\nas _W_ = _U_ Σ _V_ and the singular values _σ_ = ( _σ_ 1 _, σ_ 2 _, ..., σ_ _min_ ( _M,N_ ) ) _[T]_ are the non-negative diagonal\nentries of the matrix Σ. The singular value distribution is denote as:\n\n\n_σ_ _i_\n_p_ _i_ = (6)\n_∥σ∥_ 1\n\n\nDenote the Shannon Entropy as:\n\n\n\n_H_ ( _p_ 1 _, p_ 2 _, ..., p_ _min_ ( _M,N_ ) ) = _−_\n\n\nThe effective rank is define as:\n\n\n\n_min_ ( _M,N_ )\n� _p_ _i_ log _p_ _i_ (7)\n\n\n_i_ =1\n\n\n\n_erank_ ( _W_ ) = _e_ _[−]_ [�] _i_ _[min]_ =1 [(] _[M,N]_ [)] _p_ _i_ _logp_ _i_ (8)\n\n\nIn Figure 9, we present the effective rank of key and value caches across all the layers. The plot\nshows a continuous increase in the effective rank of value caches after applying C2C, especially\nin the shallow layers. Key caches after applying C2C also have a comparable effective rank and\nincrease at deep layers.\n\n\n20\n\n\nPreprint version\n\n\n\n\n\n\n\n\n\n(a) Effective rank of the Key cache.\n\n\n\n(b) Effective rank of the Value cache.\n\n\n\nFigure 9: Effective rank of Key and Value caches across layers for Qwen3-4B (Sharer) and Qwen30.6B (Receiver).\n\n\nA.4.2 G ATING B EHAVIOR\n\n\nWe analyze the behavior of the learnable gates by contrasting models trained on general-purpose\nversus task-specific data. This comparison reveals markedly different gating dynamics across the\ntwo regimes.\n\n\n**General-purpose training** . When C2C is trained on the OpenHermes-2.5 dataset, the learned key\nand value gates remain almost fully open. Across the three model combinations reported in Table 3, the average gate activation ratio exceeds 98.21%. Despite this near-complete activation, we\nobserve that in certain layers the dynamic weights are concentrated at very small values—for example, the average key weight in some layers falls below 0.1. This suggests that under general-purpose\ntraining, C2C leverages the dynamic weighting mechanism to modulate how much information is incorporated from the sharer on a per-query basis, effectively treating dynamic weights as the primary\ncontrol signal while leaving most gates open.\n\n\n**Task-specific training** . In contrast, when C2C is trained on the MMLU auxiliary ~~t~~ rain split, the\ngates exhibit a much sparser activation pattern. Across model combinations shown in Table 6, the\naverage gate activation ratio drops to 52.67%. For the layers where gates do open, however, the\ndynamic weights are substantially larger, with most layers exhibiting average weights above 0.4.\nThis indicates that under task-specific training, the gating mechanism selects a smaller subset of\nlayers that are consistently useful, while the dynamic weights primarily regulate the contribution\nstrength of these selected layers.\n\n\nOverall, these findings highlight the adaptive interplay between gates and weights: general-purpose\ntraining favors broad gate activation with fine-grained modulation via weights, whereas task-specific\ntraining favors sparse gate activation with stronger reliance on the selected layers.\n\n\nA.4.3 O UTLIER CASES IN INFERENCE TIME\n\n\n**Llama3.2** . We observe that the Llama3.2 model achieves significantly lower inference time compared to other baselines in Table 3. This improvement can be attributed to two factors. First, the\nLlama3.2 model itself has faster inference speed due to its implementation. Second, under the NonCoT evaluation prompts described in Section A.3.6, the model tends to output only a single option\nletter (e.g., “A” or “B”), rather than a longer formatted string such as “The correct answer is A.” The\nshorter outputs further reduce the average decoding time, leading to the observed advantage.\n\n\n**Qwen2.5-Math** . In contrast, the Qwen2.5-Math model exhibits considerably longer inference time,\nas shown in Table 6. The primary cause is its tendency to ignore the Non-CoT evaluation and\nT2T prompts described in Section A.3.6, producing verbose, step-by-step solutions rather than concise answers. To accommodate these long outputs and avoid truncation, we set both the maximum\nresponse length and the maximum communication length to 1024 tokens during evaluation. Under\nthis configuration, the model decodes substantially more tokens on average, resulting in significantly\nlonger inference time.\n\n\n21\n\n\nPreprint version\n\n\nA.4.4 E XAMPLE M ODEL O UTPUT\n\n\nIn this physics problem, the Sharer alone introduces proportional errors in its reasoning chain, while\nthe Receiver alone skips the reasoning step. Thus, both models are given the wrong answer. T2T\npasses reasoning through text to point out the important knowledge - _the force between two point_\n_charges is directly proportional to the product of their charges and inversely proportional to the_\n_square of the distance between them_ - but did not provide more understanding for it and how to use\nthe information to solve the problem. Therefore, though given an important hint, the model failed to\ngive the correct answer. In contrast, C2C directly transfers the KV-Cache, allowing the Receiver to\ngain rich contextual understanding from the Sharer and thus correctly answer the question.\n\n\n\n\n\n22\n\n\nPreprint version\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n23\n\n\n"
  },
  {
    "metadata": {
      "id": "2510.03209v1",
      "title": "Joint Bidding on Intraday and Frequency Containment Reserve Markets",
      "authors": [
        "Yiming Zhang",
        "Wolfgang Ridinger",
        "David Wozabal"
      ],
      "summary": "As renewable energy integration increases supply variability, battery energy\nstorage systems (BESS) present a viable solution for balancing supply and\ndemand. This paper proposes a novel approach for optimizing battery BESS\nparticipation in multiple electricity markets. We develop a joint bidding\nstrategy that combines participation in the primary frequency reserve market\nwith continuous trading in the intraday market, addressing a gap in the extant\nliterature which typically considers these markets in isolation or simplifies\nthe continuous nature of intraday trading. Our approach utilizes a mixed\ninteger linear programming implementation of the rolling intrinsic algorithm\nfor intraday decisions and state of charge recovery, alongside a learned\nclassifier strategy (LCS) that determines optimal capacity allocation between\nmarkets. A comprehensive out-of-sample backtest over more than one year of\nhistorical German market data validates our approach: The LCS increases overall\nprofits by over 4% compared to the best-performing static strategy and by more\nthan 3% over a naive dynamic benchmark. Crucially, our method closes the gap to\na theoretical perfect foresight strategy to just 4%, demonstrating the\neffectiveness of dynamic, learning-based allocation in a complex, multi-market\nenvironment.",
      "published": "2025-10-03T17:48:21Z",
      "pdf_url": "http://arxiv.org/pdf/2510.03209v1"
    },
    "text": "## Joint Bidding on Intraday and Frequency Containment Reserve Markets\n\nYiming Zhang [∗] Wolfgang Ridinger [†] David Wozabal [‡]\n\n\nOctober 6, 2025\n\n\n**Abstract**\n\n\nAs renewable energy integration increases supply variability, battery energy storage systems\n\n\n(BESS) present a viable solution for balancing supply and demand. This paper proposes a novel\n\n\napproach for optimizing battery BESS participation in multiple electricity markets. We develop a\n\n\njoint bidding strategy that combines participation in the primary frequency reserve market with\n\n\ncontinuous trading in the intraday market, addressing a gap in the extant literature which typically\n\n\nconsiders these markets in isolation or simplifies the continuous nature of intraday trading. Our ap\n\nproach utilizes a mixed integer linear programming implementation of the rolling intrinsic algorithm\n\n\nfor intraday decisions and state of charge recovery, alongside a learned classifier strategy (LCS) that\n\n\ndetermines optimal capacity allocation between markets. A comprehensive out-of-sample backtest\n\n\nover more than one year of historical German market data validates our approach: The LCS in\n\ncreases overall profits by over 4% compared to the best-performing static strategy and by more than\n\n\n3% over a naive dynamic benchmark. Crucially, our method closes the gap to a theoretical perfect\n\n\nforesight strategy to just 4%, demonstrating the effectiveness of dynamic, learning-based allocation\n\n\nin a complex, multi-market environment.\n\n### **1 Introduction**\n\n\nThe increasing share of variable renewable sources of electricity (VRES), such as wind and solar energy,\n\n\nchanges electricity systems around the world. As a consequence, unexpected and costly imbalances caused\n\n\nby the unpredictability of VRES output may lead to grid instability and even blackouts. Electricity\n\n\nstorage can offer flexibility to help mitigate these problems and stabilize electricity systems, thereby\n\n\ngenerating welfare gains (Sioshansi, 2010).\n\n\nTraditionally, electricity storage is a scarce resource and pumped-hydro storage facilities have been\n\n\nthe primary form of large-scale electricity storage available. These systems offer immense energy capacity,\n\n\nbut their deployment is limited by strict geographical requirements – namely, the need for significant\n\n\nelevation differences between two large reservoirs. Consequently, the potential for expanding the capacity\n\n\nof pumped hydro storage is severely limited in most electricity systems, creating a pressing need for\n\n\nalternative flexibility solutions to accommodate the growing share of VRES.\n\n\n∗ The Hong Kong Polytechnic University, Hong Kong; this work was mainly carried out at the Technical University of\n\n\nMunich and Entrix GmbH, Munich, Germany, `yiming1.zhang@connect.polyu.hk`\n\n  - Entrix GmbH, Munich, Germany, `wolfgang.ridinger@entrixenergy.com`\n\n  - Vrije Universiteit Amsterdam, Amsterdam, Netherlands, `d.wozabal@vu.nl`\n\n\n1\n\n\nTo fill this gap, a new portfolio of flexibility technologies is emerging, including battery energy storage\n\n\nsystems (BESS), green hydrogen for long-duration and seasonal energy storage, and demand response\n\n\nprograms that provide flexibility by actively managing consumption patterns. Among these technologies,\n\n\nBESS stands out for their rapid response times and locational flexibility, making them particularly\n\n\nsuitable for short-term energy arbitrage and ancillary services. Their high power-to-capacity ratio makes\n\n\nthem well-suited for grid stabilization services, where supply-demand imbalances must be corrected on\n\n\na second-to-second basis, and for participation in short-term electricity markets.\n\n\nDue to the dramatic and ongoing reduction in battery costs, BESS are becoming an increasingly\n\n\nviable option for grid-level electricity storage. For instance, lithium-ion battery pack prices have seen a\n\n\ndrop of more than 75% in the past 10 years with prices decreasing from $463/kWh in 2015 to $115/kWh\n\n\nin 2024 (BloombergNEF, 2024). This downward trend is projected to continue, with costs expected to\n\n\ndrop below $100/kWh well before 2030 (Cole and Karmakar, 2023).\n\n\nIn this paper, we propose a joint model for the participation in the market for primary frequency\n\n\nreserve and the intraday market (IDM), explicitly modeling orderbook-based continuous trading to max\n\nimize the potential to reoptimize positions multiple times.\n\n\nWhile a large number of literature addresses optimal bidding strategies for electricity storage, studies\n\n\nthat jointly consider the intraday and frequency control markets remain rare. Most existing approaches,\n\n\nsuch as (De Boeck et al., 2025; Fleten and Kristoffersen, 2008), focus exclusively on the day-ahead market.\n\n\nAlthough some papers incorporate both the day-ahead and intraday markets, the intraday market is\n\n\ntypically represented as a single rebalancing decision. This simplification allows the decision maker to\n\n\nadjust positions taken in the day-ahead market but overlooks the continuous nature of intraday trading\n\n\nand the resulting opportunities for multiple rebalancing actions. Examples of such papers include (Faria\n\n\nand Fleten, 2011; L¨ohndorf et al., 2013; Kongelf et al., 2019; L¨ohndorf and Wozabal, 2023). Wozabal and\n\n\nRameseder (2020) consider the problem of coordinated bidding in sequential auctions for a renewable\n\n\npower producer without storage in the Spanish intraday market.\n\n\nIn recent years, the intraday market has attracted an increasing number of participants seeking to\n\n\ncorrect short-term deviations from planned generation and load schedules. Various trading strategies in\n\n\nthe IDM were proposed. Dynamic programming approaches that model the intraday bidding process\n\n\nare proposed in (Jiang and Powell, 2015) and (A¨ıd et al., 2016), although neither accounts for intraday\n\n\nproducts with different times to maturity; and while A¨ıd et al. (2016) model the intraday market, they\n\n\ndo not consider storage. Bertrand and Papavasiliou (2019a,b) train a threshold policy for a storage\n\n\nunit on German limit order book data using reinforcement learning. The authors report significant\n\n\ngains for their trading strategy over a greedy strategy. Boukas et al. (2021) deploy a markov decision\n\n\nprocess–based modeling framework with a distributed fitted Q-iteration algorithm to optimize to decide\n\n\nwhen to employ the rolling intrinsic algorithm to trade on the intraday market.\n\n\nA limited amount of research literature has been devoted to the bidding strategy in the in reserve\n\n\nmarkets. A small body of work (Dimoulkas et al., 2016; Kraft et al., 2020) has concentrated on the\n\n\nprediction of frequency containment reserve (FCR) prices. The former predicted the SE2 price and\n\n\nvolume in the Nordic balancing market using a hidden Markov model. The latter study employed a\n\n\nstatistical model and neural networks in a rolling one-step framework to estimate the capacity-weighted\n\n\naverage price. The authors of both papers did not propose a bidding strategy based on their forecasts.\n\n\nA bidding strategy for the Nordic FCR is proposed in (Divshali and Evens, 2020), where the trading\n\n\nproblem is modeled as a non-linear constrained optimization problem, explicitly taking into account\n\n\ndegradation, energy, and balancing costs. Thien et al. (2017) investigates the profitability of different\n\n\n2\n\n\nstrategies for the German FCR market and Brivio et al. (2016) compares revenues on the Italian primary\n\n\nreserve market with arbitrage strategies on the wholesale market. Astero and Evens (2020); Ansari et al.\n\n\n(2015); Sortomme and El-Sharkawi (2012) consider using electric vehicle batteries in the FCR, while\n\n\nBroneske and Wozabal (2017) consider the same problem in the secondary reserve market.\n\n\nAn important topic when participating in the primary reserve market is the issue of state-of-charge\n\n\n(SoC) recovery after a frequency excursion event. In (Zhang et al., 2016), a theoretical stochastic dynamic\n\n\nprogramming framework was proposed to plan and control a BESS that participates in frequency reserves.\n\n\nXu et al. (2014) propose a SoC recovery by trading on the IDM. An online control mechanism was\n\n\nproposed in (Divshali and Evens, 2020).\n\n\nIn the papers reviewed so far, either the balancing market or the IDM are considered. The authors\n\n\nin (Biggins et al., 2022) propose a bidding strategy in the frequency reserves and day-ahead markets in\n\n\nGreat Britain based on machine learning. Their results show that the firm frequency response (FFR) is a\n\n\nstronger source of revenue than the day-ahead market for battery storage and highlight that the simulta\n\nneous performance in the FFR and the day-ahead market increases profits. Using machine learning-based\n\n\nprice prediction and uncertainty metrics, three bidding strategies are devised to trade in different fre\n\nquency reserves in (Kempitiya et al., 2020). Although multiple power markets were considered in the\n\n\naforementioned papers, they do not consider the IDM in their strategies.\n\n\nIn this paper, we propose the first approach that combines a realistic representation of a continuous\n\n\nintraday market with participation on a frequency control market. We use a version of the rolling intrinsic\n\n\n(RI) strategy to make trading decisions on the IDM and recovery SoC and machine learning classifier to\n\n\ndecide how much capacity to commit to which market. The aim of the overall approach is to define an\n\n\nalgorithm that can be used for high-frequency automatized decision-making.\n\n\nIn particular, the main contributions of the paper are:\n\n\n1. We propose a mixed-integer linear programming version of the intrinsic policy that can handle\n\n\norderbook-based continuous trading. Furthermore, we detail a rolling intrinsic algorithm that,\n\n\ngiven commitments on the primary reserve market, coordinates the repeated re-computation of\n\n\nthe intrinsic strategy with the necessary SoC recovery necessary for participation on the primary\n\n\nreserve market.\n\n\n2. We devise a machine learning classifier that decides how much of the available battery capacity to\n\n\ncommit on the reserve market. The decision is based on features that are available at the time of\n\n\nFCR bidding and the aim is to predict the times when the RI is more profitable than the FCR\n\n\nmarket and to optimally split the BESS’ capacity between the two markets.\n\n\n3. We present a comprehensive and realistic out-of-sample study that benchmarks the proposed strat\n\negy against a battery of alternatives using two years of real market data. Using detailed historical\n\n\ndata from these two markets, our results show that using our algorithm to combine FCR and\n\n\nIDM trading returns up to 4% more revenue than delivering FCR only and delivers 96% more than\n\n\ntrading in the continuous intraday market only. Our algorithm clearly outperforms naive strategies\n\n\nand is only 4% worse than a clairvoyant strategy that always chooses the right mix between the\n\n\ntwo markets.\n\n\nThe rest of the paper is structured as follows: In Section 2, we will discuss the IDM and FCR markets\n\n\nwith a focus on the European and, more specifically, the German situation. We outline market rules and\n\n\ndescribe interactions in the form of constraints that bids in one market imply for the other. Section 3\n\n\n3\n\n\nStart\n\n\n\nFCR Closure\n\n\n\nDay-ahead Auction\n\n\nIntraday Auction/Continuous\n\n\n\nFigure 1: Order of events on any given trading day for EPEX day-ahead and intraday markets and the\n\n\nFCR auctions in Germany.\n\n\n**FCR** **Intraday**\n\n\n2020 2021 2022 2023 2020 2021 2022 2023\n\n\n**Traded quantity** 573 562 555 570 77 85 92 120\n\n\n**Bidding frequency** Daily, 4-hour blocks Continuous\n\n\n**Pricing mechanism** Uniform price auction Order book based\n\n\nTable 1: Daily volumes auctioned in the German FCR market (in MW) from Bundesnetzagentur (see\n\n\n`[https://data.bundesnetzagentur.de/](https://data.bundesnetzagentur.de/)` ), combined EPEX intraday annual trading volumes (in TWh)\n\n\nfor the CWE region (AT, BE, DE/LU, FR, NL) taken from the EPEX Spot annual reports (EPEX\n\n\nSPOT SE, 2020-2023).\n\n\ndescribes our implementation of the RI method, as well as the classifier that decides between the two\n\n\nmarkets. Section 4 is devoted to the results of our case study. We first give some insight into stylized\n\n\nfacts related to strategies in order to motivate our choices for the out-of-sample study and then present\n\n\na detailed comparison of our proposed strategy with a set of benchmarks for the years 2023 and 2024.\n\n\nSection 5 concludes the paper.\n\n### **2 Setting & Markets**\n\n\nIn this section, we describe the relevant features of the FCR market and the IDM. To fix ideas and since\n\n\nour numerical study in Section 4 uses Germany as an example, we focus on the German situation as a\n\n\ntypical example of a European market design.\n\n\nIn Figure 1, we show the order and clearing times for the relevant markets. On any given day, the\n\n\nFCR auctions close at 8 am. The IDM market opens at 3 p.m. for the hourly products and at 4 p.m.\n\n\nfor the quarter hourly products. Hence, in our setting, we have to first decide about the bids on the\n\n\nFCR market before the trading on the IDM starts. In the following, we give a detailed description of the\n\n\nrelevant aspects for both markets. Table 1 gives a first impression in terms of size, bidding frequency,\n\n\nand traded volumes.\n\n\n**2.1** **The Market for Frequency Containment Reserve**\n\n\nMaintaining a balance between electricity production and consumption is crucial for the stability of\n\n\nthe power grid. In a situation where production and consumption do not exactly match, the grid\n\n\nfrequency deviates from the nominal frequency, which could damage electronic devices, lead to a failure\n\n\nof components, and may result in selective power cuts or even system-wide blackouts. The FCR is\n\n\n4\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||~~P(f)~~||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\nFigure 2: Required activation of reserve power for a bid of _P_ _bid_ MW as a function of the frequency\n\n\ndeviation ∆ _f_ .\n\n\na service organized by the transmission system operators with the goal of recovering from frequency\n\n\ndeviations and outages by stabilizing frequencies near the nominal value.\n\n\nThe four German transmission system operators hold a joint procurement auction for FCR capacity\n\n\neach day at 8am for the following day. The day is split into six blocks of four consecutive hours, so-called\n\n\nEFA ( _electricity forward agreement_ ) blocks. There are six simultaneous uniform-price procurement\n\n\nauctions, one for each EFA block of the following day, in which companies offer their capacity. The\n\n\nminimum bid size in the auction is 1 MW with increments of 1 MW (Figgener et al., 2022). Divisible\n\n\nand indivisible bids are both allowed, and the maximum size of an indivisible bid is 25 MW.\n\n\nSuccessful bidders for a specific EFA block are required to instantaneously provide an amount of\n\n\npower proportional to the size of the accepted bid and the deviation of the grid frequency _f_ from the\n\n\nnominal frequency _f_ _[n]_ (50 Hz in Germany), defined as ∆ _f_ := _f −_ _f_ _[n]_ . More specifically, FCR market\n\n\nparticipants have the obligation to provide upward and downward regulations if the absolute deviation\n\n\n∆ _f_ of the grid frequency exceeds a deadband of 0.01 Hz: If ∆ _f <_ 0 upward regulation is provided, that\n\n\nis, additional energy must be fed into the grid. In contrast, if ∆ _f >_ 0, that is, the frequency exceeds the\n\n\nnominal frequency, the FCR providers are required to increase energy consumption. The activation _P_\n\n\nresulting from an accepted bid of _P_ bid MW is defined as (see Figure 2),\n\n\n\n0 _,_ if _|_ ∆ _f_ _| ≤_ 0.01 Hz\n\n\n\n_P_ =\n\n\n\n\n\n\n\n∆0 _.f_ 2 _[·][ P]_ [bid] _[,]_ 0.01 Hz _< |_ ∆ _f_ _| ≤_ 0.2 Hz\n\n\n\n(1)\n\n\n\n_|_ ∆∆ _ff_ _−−_ 00 _.._ 22 _|_ _[·][ P]_ [bid] _[,]_ _|_ ∆ _f_ _| >_ 0.2 Hz _,_\n\n\n\nwhere a positive _P_ implies that energy has to be absorbed from the grid while a negative sign indicates\n\n\nthat additional energy has to be provided (Thien et al., 2017).\n\n\nFirms must deliver at least 50% of activation within 15 seconds after receiving the signal ∆ _f_ and\n\n\nshould be fully ramped up/down after 30 seconds. Furthermore, firms have to guarantee an energy\n\n\ncapacity large enough to cover the full activation _P_ bid for at least 15 minutes per incident (Figgener\n\n\net al., 2022). Failure to offer the promised balancing service results in a penalty. If an imbalance persists\n\n\nfor more than 30 seconds, the TSO dispatches secondary reserves that relieve the providers of primary\n\n\nreserve. If there is still a positive _|_ ∆ _f_ _|_ after 5 minutes, the manually controlled tertiary reserve and\n\n\n5\n\n\npossible redispatch mechanisms are used to return the system to equilibrium.\n\n\nIn order to guarantee the 15-minute criterion, the energy stored in the BESS has to remain in the\n\nrange of [0 _._ 25 _P_ bid _,_ _E_ [¯] _−_ 0 _._ 25 _P_ bid ], where _E_ [¯] is the battery’s energy capacity. However, there may be\n\n\nmore than one frequency excursion event in an EFA block and participants are obliged to provide the\n\n\nservice for all incidents. Therefore, after each FCR activation, the providers must restore their SoC\n\n\nto an acceptable range that allows them to service the next event. The above also implies an upper\n\nbound 0 _._ 25 _P_ bid _≤_ 0 _._ 5 _E_ [¯] for the bid power, which is, however, rarely of practical relevance since the power\n\n\ncapacity (in MW) usually does not exceed the energy capacity (in MWh) for BESS.\n\n\nClearly, at the time of bidding capacities for the FCR market, the direction and magnitude of _P_ is\n\n\nuncertain. For storage, this has two implications: Firstly, the operators have to ensure that the SoC is\n\n\nin a range where it is guaranteed that they can honor their commitments made on the FCR. Secondly,\n\n\nalthough cumulative activations are, in most periods, close to zero, it often happens that the SoC at the\n\n\nend of a 4-hour EFA block significantly differs from the initial SoC, i.e., that activations do not average\n\n\nout. This effect is further exacerbated by the fact that battery efficiency losses (charging and discharging\n\n\ninefficiencies) result in a net energy loss even if positive and negative FCR activations within an EFA\n\n\nblock cancel out on average.\n\n\n**2.2** **The Intraday Market**\n\n\nThe European intraday market framework is designed around the principle of market integration and\n\n\nconvergence toward a single European electricity market. The European Commission established a\n\n\ntarget model to integrate all intraday markets based on continuous trading through the Commission\n\n\nRegulation (EU) 2015/1222 establishing guidelines on _capacity allocation and congestion management_ .\n\n\nThis integration is facilitated by the Cross-Border Intraday (XBID) platform, which operates with a\n\n\nshared order book that enables market participants to trade electricity continuously across European\n\n\nborders, provided that sufficient cross-border transmission capacity is available. The aim of this _single_\n\n\n_intraday coupling_ is to create a single EU cross-zonal intraday electricity market where buyers and\n\n\nsellers can work together across Europe to trade electricity continuously on the day the energy is needed\n\n\n(European Commission, 2015). It serves as a complementary market to the day-ahead market and is\n\n\nused to continuously buy or sell electricity to manage short-term deviations from sudden changes in\n\n\nenergy supply and demand.\n\n\nThe intraday market in Germany is mainly organized by the European Power Exchange (EPEX),\n\n\nthe largest intraday power exchange in Europe. In recent years, the intraday market has been growing\n\n\nsteadily, as can be seen from the traded volumes in Table 1. The market is organized as an order book\n\nbased continuous trading market that features hourly, half-hourly, quarter-hourly, and block products.\n\n\nCurrently, market participants in Germany can trade until 5 minutes before physical delivery in their\n\n\ncontrol areas and until up to 30 minutes before delivery in a combined national market (Kuppelwieser\n\n\nand Wozabal, 2023).\n\n\nEach buy and sell order on the intraday market for a given product contains basic information about\n\n\nquantity, limit price, and validity time. A _market order_ is cleared immediately against the best available\n\n\norder in the limit order book (LOB), while a limit order is only executed with matching orders on the\n\n\nother side of the market up to a certain price (the limit). If this is not possible, the order is kept in the\n\n\nLOB until its _end validity date_ to be cleared with future orders. If the quantities of two matched orders\n\n\ndo not agree, the order with the higher order quantity is only partially cleared and remains in the order\n\n\n6\n\n\n_T_ = 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n_T_ = 1\n\n\n\n\n\n\n\n_T_ = 2\n\n\n\n_T_ = 1\n\n\n\n\n\n_T_ = 2\n\n\n\nFigure 3: Illustration of the limit order book clearing mechanism. Source (Graf et al., 2024).\n\n\nbook with the remaining quantity. The minimum bid volume is 0 _._ 1 MW, and the price per MWh ranges\n\n\nfrom _−_ 9999 to 9999 euros.\n\n\nMarket participants can add the usual order qualifiers such as _immediate-or-cancel_ (IOC) or _fill-or-_\n\n\n_kill_ (FOK). Additionally, _iceberg_ orders are allowed for which only a fraction of the order quantity is\n\n\nvisible to other market participants. As soon as the visible quantity is cleared, the next part of the order\n\n\nis automatically placed in the limit order book.\n\n\nThe basic mechanism of continuous trading is illustrated in Figure 3 by a concrete example (taken\n\n\nfrom Graf et al., 2024): In the left panel, the state of the order book at _T_ = 1 is displayed with the\n\n\norders sorted according to their limit price and organized into a _bid stack_ and _ask stack_ . The state of\n\n\nthe LOB changes with the placement of a new order, with the modification of an order, and at the end\n\nvalidity-time of an active order. The limit price of the order with the lowest sell price is called _best-ask_,\n\n\nwhile the order with the highest buy price defines the _best-bid_, and the difference between the two prices\n\n\nis the _bid-ask-spread_ . In the right panel, the clearing of a new hypothetical buy order is illustrated: A\n\n\nnew buy order with a price of 50e is added to the book at _T_ = 2 and is then cleared against the cheapest\n\n\npossible offers until either the whole order is fulfilled (as is the case in the figure) or there are no offers\n\n\nwith lower prices left. In this example, 22 MW out of 29 MW are cleared against the sell order with\n\n\nprice 37e and the remaining 29 MW _−_ 22 MW = 7 MW are cleared against the sell order with price\n\n\n45e. The remaining quantity of 12 MW _−_ 7 MW = 5 MW of the latter order stays in the order book.\n\n\nNote that the clearing is instantaneous, i.e., columns 2 and 3 in the right panel are purely illustrative\n\n\nand do not correspond to market states that can be observed by traders.\n\n### **3 A Joint Optimization Method for Intraday FCR Trading**\n\n\nParticipating in both the FCR and the IDM involves (i) bidding in the FCR auction one day ahead\n\n\nof delivery and (ii) trading in the IDM while honoring the battery’s FCR commitments. Finding an\n\n\noptimal combined bidding strategy for the FCR and the IDM thus involves solving a fairly complicated\n\n\nstochastic optimization with a large number of intraday trading stages that model the possibility of\n\n\nfrequent rebalancing. Solving this problem exactly is computationally intractable.\n\n\nWe therefore propose a heuristic policy that selects one of finitely many FCR bidding strategies and\n\n\n7\n\n\nthen uses the RI policy to solve the intraday bidding problem. To that end, we frame the problem of\n\n\nselecting an optimal FCR strategy as a classification problem that picks the strategy with the highest\n\n\noverall expected profits.\n\n\nWe discuss the strategy space for the FCR market in Section 3.1, a version of the rolling intrinsic\n\n\npolicy adapted to continuous trading in Section 3.2, and finally the classification problem for the different\n\n\nstrategies in Section 3.3.\n\n\n**3.1** **Overview of Joint Bidding Model**\n\n\nFor each EFA block _i_, suppliers can bid the price (EUR/MW) as the requested compensation for the\n\n\nservice. In this paper, we assume that the bidding strategy only includes the quantity and that the\n\n\nbidding price is zero. Due to the pay-as-cleared mechanism, even with the bidding price being zero,\n\n\nproviders still get the uniform market clearing price as their final remuneration.\n\n\nMore specifically, we use a vector _X_ = ( _X_ 1 _, X_ 2 _, X_ 3 _, X_ 4 _, X_ 5 _, X_ 6 ) with six components to represent a\n\n\nbid strategy in the six FCR markets, where _X_ _i_ denotes the capacity (in MW) that is committed to the\n\nmarket _i_ . For a battery with capacity _X_ [¯], this implies that in principle ( _X_ [¯] _−_ _X_ _i_ ) MW are available for\n\n\ntrading on the intraday market in the time period covered the EFA block _i_ . Hence, the FCR strategy\n\n\ndetermines the feasible bids on the intraday market.\n\n\nThe regulation of the FCR market in Germany stipulates that a storage can provide the maximum\n\n\nup-regulation or down-regulation of the full reserve bid _X_ _i_ for at least 15 minutes without interruptions\n\n(see Section 2). Therefore, the storage level must be in [ _[X]_ _[i]_ _/_ 4 _,_ _E_ [¯] _−_ _[X]_ _[i]_ _/_ 4 ] at any time during the period\n\n\ncovered by the market _i_ . Furthermore, we limit the maximum FCR bid to 80% of the asset’s power\n\n\ncapacity. Note that this restriction is not required by market rules but is based on backtesting evidence,\n\n\nwhich shows that larger bids often caused SoC management issues. This assumption is reasonable for\n\n\nBESS with a power-to-energy ratio, as considered in this study (approximately 1).\n\n\nAs described in Section 2.1, admissible bids are in whole MW with a minimum bid size of 1 MW.\n\n\nHence, for a storage with capacity _X_, there are _⌊_ 0 _._ 8 _· X_ + 1 _⌋_ [6] different admissible bidding strategies for\n\n\nthe FCR markets of a single day in our setting.\n\n\nIf _X_ _i_ = 0, the lower limit on stored energy in the corresponding time periods _t_ covered by market _i_\n\nequals _α_ _×_ _E_ [¯] while the upper limit is ~~_α_~~ _×_ _E_ [¯] . These limits are in place to preserve battery health and to\n\n\navoid complete discharge as well as overcharging. In case that _X_ _i_ _>_ 0 these bounds tighten according to\n\n\nthe rules outlined above. Hence, for any time _t_ in the time span covered by market _i_, we get that the\n\n\nstate of charge _c_ _t_ has to be in the interval\n\n\n[ _c_ _t_ _,_ ~~_c_~~ _t_ ] := �max( _α_ _×_ _E,_ [¯] _[X]_ _[i]_ _/_ 4 ) _,_ min ~~(~~ ~~_α_~~ _×_ _E,_ [¯] _X_ [¯] _−_ _[X]_ _[i]_ _/_ 4 )� _._ (2)\n\n\nThis limitation is incorporated into the RI as a constraint as described in the next section. If the\n\n\nFCR activation causes the battery charge to exceed the boundary, the constraint requires the RI to\n\n\nimmediately restore the SoC to acceptable limits by corresponding IDM trades.\n\n\nAlthough positive and negative deviations from activations on the FCR market tend to cancel out,\n\n\nparticipation in the market can produce a slight drift in the state of charge of a storage. This is\n\n\nacerbated by the efficiency losses of storage. The energy drift in the time interval [ _t, t_ + ∆ _t_ ] is dependent\n\n\non fluctuations of the national grid frequency and can be expressed as\n\n\n\n_Kt_ [)] _[η]_ [(] _[P]_ [(∆] _[f]_ _[t]_ [+] _[k]_ [∆] _K_ _[t]_\n\n\n\n_t_ +∆ _t_\n_D_ _t,t_ +∆ _t_ =\n� _t_\n\n\n\n_P_ (∆ _f_ _s_ ) _η_ ( _P_ (∆ _f_ _s_ )) _ds ≈_ [∆] _[t]_\n_t_ _K_\n\n\n\n_K_ _[t]_ [))] _[,]_ (3)\n\n\n\n_K_\n\n\n\n_K_\n�\n\n\n\n� _P_ (∆ _f_ _t_ + _k_ ∆ _Kt_\n\n_k_ =1\n\n\n\n8\n\n\nwhere _P_ is defined in (1) and ∆ _f_ _s_ is the deviation of the national frequency at time _s_ . Furthermore, _η_\n\n\nmaps the positive and negative deviations to the charging and discharging efficiency, respectively, and _K_\n\n\nis a positive integer defining the discretization grid in the approximation of the integral by a Riemann\n\n\nsum.\n\n\nWhen making a bidding decision on the FCR market, the BESS trades off revenues for every EFA\n\n\nblock with the revenues that can be potentially earned on the intraday market. The total daily profits\n\n\n_π_ are the sum of the profits _π_ _F CR_ from bidding on the FCR market and the profits _π_ _IDM_ from trading\n\n\non the intraday market.\n\n\n\n_π_ ( _X_ ) =\n\n\n\n6\n� _P_ _F CR_ _[k]_ _[X]_ _[k]_\n\n\n_k_ =1\n\n\n� ~~�~~ � ~~�~~\n_π_ _F CR_ ( _X_ )\n\n\n\n+ _π_ _IDM_ ( _X_ ) _,_ (4)\n\n\n\nwhere _P_ _F CR_ _[k]_ [is the clearing price of the FCR for the EFA block] _[ k]_ [ and the profits profits on the IDM are]\n\na function of _X_ as will be discussed in the next section. Since neither _P_ _F CR_ _[k]_ [nor] _[ π]_ _[IDM]_ [(] _[X]_ [) is known at]\n\n\nthe time of bidding on the FCR market, _π_ ( _X_ ) is a random quantity.\n\n\nTherefore, in order to select an FCR strategy for the six markets of one day, we aim to solve the\n\n\nmulti-stage stochastic optimization problem\n\n\nmax (5)\n_X∈X_ [E][[] _[π]_ [(] _[X]_ [)]] _[,]_\n\n\nwhere bids _X_ are the first-stage decisions, _X_ is the feasible set, and the bids on the IDM represent the\n\n\nrecourse decisions. Since the intraday market is continuous, the problem has an excessive amount of\n\n\nstages and therefore solving (5) exactly is computationally intractable.\n\n\n**3.2** **The Continuous Rolling Intrinsic Strategy for Intraday Trading**\n\n\nThe intrinsic value of a storage asset is the profit that can be realized by exploiting the currently\n\n\nobservable price spreads in the market. A static intrinsic strategy therefore determines an optimal set\n\n\nof buy and sell positions based on the forward curve at a single point in time. This strategy focuses on\n\n\nlocking in guaranteed profits without speculating on future price movements.\n\n\nThe RI policy, originally introduced in (Gray and Khandelwal, 2004) for the evaluation of gas storage,\n\n\nis a dynamic extension of the intrinsic value. Starting from an initial SoC and a portfolio of forward\n\n\npositions acquired in previous periods on the intraday market, the RI repeatedly checks for chances\n\n\nof profitable rebalancing by re-solving the intrinsic problem. Although the resulting decisions are still\n\n\nmyopic, since the policy at no point trades on anticipated future price changes, the RI clearly represents\n\n\nan improvement over the static intrinsic that does not adapt positions at all. Furthermore, the myopic\n\n\nnature of decisions has the advantage that the RI only enters immediately profitable positions and\n\n\ntherefore does not run the risk of accumulating losses based on wrong assumptions about the future. For\n\n\nthese reasons and because of its conceptual simplicity and low computational cost, the RI has attained\n\n\nwidespread industry adoption.\n\n\nThe classic version of the RI is based on a price taking intrinsic strategy. To be able to employ the\n\n\nalgorithm to the continuous intraday market, we adapt the intrinsic algorithm to explicitly take into\n\n\naccount the LOB and corrections based on energy drift from FCR market activations when trading. In\n\n\nparticular, for every rebalancing decision, the algorithm receives a snapshot of the current state of the\n\n\norder book and subsequently checks whether the positions of forward contracts built up in previous steps\n\n\ncan be profitably updated.\n\n\n9\n\n\nIn order to discuss the RI, we first formally define the intrinsic problem for continuous intraday\n\n\nmarkets, which is solved repeatedly in the process. To that end, we define the set of tradeable contracts\n\n\n\n_T_ at the time the intrinsic policy is executed, as well as the order book information _O_ = [�]\n\n\n\n_t∈T_ _[O]_ _[t]_ [, which]\n\n\n\ncontains order numbers of all bids and asks currently in the orderbook _O_ _t_ for contract _t_ . In addition, we\n\n\ndefine the set of contracts that go into delivery on the current day as _T_ 0 . For an order _i ∈O_, we denote\n\n\nby _P_ _i_ its limit price (in EUR/MWh), by _σ_ _i_ its direction ( _−_ 1 for bid and 1 for ask), and finally by _Q_ _i_\nthe quantity. In order to capture the history of trading decisions, we denote by _c_ 0 and _b_ [0] _t_ [the energy and]\n\n\npower commitments resulting from previous trades where _c_ 0 is adjusted for the energy drift that results\n\n\nfrom call offs on the FCR market and by products that go into delivery (see below). To model the cost\n\n\nof battery degradation, we use a simple linear approximation by multiplying the absolute magnitude of\n\n\nplanned charging/discharding by the degradation cost _κ >_ 0 (in EUR/MWh).\n\n\nNote that the last tradeable contract _t_ is always the last period traded in a day. We fix a terminal\n\n\nstorage level _C_ _T_ at the end of a trading day, i.e., at time _T_ = max _T_ . With these preparations in place,\n\n\nwe can formulate the intrinsic problem, which decides for every order _i ∈O_ about the quantity _q_ _i_ (in\n\n\nMW) that is matched by the intrinsic policy as follows.\n\n\n\nmax ∆\n�\n_q_ _i_ _,c_ _t_ _,b_ _t_ _,b_ _[±]_ _t_ _t∈T_\n\n\n\n� _P_ _i_ _σ_ _i_ _q_ _i_ _−_ _κ_ ∆ �\n\n_i∈O_ _t_ _t∈T_\n\n\n\n�\n\n\n\n�( _b_ [+] _t_ [+] _[ b]_ _[−]_ _t_ [)] (6a)\n\n_t∈T_\n\n\n\ns.t. 0 _≤_ _q_ _i_ _≤_ _Q_ _i_ _,_ _∀i ∈O,_ (6b)\n\n_b_ _t_ = _b_ [0] _t_ _[−]_ � _σ_ _i_ _q_ _i_ _,_ _∀t ∈T,_ (6c)\n\n_i∈O_ _t_\n\n\n_b_ _t_ = _b_ [+] _t_ _[−]_ _[b]_ _[−]_ _t_ _[,]_ _∀t ∈T,_ (6d)\n\n\n_b_ [+] _t_ _[≤]_ _[δ]_ _[t]_ _[b]_ _[t]_ _[,]_ _∀t ∈T,_ (6e)\n\n\n_b_ _[−]_ _t_ _[≤]_ [(1] _[ −]_ _[δ]_ _[t]_ [)(] _[−][b]_ ~~_t_~~ [)] _[,]_ _∀t ∈T,_ (6f)\n\n\n\n� _η_ _ch_ _b_ [+] _a_ _[−]_ [∆] �\n\n_a≤t_ _a≤t_\n\n\n\n_c_ _t_ = _c_ 0 + ∆ �\n\n\n\n_a≤t_\n\n\n\n1\n_b_ _[−]_ _a_ _[,]_ _∀t ∈T,_ (6g)\n_η_ _dis_\n\n\n\n_b_ ~~_t_~~ _≤_ _b_ _t_ _≤_ _b_ _t_ _,_ _∀t ∈T,_ (6h)\n\n\n_c_ ~~_t_~~ _≤_ _c_ _t_ _≤_ ~~_c_~~ _t_ _∀t ∈T,_ (6i)\n\n\n_b_ [+] _t_ _[, b]_ _[−]_ _t_ _[≥]_ [0] _[,]_ _∀t ∈T_ (6j)\n\n\n_δ_ _t_ _∈{_ 0 _,_ 1 _},_ _∀t ∈T_ (6k)\n\n\n_c_ _T_ = _C_ _T_ _,_ (6l)\n\n∆ [�] _t∈T_ [(] _[b]_ _t_ [+] [+] _[ b]_ _[−]_ _t_ [)] _≤_ _C_ [¯] [0] _,_ (6m)\n\n2 _E_ ~~[¯]~~\n\n\nwhere ∆is the duration of the delivery period of the traded contracts (e.g., one hour or 15 minutes),\n\n_b_ _t_, ~~_c_~~ _t_ _∈_ R [+], _b_ ~~_t_~~ _∈_ R _[−]_, _c_ ~~_t_~~ _∈_ R [+] are upper and lower bounds on stored energy and charged discharged\n\n\npower in period _t ∈T_ (see discussion above), respectively and _η_ _ch_ _, η_ _dis_ _<_ 1 is the charging/discharging\n\n\nefficiency factor (depending on the direction of the trade). Note that the bounds on power and energy\n\n\ndepend on the commitments on the FCR market for the corresponding EFA block (see last section).\n\n\nThe variables _b_ _t_ defined in (6c) model the accumulated power (in MW) of all orders with the delivery\n\nperiod _t_ and depend on the corresponding values from previous trades _b_ [0] _t_ [and the decisions in the current]\n\nproblem, where _b_ [+] _t_ _[, b]_ _[−]_ _t_ are the positive and negative parts of _b_ _t_, respectively, as modeled by (6d) to\n\n\n(6f), where the constraints make sure that for every product energy is either bought or sold, which is\n\n\nimportant for contracts with negative prices where simultaneous charging and discharging is potentially\n\n\n10\n\n\nprofitable, depending on efficiencies and the observed bid/ask spread. Similarly, the variables _c_ _t_ in (6g)\n\n\ndefine the total energy stored in the storage until the end of period _t_ . Note that the factor ∆translates\n\n\npower (in MW) into energy (in MWh). The constraints (6h) and (6i) represent the limits for power and\n\n\nenergy imposed by the physical limitations of the storage as well as commitments on the FCR market.\n\n\nThe last constraint (6l) requires that the storage level is equal to _C_ _T_ at the end of the planning horizon.\n\nThe constraint (6m) enforces a daily cycling limit, where _C_ [¯] [0] is the number of permissible cycles left\n\n\non the current day, which is updated in the RI Algorithm 1. Finally, the first term of the objective\n\n\nfunction (6a) models the profits from trading, while the second term yields the planned losses from\n\n\nbattery degradation.\n\n\nNote that while on the intraday market different types of contracts are traded (hourly, half-hourly,\n\n\nquarter-hourly), we only consider one type in the above formulation. Clearly, at the expense of a more\n\n\ninvolved notation, the problem could easily be extended to cover different types of contracts. However,\n\n\nfor the sake of simplicity and because we only use one type of contract in our numerical study, we refrain\n\n\nfrom this complication.\n\n\nThe intrinsic problem (6) is the main building block of the rolling intrinsic policy, which is detailed\n\n\nin Algorithm 1. The RI reoptimizes a given position either periodically or every time there is a change\n\n\nin the LOB. The policy is myopic in the sense that trades are only executed if the resulting immediate\n\n\nprofit is positive. This leads to profits that are guaranteed to be non-negative and a strategy that is\n\n\nnot able to anticipate future prices foregoing potential profits of more speculative strategies but also\n\n\neliminating any downside risks.\n\n\nNote that the RI profits are not taken from the objective of the problem (6), but calculated outside of\n\n\nthe optimization problem in Algorithm 1. The reasons for this are twofold: Firstly, to not count battery\n\n\ndegradation costs for the same periods multiple times, the degradation cost is accounted for in lines 6\n\n\nand 20 for the periods that are no longer traded and go into physical delivery. Secondly, some of the\n\n\ntrades found by the linear program (6) may not be feasible, since they are not forced to be multiples\n\n\nof the minimum bid size _δ_ . Note that this happens only for the most expensive accepted ask and the\n\n\ncheapest accepted bid per period. We correct for these inaccuracies in line 14.\n\nOn a related note, we remark that the initial schedule ( _c_ [0] _t_ [)] _[t][∈T]_ [for any of the intrinsic solves need not]\n\n\nbe physically feasible due to the rounding after the solution of the last problem, as well as due to the\n\n\nenergy drift that occurs between two consecutive runs of the intrinsic policy. Everytime (6) is solved,\n\n\nthese violations are corrected. If there are not enough orders in the LOB for correction, the problem (6) is\n\n\npotentially infeasible. Hence, the if-statement in line 13. If such as situation persists over a longer period\n\n\nof time, we theoretically could end up with a physically infeasible schedule that has to be corrected on\n\n\nthe balancing market. However, in Section 4, we demonstrate that this case does not occur in practice\n\n\nfor sufficiently conservative limits to storage operation.\n\n\n**3.3** **Strategy selection as a classification task**\n\n\nIn this section, we describe how we approximate (5). The main idea is to simplify the problem by first\n\n\ndeciding among the finitely many feasible _X ∈X_ and then relegating the recourse decisions to the rolling\n\n\nintrinsic alogrithm.\n\n\nTo be able to choose among the strategies in _X_, we fit a classification model with the objective of\n\n\npicking the strategy _X ∈X_ that, paired with the RI trading strategy in the IDM, produces the highest\n\n\naverage profits. We base the predictor on a supervised learning method that uses a set of features and\n\n\n11\n\n\n**Data:** Storage level _C_ 0, planned final storage level _C_ _T_, _T_, minimal bid size _δ_\n\n**1** _∀t ∈T_ : _b_ [0] _t_ _[←]_ [0,] _[ c]_ 0 _[←]_ _[C]_ 0 [;]\n\n\n**2** _π ←_ 0, _T_ _[−]_ _←T_ ;\n\n\n**3** **while** _T ̸_ = _∅_ **do**\n\n```\n   /* Correct for drift and calculate realized battery degradation cost */\n\n```\n\n**4** Set ∆ _T ←T_ _[−]_ _\\ T_, fetch order books _O_ _t_ and FCR SoC drift _D_ since last optimization;\n\n\n\n_t∈_ ∆ _T_ _[η]_ _[ch]_ _[b]_ _t_ [+] _[−]_ [∆] [�]\n\n\n\n**5** _c_ 0 _←_ _c_ 0 – _D_ + ∆ [�]\n\n\n\n1\n_t∈_ ∆ _T_ _η_ _dis_ _[b]_ _t_ _[−]_ [;]\n\n\n\n**6** _π ←_ _π −_ ∆ _κ_ [�] _t∈_ ∆ _T_ _[|][b]_ _t_ [0] _[|]_ [;]\n\n\n```\n   /* Update cycling limit */\n\n```\n\n**7** **if** _a new day has started since the last solve_ **then**\n\n**8** _C_ ¯ [0] _←_ _N_ _[cycles]_ ;\n\n\n**9** **else**\n\n**10** _C_ ¯ [0] _←_ _C_ ¯ [0] _−_ ∆ [�] _t∈_ 2 _E_ ∆ ~~[¯]~~ _T_ _[|][b]_ _[t]_ _[|]_ ;\n\n\n**11** **end**\n\n```\n   /* Initialize new contracts */\n\n```\n\n**12** _b_ [0] _t_ _[←]_ [0 for all] _[ T \\ T]_ _[ −]_ [;]\n\n```\n   /* Solve the intrinsic and round results to implementable trades */\n\n```\n\n**13** Solve the intrinsic problem resulting in optimal ( _b_ _t_ ) _t∈T_, ( _c_ _t_ ) _t∈T_ and ( _q_ _i_ ) _i∈O_ ;\n\n\n**14** **if** _intrinsic is feasible_ **then**\n\n\n**15** _q_ _i_ _←_ round( _q_ _i_ _/δ_ ) _× δ,_ _∀i ∈O_ ;\n\n\n**16** _π ←_ _π_ + ∆ [�] _i∈O_ _[P]_ _[i]_ _[σ]_ _[i]_ _[q]_ _[i]_ [;]\n\n**17** _b_ [0] _t_ _[←]_ _[b]_ [0] _t_ _[−]_ [�] _i∈O_ _t_ _[σ]_ _[i]_ _[q]_ _[i]_ _[,]_ _∀t ∈T_ ;\n\n\n**18** **end**\n\n```\n   /* Prepare next iteration */\n\n```\n\n**19** _T_ _[−]_ _←T_, wait for next trading time, and update _T_ ;\n\n\n**20** **end**\n\n\n**21** _π ←_ _π −_ ∆ _κ_ [�] _t∈T_ _[|][b]_ _[t]_ _[|]_ [;]\n\n\n**Algorithm 1:** Rolling-intrinsic algorithm for one day of trading on the intraday market.\n\n\nis calibrated using historical data. More specifically, for every day _d ∈D_ in our training data, we have\n\n\na feature vector _f_ _d_ and profits _π_ _d_ ( _X_ ) as calculated in (4) dependent on the FCR strategy and on IDM\n\n\ntrading by the RI.\n\n\nThus, we frame the optimal strategy selection for the FCR market as a classification task where the\n\n\nfeatures _f_ _d_ are used to forecast the labels _X_ _d_ _[∗]_ [with] _[ X]_ _d_ _[∗]_ [the strategy] _[ X]_ [ that maximizes trading profits on]\n\n\nday _d_, i.e.,\n\n\n_X_ _d_ _[∗]_ [= arg max] _[ {]_ [E][[] _[π]_ _[d]_ [(] _[X]_ [)] :] _[ X][ ∈X}][ .]_\n\n\nTo do so, we fix a hypothesis class _H_ such that _h_ : _F →X_ for every _h ∈H_, where _F_ is the feature space\n\n\nand solve\n\n\n\n� _L_ ( _X_ _d_ _[∗]_ _[, h]_ [(] _[f]_ _[d]_ [))] _[,]_ (7)\n\n_d∈D_\n\n\n\n_h_ _[∗]_ = arg max\n_h∈H_\n\n\n\n1\n\n_|D|_\n\n\n\nwith _L_ the cross entropy loss.\n\n\nNote that in order to calculate the labels of the above problem, we have to evaluate the profits _π_ _d_ ( _X_ )\n\n\nfor every strategy _X ∈X_ for all days _d ∈D_ in the training data. However, evaluating IDM trading with\n\norder book data for every of the _⌊_ 0 _._ 8 _· X_ + 1 _⌋_ [6] candidate FCR strategies is computationally expensive.\n\n\nFor this reason and in order to limit the potential class imbalance in (7), we restrict the set _X_ of possible\n\n\n12\n\n\nstrategies.\n\n\nIn order to do this, we proceed in two steps. In a first step, we limit the set _X_ to a reduced set of\n\n\nstrategies _M ⊆X_ based on our analysis of the interaction between the revenues that can be generated\n\n\nin the two markets in Section 4. We evaluate profits for all days _D_ and all strategies _X ∈M_ . To further\n\n\nreduce class imbalance, we reduce the strategy space to _S ⊆M_ base strategies. An approach to do\n\n\nthis would be to simply select a fixed set of strategies from _M_ that yield the highest average profits.\n\n\nHowever, this does not necessarily yield an optimal subset of strategies, since rather similar strategies\n\n\nmight be chosen instead of designing _S_ to consist of complementary strategies. [1]\n\n\nIn order to find a well performing yet small strategy pool set of size _S < |M|_, we pick a subset _S ⊆M_\n\n\nproducing the highest overall profit on the training data under the hypothesis of perfect classification,\n\n\ni.e., under the assumption that for every day the best strategy _X ∈S_ is chosen by the classifier. We do\n\n\nthis by solving the following mixed integer linear program\n\n\n\n� _w_ _Xd_ _π_ _d_ ( _X_ ) (8a)\n\n_d∈D_\n\n\n\nmax\n\n_z_ _X_ _,w_ _Xd_\n\n\n\n�\n\n_X∈M_\n\n\n\ns.t. _z_ _X_ _∈{_ 0 _,_ 1 _},_ _X ∈M_ (8b)\n\n\n_w_ _Xd_ _∈_ [0 _,_ 1] _,_ _∀X ∈M, ∀d ∈D_ (8c)\n\n\n_w_ _Xd_ _≤_ _z_ _X_ _,_ _∀X ∈M, ∀d ∈D_ (8d)\n� _w_ _Xd_ = 1 _,_ _∀d ∈D_ (8e)\n\n_X∈M_\n\n� _z_ _X_ = _S._ (8f)\n\n_X∈M_\n\n\nIn the above problem, the variables _z_ _X_ determine whether a strategy is in _S_ and constraint (8f) enforces\n\n\nthat only _S_ strategies can be chosen. The variables _w_ _Xd_ model whether a strategy _X_ is chosen for day\n\n\n_d_ . Note that since the objective is linear in _w_ _Xd_ the optimal values of _w_ _Xd_ will be in _{_ 0 _,_ 1 _}_ and the sum\n\n\nin (8e) ensures that only one strategy is chosen per day. Clearly, if _z_ _X_ = 0, _w_ _Xd_ = 0 for any day _d_ due\n\n\nto (8d). Given the optimal solution of the above problem, we define\n\n\n_S_ = _{X ∈M_ : _z_ _X_ = 1 _}_\n\n\nand define our labels as _X_ _d_ _∈_ arg max _{π_ _d_ ( _X_ ) : _X ∈S}_ .\n\n\nIn order to solve the ensuing classification problem based on the data ( _X_ _d_ _, f_ _d_ ) _d∈D_ as our training\n\n\nset, we use _XGBoost_ (Chen and Guestrin, 2016), a gradient boosting method based on decision trees.\n\n\nEnsemble methods such as gradient boosting are the current state of the art for tabular data and often\n\n\noutperform other methods (e.g. Shwartz-Ziv and Armon, 2022; Grinsztajn et al., 2022) and specifically\n\n\nXGBoost has shown remarkable performance in machine learning competitions since its inception and\n\n\noffers high computational efficiency, the ability to manage class imbalance, and has powerful feature\n\n\nselection capabilities.\n\n\n1 To see this consider the case where _|M|_ = 3 and _|S|_ = 2 and where there are two strategies _X_ 1 and _X_ 2 that perform\n\nbest on average but are rather similar as well as a third strategy _X_ [3] which yields good results on those days where _X_ [1]\n\nand _X_ [2] have poor performance. In this case, it is clearly advantageous to choose _S_ = _{X_ [1] _, X_ [3] _}_ as a set of complementary\n\n\nstrategies which contains a good strategy for every day rather than simply choosing the best two strategies.\n\n\n13\n\n\n### **4 A Numerical Case Study**\n\nThis section discusses a numerical out-of-sample study that puts the method developed in the previous\n\n\nsections to the test. Throughout we assume a 10MW/10MWh battery, i.e., a battery with a power\n\n\ncapacity of 10MW and an energy capacity of 10MWh and use _α_ = 0 _._ 01 and ~~_α_~~ = 0 _._ 985 to define SoC\n\n\nlimits. We use historical limit order books from the German continuous intraday market from EPEX\n\n\nSpot and historical FCR clearing prices from Regelleistung.net from 01/01/2023 until 30/09/2024 for\n\n\nour study.\n\n\nWe begin by investigating some static FCR strategies _X_, which means a constant allocation of battery\n\n\npower to the FCR and IDM markets for every day of the observation period. We show that, contrary\n\n\nto intuition, revenue per market does not change linearly with the allocated battery power. We also\n\n\nexplain what drives the observed relationship. Having built some intuition around the optimal allocation\n\n\nof battery power between FCR and IDM, we then move to the results of the dynamic FCR strategy as\n\n\na classification problem.\n\n\n**4.1** **Decreasing Intraday Returns**\n\n\nWe start by discussing the fact that per MW revenues in the IDM are a decreasing function of the\n\n\namount of battery capacity left for intraday trading. There are essentially two reasons for this: Firstly,\n\n\nlimited liquidity of the IDM favors strategies with less capacity, since these do not experience a large\n\n\nprice response when trading (Kuppelwieser and Wozabal, 2021). Secondly, the less capacity is committed\n\n\non the FCR market the higher the duration of the remaining storage that can be traded on the intraday\n\n\nmarket, which tends to decrease profits per unit of capacity. We will discuss this less obvious aspect\n\n\nbelow.\n\n\nThe battery power allocated to the FCR market determines not only the battery power left on for\n\n\nIDM trading, but also the energy capacity of the battery still available for use on the IDM market. In\n\n\nparticular, the FCR regulation states that the battery must at all times be able to delivery the committed\n\n\npower for at least 15 minutes in both directions (see discussion in Sections 2 and 3). In our case this\n\n\nmeans that with a commitment of 8 MW in the FCR market the 10MW/10MWh battery has to maintain\n\n\na charge level between 2MWh and 8MWh. This leaves 2MW and 6MWh of flexibility on the IDM market,\n\n\nincreasing the energy to power ratio from 1 (of the original 10MW/10MWh battery) to 3 on the IDM\n\n\nmarket.\n\n\nThe increase in total profits from intraday trading when duration changes depends on the price\n\n\npatterns. The two most extreme cases are depicted in Figure 4: The left panel shows a situation where\n\n\nthe prices in the first 12 hours of the day are low, enabling the storage to continuously charge in that\n\n\ntime and sell back the energy in the second half of the day for high prices. In this case, a perfectly\n\n\nefficient battery with a duration of 12 hours is optimal. Contrast this with the situation in the right\n\n\npanel, where prices alternate between high and low prices every hour and the storage would only charge\n\n\nfor 1 hour before discharging again and therefore durations above 1 do not yield additional profits.\n\n\nIn reality, price patterns fall between the two extremes depicted in Figure 4, showing a more nuanced\n\n\neffect of duration on profits. We explore this relationship in Figure 5 by computing the profits of a\n\n\n10MW storage with varying energy capacity resulting in different storage duration. The experiment is\n\n\ncarried out with price data from January 2023. In the experiment, the energy capacity varies from 10\n\n\nMWh, that is, a duration of 1, to 60 MWh, i.e., a duration of 6.\n\n\nThe left panel of Figure 5 shows how the duration of the storage on the IDM increases with the size\n\n\n14\n\n\nFigure 4: Two extreme price patterns for one day.\n\n\n\n\n\n\n\nFigure 5: The left panel shows the relationship between the bid size in the FCR and the duration ratio,\n\n\nwhile the right panel depicts the how the capacity to power ratio affects the profits in the IDM.\n\n\nof the FCR bid and the right panel shows the connection between duration and overall average profits for\n\n\none day of trading with the rolling intrinsic. This analysis reveals that there is a concave and increasing\n\n\nrelationship between duration and per-MW revenue on the IDM.\n\n\nOverall, this demonstrates that due to the way power and capacity are regulated in the FCR market,\n\n\nrevenues in the IDM market fall more slowly than may naively be expected when increasing the battery\n\n\npower committed in the FCR market. Since FCR revenues per MW are constant in the FCR bid size,\n\n\nit also shows that strategies with a higher power committed to FCR tend to be more attractive for the\n\n\n10MW/10MWh battery under consideration.\n\n\n**4.2** **Pre-selection of FCR Strategies**\n\n\nIn our case, there are in total 9 [6] different possible FCR bids, making it impractical to backtest all of\n\n\nthem. The findings in the last section and initial experiments show that allocation of a large proportion\n\n\nof the BESS’ capacity to the FCR market is preferable. However, the afternoon and evening EFA blocks\n\n\nfour, five, and six can on some days exhibit substantial volatility with big price spikes in the intraday\n\n\nmarket. In these instances having more power capacity to trade in the IDM market to benefit fully from\n\n\nthese spikes may outweigh the lost revenue in the FCR market.\n\n\nBased on these findings, we choose the 28 strategies listed in Table 2 as the set _M_ for backtesting.\n\n\nAll strategies are computed by reoptimizing once every minute in the rolling intrinsic strategy with a\n\n\n15\n\n\n**2023** **2024**\n\n\n**Strategy** **Overall** **IDM** **FCR** **Best** **Overall** **IDM** **FCR** **Best**\n\n\n(5,5,5,8,8,8) 2815 827 1988 14 3559 750 2809 **19**\n\n\n(8,8,8,5,5,5) 3079 1077 2003 3 3535 919 2616 0\n\n\n(8,8,8,0,0,0) 2992 1744 1247 23 2945 1533 1412 3\n\n\n(8,8,8,0,0,5) 3033 1601 1432 20 3057 1399 1658 4\n\n\n(8,8,8,0,0,8) 2985 1442 1543 6 3061 1256 1805 2\n\n\n(8,8,8,0,5,0) 2928 1436 1492 0 3029 1253 1777 0\n\n\n(8,8,8,0,5,5) 2969 1292 1677 1 3124 1102 2022 1\n\n\n(8,8,8,0,5,8) 2899 1111 1788 3 3128 959 2169 1\n\n\n(8,8,8,0,8,0) 2863 1224 1639 3 3046 1051 1995 1\n\n\n(8,8,8,0,8,5) 2874 1051 1824 2 3136 896 2241 0\n\n\n(8,8,8,0,8,8) 2799 865 1935 2 3143 755 2388 0\n\n\n(8,8,8,5,0,0) **3174** 1601 1573 **26** 3388 1381 2007 2\n\n\n(8,8,8,5,0,5) **3185** 1427 1758 17 3478 1226 2252 1\n\n\n(8,8,8,5,0,8) 3110 1241 1869 3 3476 1077 2399 3\n\n\n(8,8,8,5,5,0) 3090 1273 1818 4 3451 1080 2371 1\n\n\n(8,8,8,5,5,8) 2998 885 2114 0 3540 777 2763 2\n\n\n(8,8,8,5,8,0) 2989 1024 1965 1 3460 871 2589 1\n\n\n(8,8,8,5,8,5) 2973 823 2149 4 3549 714 2835 1\n\n\n(8,8,8,5,8,8) 2894 634 2260 3 3554 572 2982 1\n\n\n(8,8,8,8,0,0) **3242** 1473 1769 **77** 3604 1241 2363 **29**\n\n\n(8,8,8,8,0,5) **3221** 1267 1953 **27** **3683** 1075 2608 11\n\n\n(8,8,8,8,0,8) **3141** 1076 2064 20 3682 927 2756 **17**\n\n\n(8,8,8,8,5,0) 3127 1114 2013 10 3663 936 2727 8\n\n\n(8,8,8,8,5,5) 3106 908 2198 11 **3745** 773 2973 7\n\n\n(8,8,8,8,5,8) 3025 716 2309 2 **3750** 630 3120 11\n\n\n(8,8,8,8,8,0) 3014 854 2160 9 3664 718 2946 12\n\n\n(8,8,8,8,8,5) 2999 654 2345 **28** **3751** 560 3191 **20**\n\n\n(8,8,8,8,8,8) 2925 469 2456 **46** **3761** 422 3338 **116**\n\n\nTable 2: Results of static strategies for 2023 and the first three quarters of 2024. The reported profits\n\n\nare in EUR per day and the results of the column best indicate on how many days a strategy is the best\n\n\nstrategy. Boldface in the columns _Overall_ and _Best_ indicate the best five strategies in the respective\n\n\nyear with regard to profits and number of times a strategy is the best strategy.\n\n\nsnapshot of the order book that contains the best four orders in either direction. In order to separate\n\n\ndays and get revenues per day of trading, we start trading at 7 p.m. of the previous day for every trading\n\n\nday and assume an initial storage level of 2 MWh, which proved to be a good choice in our tests.\n\n\nTable 2 reveals that the best strategy in the year 2023 was (8,8,8,8,0,0), reducing the allocation of\n\n\npower to FCR in the late afternoon and evening blocks to take advantage of price spikes in the IDM in the\n\n\ncorresponding hours. Generally, strategies that sell less power in the FCR market in these blocks perform\n\n\nbest in 2023. Interestingly in 2024 the situation changes and the strategy (8,8,8,8,8) that commits the\n\n\nmaximum capacity on the FCR market comes out on top with respect to average daily profits and the\n\n\nnumber of days the strategy dominates all the other strategies.\n\n\n16\n\n\nFigure 6: Number of strategies _S_ versus profit percentages loss relative to CV-28. The results comes\n\n\nfrom 01/01/2023 to 30/09/2024\n\n\nLooking at the IDM and FCR profits, we find average values of EUR 1111 and EUR 1905 in 2023 and\n\n\nEUR 959 and EUR 2468 in 2024. This implies that while the IDM market appears to have become less\n\n\nprofitable for storage in 2024 the opposite appears to have happened to the FCR prices, which explains\n\n\nthe change in the type of optimal strategies between years. Overall, the average profit is higher in 2024.\n\n\nWe furthermore observe that there are strategies that are not particularly good on average but are\n\n\nthe best strategy on a significant amount of days. Examples include strategies (8,8,8,8,8) and (8,8,8,8,5)\n\n\nin 2023. This supports our approach of selecting _S ⊆M_ strategies that complement each other in an\n\n\noptimal way, rather than simply choosing the strategies that perform best on average.\n\n\nAlthough we limit the set _M_ to a reduced strategy pool with only 28 strategies, picking up the optimal\n\n\nstrategy for any given day from the such pool, equivalent to a classification problem of 28 classes, which\n\n\nis made difficult by the inherent class imbalance in the data. To this end, we propose to select a subset\n\n\n_S ⊆M_ to reduce the size of the strategy pool while staying close to the revenue potential of the entire\n\n\nset _M_ .\n\n\nFigure 6 shows the loss with optimal strategy selection relative to the clairvoyant strategy that always\n\n\nchooses the best strategy from _M_ under the assumption that the best among the _S_ chosen strategies is\n\n\nused on every day. It can be clearly seen that for a small number of strategies _S_ the loss reduces rapidly\n\n\nbut starts to level off around _S_ = 4. This presents valuable insights in choosing the optimal number of\n\n\nstrategies: Too many classes not only increase the difficulty of the task, but also offer only marginally\n\n\nincreased profits.\n\n\n**4.3** **Features**\n\n\nTo select the most profitable FCR bidding strategy for a given day, we train our classification model\n\n\non a set of features designed to capture the prevailing and expected market conditions. This feature\n\n\nset comprises primary predictors derived from market data and forecasts, as well as carefully selected\n\n\ninteraction terms to model non-linear relationships between market drivers.\n\n\nOur model is built upon a foundation of four groups of base features:\n\n\n1. Price of the day-ahead auction (DAA): Naturally, the day-ahead market prices are good predictors\n\n\n17\n\n\nof continuous intraday prices (Uniejewski et al., 2019; Marcjasz et al., 2020; Narajewski and Ziel,\n\n\n2020). We incorporate day-ahead auction prices as input features, focusing on four selected regions\n\n\n— ‘DE-LU’, ‘IT-North’, ‘NO2’, and ‘SE4’ — due to their relatively low price correlations, which\n\n\nintroduces greater regional diversity into the model. To better capture temporal market dynamics,\n\n\nwe compute the average day-ahead prices within each EFA block. In addition, we calculate the\n\n\nstandard deviation for each EFA block to reflect price volatility, which can be informative to capture\n\n\nrisk and uncertainty in market behavior. In all this yields 4 _×_ 6 _×_ 2 = 48 features for our model.\n\n\n2. VRES producers typically sell their products on the DAM on the basis of production forecasts.\n\n\nLarger forecast RES production therefore also yields to potentially larger absolute forecast errors,\n\n\nprompting more significant rebalancing on the intraday market. Hence, renewable generation\n\n\nforecasts are a major driver of intraday prices and, in particular, intraday price variance. This\n\n\nin turn influences the profitability of trading with the storage on the intraday market and is\n\n\ntherefore a good feature in our strategy selection. See also (Karanfil and Li, 2017; Goodarzi\n\n\net al., 2019; Kulakov and Ziel, 2019; Kuppelwieser and Wozabal, 2023) for a discussion on the\n\n\nimpact of renewable generation forecasts on intraday prices. Therefore, we also utilize public\n\n\npower forecasting (PPF) data, which includes forecasts for solar generation, onshore wind, offshore\n\n\nwind, and electricity load in Germany. To align with the temporal structure of our model, we\n\n\ncalculate the average values of these forecasts within each EFA block, as well as the standard\n\n\ndeviation of hourly values within each block, which yields an additional 6 _×_ 2 _×_ 4 = 48 features.\n\n\n3. Historical FCR clearing prices serve as strong indicators of the potential profitability of the FCR\n\n\nmarket for the next day. To take advantage of this information, we include the FCR clearing price\n\n\nof the previous day for each of the 6 EFA blocks as features in our model. This helps capture recent\n\n\nmarket conditions and short-term trends relevant to price formation.\n\n\n4. We add eight dummy features to capture temporal patterns. These include the _weekday_, ranging\n\n\nfrom 0 (Sunday) to 6 (Saturday); a _weekend indicator_, equal to 1 for Saturday and Sunday and\n\n\n0 otherwise. Furthermore, we include the _day of year_, denoting the _i_ -th calendar day (1–365);\n\n\nand a _trend variable_, representing the _i_ -th day in the backtesting period (0–636). To account for\n\nseasonality, we further include both _annual seasonality_ terms [2] as well as _weekly seasonality_ terms [3] .\n\n\nWhile the base features are informative, the optimal strategy often depends on the complex interplay\n\n\nbetween different market drivers (e.g., the impact of a high wind forecast may be different on a day with\n\n\nhigh prior FCR prices versus low ones). To model these relationships, we introduce quadratic interaction\n\n\nterms. However, creating all possible pairwise interactions from the base features would result in an\n\n\nunmanageably large feature set (roughly 10000 interactions), increasing the risk of overfitting and making\n\n\nthe backtesting process computationally prohibitive. We therefore adopt a multi-step heuristic approach\n\n\nto construct a concise yet powerful set of interaction features:\n\n\n1. Focus on Primary Drivers: We generate interactions using only the average values of DAA prices\n\n\nand PPF forecasts for each EFA block, excluding the standard deviations to reduce the initial\n\n\nscope.\n\n\n\n2 sin 2 _π·_ day of year\n~~�~~ 365\n\n\n\n365y of year ~~�~~, cos ~~�~~ 2 _π·_ da365y of year\n\n\n\ny of year\n\n365 ~~�~~\n\n\n\n3 sin 2 _π·_ weekday\n� 7\n\n\n\n7 y �, cos� 2 _π·_ weekda7 y\n\n\n\ny\n\n7 �\n\n\n\n18\n\n\n|1|Train set<br>Validation set|Train set<br>Validation set|\n|---|---|---|\n||||\n|3<br>4<br>|3<br>4<br>|3<br>4<br>|\n\n\nFigure 7: Anchored walk-forward validation.\n\n\n2. Prioritize Cross-Group Interactions: We hypothesize that the most significant effects arise from\n\n\nthe interplay between different feature groups. We therefore limit the interactions to two specific\n\n\ntypes: _Day-ahead price averages × Historical FCR prices_ as well as _Renewable forecast averages_\n\n\n_× Historical FCR prices_\n\n\n3. Remove Redundancy via Correlation Filtering: This process results in an intermediate feature set\n\n\nof approximately 400 base and interaction terms. To mitigate multicollinearity and further reduce\n\n\ndimensionality, we perform a final filtering step. We compute the Pearson correlation matrix for\n\n\nthis expanded set of features and iteratively remove one feature from any pair with a correlation\n\n\ncoefficient exceeding a threshold of 0.94.\n\n\nThis structured approach yields a final, manageable set of under 300 features that captures key linear\n\n\nand non-linear market dynamics.\n\n\n**4.4** **Rolling Horizon Evaluation**\n\n\nIn our out-of-sample model evaluation, we mimic how the model would be used in trading practice on\n\n\na day-to-day basis. For any given day in the out-of-sample window, we estimate an optimal policy to\n\n\nbid on the FCR market for one day with a model trained with data from the last _N_ days. Then we\n\n\nimplement the policy and record its profits. Subsequently, we refit the model for the next trading day\n\n\nupdating the training data with the last day. We generate out-of-sample profits by rolling through our\n\n\nentire data in this fashion.\n\n\nMore specifically, we evaluate the out-of-sample performance of the model in the rolling horizon\n\n\nfashion described above using data from January 1, 2023, to September 30, 2024. Each day, a training\n\n\nwindow of _N_ = 240 days is used. In this way, we generate a set of 396 out-of-sample daily profits covering\n\nthe period from August 30, 2023, to September 30, 2024. [4] Evaluating performance on this large and\n\n\ndiverse set of days helps ensure that the results are not unduly influenced by any specific time frame or\n\n\nby particular patterns or anomalies that might appear in a smaller subset of the data.\n\n\nEvery time we retrain the model for a day of trading, we first solve equation (8) to select a subset of\n\n\nsize _S_ = 3 from _M_ that achieves the highest average profits on the training sample, and then train an\n\n\nXGBoost classifier to further identify the optimal strategy from this subset. The size of _S_ = 3 strikes a\n\n\n4 Note that since we use lagged prices, we actually need 241 days insample data to generate data for 240 training days.\n\n\n19\n\n\nStrategy FCR IDM Overall % of CV-28 Equals CV-3 Equals CV-28 Beats LCS\n\n\nCV-28 1113 407 1520 0.0 46.4 100.0 59.3\n\n\nCV-3 1127 370 1497 -1.5 100.0 46.4 27.8\n\n\nLCS 1146 313 1459 -4.0 72.2 38.9 0.0\n\n\nDB 1063 349 1412 -7.1 36.9 20.1 16.4\n\n\nSB 969 433 1402 -7.8 4.3 0.3 30.3\n\n\nOnly FCR 1238 170 1408 -7.4 57.1 37.1 14.14\n\n\nOnly IDM 0 774 774 -49.1 0.0 0.0 17.7\n\n\nTable 3: Overall out-of-sample profits (thousand Euro) of all strategies split into FCR and IDM profits.\n\n\nThe last four columns give the shortfall from CV-28 (in % of profit), the fraction of days (%) where\n\n\npolicies take the same bidding decisions as CV-3 and CV-28, as well as the fraction of days where\n\n\npolicies outperform the LCS strategies, respectively.\n\n\ngood balance between the profit of the combined strategies and the difficulty of the ensuing classification\n\n\nproblem (see Section 3.1).\n\n\nWhen using XGBoost, we optimize the hyperparameters of the method. Specifically, we optimize\n\n\nthe parameters _eta_ (0 _._ 01, 0 _._ 05, 0 _._ 1), _gamma_ (0, 0 _._ 5, 1, 2), _subsample_ (0 _._ 8, 1), _colsample_ ~~_b_~~ _ytree_ (0 _._ 8, 1),\n\n\n_max_ ~~_d_~~ _epth_ (3, 4, 5), _n_ _estimators_ (200, 400) using cross validation. We use random grid search to find the\n\n\nbest combination of hyperparameters selecting from the values in brackets. In particular, we randomly\n\n\nchose 20 possible combinations of hyperparameters without replacement and test their performance in\n\n\nthe validation sets and additionally include the best parameter combination found for the last day in the\n\n\nsample.\n\n\nDue to the periodicity and autocorrelation inherent in sequential data, traditional cross-validation\n\n\nmay disrupt temporal dependencies, resulting in a significant discrepancy between the validation and test\n\n\nsets. We use anchored walk-forward validation, which preserves the sequential order by always validating\n\n\non future data, to find the best set of hyperparameters, see Figure 7. Specifically, we employ five folds,\n\n\nwith the last 15 days in each fold reserved for validation. Note that by anchoring, the training data grows\n\n\nin later folds. Instead of using accuracy or F-score as the criterion for hyperparameter tuning, we adopt\n\n\nuse the negative trading profit in the validation sets as loss function for hyperparameter tuning. This is\n\n\nbecause accuracy and F-score may overlook misclassifications that have a significant negative impact on\n\n\nprofitability, whereas a profit-based score penalizes such errors more appropriately.\n\n\nWhen generating and evaluating the generated policy for every day, we add up FCR and intraday\n\n\nrevenues, the latter of which is the result of executing the RI on the order book data for that specific\n\n\nday. The total profits are calculated by subtracting the battery degradation cost as described in Section\n\n\n2 from these revenues. As for the training data, in order to run the rolling intrinsic, we use snapshots of\n\n\nthe LOB consisting of the top four prices for each traded product and rerun the intrinsic every minute.\n\n\nFor every day, we start trading at 7 p.m. on the previous day and assume a starting storage level of 20%\n\n\n(2MWh). Furthermore, we use the historical market clearing price of the FCR market to calculate the\n\n\nFCR revenues of the different strategies.\n\n\n20\n\n\n**4.5** **Results & Discussion**\n\n\nIn this section, we compare the results of the trained classifier strategy (LCS) described in the previous\n\n\nsections with some benchmark strategies. The first benchmark is the 8-8-8-8-0-5 strategy, also referred\n\n\nto as the static baseline (SB), which is the most profitable single strategy in _M_ calculated for the whole\n\n\ndata. In addition, we compare our policy with the strategy that selects the strategy that was most\n\n\nprofitable for the training data associated to a specific day. We refer to this as the dynamic baseline\n\n\n(DB). The perfect dynamic strategy denoted by CV-3 denotes an ideal policy that always selects the best\n\n\nperforming strategy from the optimal group _S_ on any given day. Finally, as another theoretical upper\n\n\nbound, we include the CV-28 that always chooses the best strategy from _M_ for every out-of-sample day.\n\n\nFurthermore, we also evaluate the strategies to bid only on the IDM and to bid the maximum amount\n\n\nallowed on the FCR market, that is, the strategies (8,8,8,8,8) and (0,0,0,0,0,0).\n\n\nThe results of this analysis are summarized in Table 3, which allows for the following conclusions:\n\n\nFirst, our proposed LCS performs exceptionally well, achieving an overall profit that is only 4 _._ 0% lower\n\n\nthan the theoretical maximum achievable with perfect foresight (CV-28). This gap represents the com\n\nbined loss from two distinct steps: the initial selection of the top-three strategy pool in (8) and the\n\n\nsubsequent classification error. Critically, the gap shrinks to a mere 1 _._ 5% when compared to CV-3, the\n\n\nclairvoyant strategy restricted to the same three strategies. This demonstrates the high accuracy of\n\n\nthe XGBoost classifier in selecting the best option from the available pool on any given day. In stark\n\n\ncontrast, the naive dynamic (DB) and static (SB) benchmarks fall short of the theoretical maximum by\n\n\n7 _._ 1% and 7 _._ 8%, respectively, highlighting the significant value added by the learning-based approach.\n\n\nFurthermore, strategies focusing exclusively on a single market are clearly suboptimal; the ’Only FCR’\n\n\nstrategy underperforms the CV-28 benchmark by 7 _._ 4%, while the ’Only IDM’ strategy lags by a sub\n\nstantial 49 _._ 1%. The later results also shows that the FCR market is more profitable than the IDM in\n\n\nthe observation period.\n\n\nSecond, a breakdown of the revenue sources reveals that the performance difference between the\n\n\ntop strategies is primarily driven by profits from the intraday market. Both CV-28 and CV-3 achieve\n\n\nhigher IDM revenues than our LCS. This indicates that the main challenge, and the primary source of\n\n\nthe remaining performance gap, lies in perfectly identifying the specific days where the intraday market\n\n\noffers exceptionally high profit opportunities.\n\n\nThird, the effectiveness of the classifier is further confirmed by analyzing the daily decision alignment\n\n\nshown in Table 3. LCS selects the same strategy as CV-3 for an impressive 72.2% of the days, a high\n\n\nhit rate that directly corresponds to its strong profit performance. The alignment with the best overall\n\n\nstrategy (CV-28) is naturally lower at 38 _._ 9%, as the LCS is limited to strategies from _S_ . The fact that\n\n\nthe CV-3 strategy itself only matches the CV-28 decision on 46 _._ 4% of days underscores the importance of\n\n\nthe initial strategy pool selection and explains why a gap between our LCS and the theoretical maximum\n\n\nremains.\n\n\nFinally, the daily decision metrics in Table 3 provide further insight into the performance of the\n\n\nmodel. The CV-3 strategy, which has perfect foresight over the same pool of three strategies available\n\n\nto our LCS, outperforms it on only 27 _._ 8% of the days. This implies a high classification accuracy for\n\n\nthe XGBoost model, as it correctly identifies the best strategy within its pool more than 70% of the\n\n\ntime. A nuanced observation arises when comparing LCS to the SB strategy. While SB outperforms LCS\n\n\non a higher fraction of days (30.1%). This occurs because SB can be the strategy outside the curated\n\n\nthree-strategy pool used by LCS; however, the marginal profits gained on those days are negligible, which\n\n\n21\n\n\nFigure 8: Cumulative profits for the LCS and all benchmarking strategies over the whole OOS period.\n\n\n\n\n\n\n\nFigure 9: A heatmap presents the normalized weekly profits across different strategies over the entire\n\n\nevaluation period. The horizontal axis denotes time (from August 2023 to August 2024), while the\n\n\nvertical axis lists the strategies. The color scale ranges from purple (low profits) to yellow (high profits).\n\n\nexplains why SB’s overall profit remains substantially lower.\n\n\nThe data also allows to evaluate the quality of our initial strategy pool selection. The fact that the\n\n\nglobally optimal strategy from all 28 candidates was contained within our chosen pool of three on 46 _._ 4%\n\n\nof the days confirms that our selection process effectively captures the best-performing strategy in nearly\n\n\nhalf of all trading scenarios. Finally, the results reaffirm the market dynamics of the study period: the\n\n\n’Only FCR’ strategy proved to be the single best choice on a remarkable 37 _._ 1% of all days, underscoring\n\n\nthe dominance of the FCR market and explaining why the top-performing strategies are heavily weighted\n\n\ntowards FCR commitments.\n\n\nFigure 8 shows the cumulative profits of the strategies during the out-of-sample period. We can see\n\n\nthat _Only IDM_ is clearly the worst strategy, while all mixed strategies earn profits in a comparable\n\n\nrange. In accordance with the risk-free nature of the RI, the profits increase monotonically over time,\n\n\nand the slope substantially increases around the middle of the observation period, which is mainly due\n\n\nto the increase in profits in the FCR market.\n\nFigure 9 displays normalized weekly profits of different strategies [5] . The results reveal clear temporal\n\n\n5 normalized weekly profits = max(weekly profits) - min(weekly profits)weekly profits - min(weekly profits)\n\n\n22\n\n\nand strategic patterns. In particular, the dynamic strategies (CV-28, CV-3, LCS) consistently achieve\n\n\nhigher profits compared to the other four strategies. In particular, there is a pronounced increase in\n\n\nprofitability between May and July 2024, when most dynamic strategies reach their peak performance\n\n\n(indicated by the yellow regions). In contrast, during late 2023 and early 2024, profits remain relatively\n\n\nlow across all strategies. Furthermore, LCS exhibits a more stable and resilient performance compared\n\n\nto DS and SB. In particular, during the low-profit months from November 2023 to February 2024, LCS\n\n\nsustains higher profit levels, avoiding the sharp declines seen in DS and SB. Similarly, in March and April\n\n\n2024, LCS continues to outperform both strategies, maintaining moderate profitability where DS and\n\n\nSB weaken. Moreover, during the peak period between May and July 2024, LCS tracks closely with CV\n\n28 and CV-3, showing consistently high profit levels (yellow regions), while DS and SB display greater\n\n\nvariability. This highlights that LCS is not only more robust in low-profit phases but also competitive\n\n\nwith the strongest combined strategies in high-profit periods.\n\n\nThe computational requirements of our approach can be divided into two phases: the initial backtest\n\n\nand the daily operational deployment. The initial backtest, which involves simulating the RI performance\n\n\nfor each FCR strategy over the entire dataset, is computationally intensive and requires approximately\n\n\n7 hours per strategy. However, this process is highly parallelizable, as simulations for strategies are\n\n\nindependent of each other. We managed this workload by performing the evaluations on 48 AWS Batch\n\n\ninstances, each equipped with 4 vCPUs and 24 GB of memory.\n\n\nIn a live trading environment, the daily computational workflow is highly efficient. The process\n\n\nbegins by updating the training data, which involves running a one-day backtest for each of the 28\n\n\ncandidate strategies using market data from the most recently completed day. This simulation step\n\n\ntakes approximately one minute. Immediately following the data update, the optimal strategy pool is\n\n\nre-selected and the XGBoost classifier is retrained to generate a bid for the upcoming day. The entire\n\n\ndaily process, from the initial data update to the final prediction, is completed in under two minutes\n\n\non a standard desktop computer. This rapid execution time confirms that our LCS framework is not\n\n\ncomputationally prohibitive and is well suited for practical deployment in real-world trading operations.\n\n### **5 Conclusion**\n\n\nThis paper introduces a novel hierarchical approach to optimize BESS participation in the frequency\n\n\ncontainment reserve and intraday markets. Our method effectively decouples the complex problem into\n\n\ntwo manageable stages: a high-frequency rolling intrinsic algorithm handles the continuous intraday\n\n\ntrading and state of charge management, while an XGBoost-based classifier selects the optimal FCR\n\n\ncommitment from a pre-screened pool of complementary strategies. Our out-of-sample backtest on\n\n\nhistorical market data validates the practical value of this approach; the proposed _learned classifier_\n\n\n_strategy_ substantially outperforms static and naive dynamic benchmarks, closing a significant portion of\n\n\nthe gap to theoretical maximum profit.\n\n\nDespite its strong performance, the framework has limitations that open clear avenues for future\n\n\nresearch. A primary constraint is the reliance on a small and curated set of FCR strategies. Prevailing\n\n\nmarket conditions have heavily favored the FCR market, causing the most profitable strategies to be\n\n\nhighly similar, allocating maximum capacity to FCR. This clustering creates a difficult classification\n\n\nproblem with severe class imbalance, limiting the performance ceiling of the LCS. Furthermore, for\n\n\ntractability, our model assumes zero-price bids in the pay-as-cleared FCR auction and uses the suboptimal\n\n\nmyopic RI for intraday trading.\n\n\n23\n\n\nFuture work should directly address these limitations. First, enhancing the intraday trading algorithm\n\n\nis a key priority. By moving beyond the RI to more sophisticated methods such as reinforcement learning,\n\n\nthe profitability of the IDM could be increased. This would naturally lead to a more diverse set of viable\n\n\nstrategies, mitigating the class imbalance and making the classification task more effective. Second, a\n\n\npromising extension is to endogenize the FCR bid price. Instead of assuming zero-price bids, a forecasting\n\n\nmodel could estimate the opportunity cost of FCR participation, effectively the IDM profit foregone for\n\n\neach EFA block. This would enable a more sophisticated bidding strategy that optimizes both the\n\n\nquantity and the price of the FCR commitment, further enhancing overall profitability.\n\n### **References**\n\n\nR. A¨ıd, P. Gruet, and H. Pham. An optimal trading problem in intraday electricity markets. _Mathematics_\n\n\n_and Financial Economics_, 10:49–85, 2016.\n\n\nM. Ansari, A. T. Al-Awami, E. Sortomme, and M. A. Abido. Coordinated bidding of ancillary services\n\n\nfor vehicle-to-grid using fuzzy optimization. _IEEE Transactions on Smart Grid_, 6(1):261–270, 2015.\n\n\ndoi: 10.1109/TSG.2014.2341625.\n\n\nP. Astero and C. Evens. Stochastic bidding strategy for electrical vehicle charging stations to participate\n\n\nin frequency containment reserves markets. _IET Generation, Transmission & Distribution_, 14(13):\n\n\n2566–2572, 2020.\n\n\nG. Bertrand and A. Papavasiliou. Reinforcement-learning based threshold policies for continuous intraday\n\n\nelectricity market trading. In _2019 IEEE power & energy society general meeting (PESGM)_, pages\n\n\n1–5. IEEE, 2019a.\n\n\nG. Bertrand and A. Papavasiliou. Adaptive trading in continuous intraday electricity markets for a\n\n\nstorage unit. _IEEE Transactions on Power Systems_, 35(3):2339–2350, 2019b.\n\n\nF. Biggins, S. Homan, J. Ejeh, and S. Brown. To trade or not to trade: Simultaneously optimising\n\n\nbattery storage for arbitrage and ancillary services. _Journal of Energy Storage_, 50:104234, 2022.\n\n\nBloombergNEF. Lithium-ion battery pack prices see largest drop since 2017, falling to $115 per kilowatt\n\nhour. Technical report, BloombergNEF, 2024. URL `[https://about.bnef.com/insights/commodit](https://about.bnef.com/insights/commodities/lithium-ion-battery-pack-prices-see-largest-drop-since-2017-falling-to-115-per-kilowatt-hour-bloombergnef/)`\n\n```\n ies/lithium-ion-battery-pack-prices-see-largest-drop-since-2017-falling-to-115-per\n\n```\n\n`[-kilowatt-hour-bloombergnef/](https://about.bnef.com/insights/commodities/lithium-ion-battery-pack-prices-see-largest-drop-since-2017-falling-to-115-per-kilowatt-hour-bloombergnef/)` . Accessed: August 23, 2025.\n\n\nI. Boukas, D. Ernst, T. Th´eate, A. Bolland, A. Huynen, M. Buchwald, C. Wynants, and B. Corn´elusse.\n\n\nA deep reinforcement learning framework for continuous intraday market bidding. _Machine Learning_,\n\n\n110:2335–2387, 2021.\n\n\nC. Brivio, S. Mandelli, and M. Merlo. Battery energy storage system for primary control reserve and\n\n\nenergy arbitrage. _Sustainable Energy, Grids and Networks_, 6:152–165, 2016.\n\n\nG. Broneske and D. Wozabal. How do contract parameters influence the economics of vehicle-to-grid?\n\n\n_Manufacturing & Service Operations Management_, 19(1):150–164, 2017.\n\n\nT. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd acm_\n\n\n_sigkdd international conference on knowledge discovery and data mining_, pages 785–794, 2016.\n\n\n24\n\n\nW. Cole and A. Karmakar. Cost projections for utility-scale battery storage: 2023 update. Technical\n\n\nReport NREL/TP-6A40-85332, National Renewable Energy Laboratory (NREL), 2023. URL `[https:](https://docs.nrel.gov/docs/fy23osti/85332.pdf)`\n\n\n`[//docs.nrel.gov/docs/fy23osti/85332.pdf](https://docs.nrel.gov/docs/fy23osti/85332.pdf)` .\n\n\nJ. De Boeck, B. Fortz, M. Labb´e, E. Marcotte, P. Marcotte, and G. Savard. Bidding in day-ahead [´]\n\n\nelectricity markets: A dynamic programming framework. _Computers & Operations Research_, 179:\n\n\n107024, 2025.\n\n\nI. Dimoulkas, M. Amelin, and M. R. Hesamzadeh. Forecasting balancing market prices using hidden\n\n\nmarkov models. In _2016 13th International Conference on the European Energy Market (EEM)_, pages\n\n\n1–5. IEEE, 2016.\n\n\nP. H. Divshali and C. Evens. Optimum operation of battery storage system in frequency containment\n\n\nreserves markets. _IEEE Transactions on Smart Grid_, 11(6):4906–4915, 2020.\n\n\nEPEX SPOT SE. Epex spot annual reports, 2020-2023. URL `[https://www.epexspot.com/](https://www.epexspot.com/)` . Accessed:\n\n\n2024-06-17.\n\n\nEuropean Commission. Commission regulation (eu) 2015/1222 of 24 july 2015 establishing a guideline\n\n\non capacity allocation and congestion management. _Official Journal of the European Union_, L 197:\n\n\n24–72, 2015.\n\n\nE. Faria and S.-E. Fleten. Day-ahead market bidding for a nordic hydropower producer: taking the elbas\n\n\nmarket into account. _Computational Management Science_, 8:75–101, 2011.\n\n\nJ. Figgener, B. Tepe, F. R¨ucker, I. Schoeneberger, C. Hecht, A. Jossen, and D. U. Sauer. The influence\n\n\nof frequency containment reserve flexibilization on the economics of electric vehicle fleet operation.\n\n\n_Journal of Energy Storage_, 53:105138, 2022.\n\n\nS.-E. Fleten and T. K. Kristoffersen. Short-term hydropower production planning by stochastic pro\n\ngramming. _Computers & Operations Research_, 35(8):2656–2671, 2008.\n\n\nS. Goodarzi, H. N. Perera, and D. Bunn. The impact of renewable energy forecast errors on imbalance\n\n\nvolumes and electricity spot prices. _Energy Policy_, 134:110827, 2019.\n\n\nC. Graf, T. Kuppelwieser, and D. Wozabal. Frequent auctions for intraday electricity markets. _The_\n\n\n_Energy Journal_, (45), 2024.\n\n\nJ. Gray and P. Khandelwal. Towards a realistic gas storage model. _Commodities Now_, 7(2):1–4, 2004.\n\n\nL. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still outperform deep learning on\n\n\ntypical tabular data? In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\n\n\n_Advances in Neural Information Processing Systems_, volume 35, pages 507–520. Curran Associates,\n\n\nInc., 2022.\n\n\nD. R. Jiang and W. B. Powell. Optimal hour-ahead bidding in the real-time electricity market with\n\n\nbattery storage using approximate dynamic programming. _INFORMS Journal on Computing_, 27(3):\n\n\n525–543, 2015.\n\n\nF. Karanfil and Y. Li. The role of continuous intraday electricity markets: The integration of large-share\n\n\nwind power generation in denmark. _The Energy Journal_, 38(2), 2017.\n\n\n25\n\n\nT. Kempitiya, S. Sierla, D. De Silva, M. Yli-Ojanper¨a, D. Alahakoon, and V. Vyatkin. An artificial\n\n\nintelligence framework for bidding optimization with uncertainty in multiple frequency reserve markets.\n\n\n_Applied Energy_, 280:115918, 2020.\n\n\nH. Kongelf, K. Overrein, G. Klæboe, and S.-E. Fleten. Portfolio size’s effects on gains from coordinated\n\n\nbidding in electricity markets: A case study of a norwegian hydropower producer. _Energy Systems_,\n\n\n10:567–591, 2019.\n\n\nE. Kraft, D. Keles, and W. Fichtner. Modeling of frequency containment reserve prices with econometrics\n\n\nand artificial intelligence. _Journal of Forecasting_, 39(8):1179–1197, 2020.\n\n\nS. Kulakov and F. Ziel. The impact of renewable energy forecasts on intraday electricity prices. _arXiv_\n\n\n_preprint arXiv:1903.09641_, 2019.\n\n\nT. Kuppelwieser and D. Wozabal. Liquidity costs on intraday power markets: Continuous trading versus\n\n\nauctions. _Energy Policy_, 154:112299, 2021.\n\n\nT. Kuppelwieser and D. Wozabal. Intraday power trading: toward an arms race in weather forecasting?\n\n\n_OR Spectrum_, 45(1):57–83, 2023.\n\n\nN. L¨ohndorf and D. Wozabal. The value of coordination in multimarket bidding of grid energy storage.\n\n\n_Operations research_, 71(1):1–22, 2023.\n\n\nN. L¨ohndorf, D. Wozabal, and S. Minner. Optimizing trading decisions for hydro storage systems using\n\n\napproximate dual dynamic programming. _Operations Research_, 61(4):810–823, 2013.\n\n\nG. Marcjasz, B. Uniejewski, and R. Weron. Beating the na¨ıve—combining lasso with na¨ıve intraday\n\n\nelectricity price forecasts. _Energies_, 13(7):1667, 2020.\n\n\nM. Narajewski and F. Ziel. Econometric modelling and forecasting of intraday electricity prices. _Journal_\n\n\n_of Commodity Markets_, 19:100107, 2020.\n\n\nR. Shwartz-Ziv and A. Armon. Tabular data: Deep learning is not all you need. _Information Fusion_,\n\n\n81:84–90, 2022.\n\n\nR. Sioshansi. Welfare impacts of electricity storage and the implications of ownership structure. _The_\n\n\n_Energy Journal_, 31(2), 2010.\n\n\nE. Sortomme and M. A. El-Sharkawi. Optimal scheduling of vehicle-to-grid energy and ancillary services.\n\n\n_IEEE Transactions on Smart Grid_, 3(1):351–359, 2012. doi: 10.1109/TSG.2011.2164099.\n\n\nT. Thien, D. Schweer, D. vom Stein, A. Moser, and D. U. Sauer. Real-world operating strategy and\n\n\nsensitivity analysis of frequency containment reserve provision with battery energy storage systems in\n\n\nthe german market. _Journal of energy storage_, 13:143–163, 2017.\n\n\nB. Uniejewski, G. Marcjasz, and R. Weron. Understanding intraday electricity markets: Variable selection\n\n\nand very short-term price forecasting using lasso. _International Journal of Forecasting_, 35(4):1533–\n\n\n1547, 2019.\n\n\nD. Wozabal and G. Rameseder. Optimal bidding of a virtual power plant on the spanish day-ahead and\n\n\nintraday market for electricity. _European Journal of Operational Research_, 280(2):639–655, 2020.\n\n\n26\n\n\nB. Xu, A. Oudalov, J. Poland, A. Ulbig, and G. Andersson. Bess control strategies for participating in\n\n\ngrid frequency regulation. _IFAC Proceedings Volumes_, 47(3):4024–4029, 2014.\n\n\nY. J. A. Zhang, C. Zhao, W. Tang, and S. H. Low. Profit-maximizing planning and control of battery\n\n\nenergy storage systems for primary frequency control. _IEEE Transactions on Smart Grid_, 9(2):712–\n\n\n723, 2016.\n\n\n27\n\n\n"
  }
]