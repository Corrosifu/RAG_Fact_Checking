[
  {
    "id": "2510.02313v1",
    "title": "Clink! Chop! Thud! -- Learning Object Sounds from Real-World\n  Interactions",
    "authors": [
      "Mengyu Yang",
      "Yiming Chen",
      "Haozheng Pei",
      "Siddhant Agarwal",
      "Arun Balajee Vasudevan",
      "James Hays"
    ],
    "summary": "Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.",
    "published": "2025-10-02T17:59:52Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02313v1"
  },
  {
    "id": "2510.02312v1",
    "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
    "authors": [
      "Anna Kuzina",
      "Maciej Pioro",
      "Paul N. Whatmough",
      "Babak Ehteshami Bejnordi"
    ],
    "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
    "published": "2025-10-02T17:59:51Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02312v1"
  },
  {
    "id": "2510.02311v1",
    "title": "Inferring Dynamic Physical Properties from Video Foundation Models",
    "authors": [
      "Guanqi Zhan",
      "Xianzheng Ma",
      "Weidi Xie",
      "Andrew Zisserman"
    ],
    "summary": "We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.",
    "published": "2025-10-02T17:59:50Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02311v1"
  },
  {
    "id": "2510.02308v1",
    "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient\n  Orthogonalization",
    "authors": [
      "Dhruv Kohli",
      "Sawyer J. Robertson",
      "Gal Mishne",
      "Alexander Cloninger"
    ],
    "summary": "Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.",
    "published": "2025-10-02T17:59:45Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02308v1"
  },
  {
    "id": "2510.02305v1",
    "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is\n  Geometry Adaptive",
    "authors": [
      "Tyler Farghly",
      "Peter Potaptchik",
      "Samuel Howard",
      "George Deligiannidis",
      "Jakiw Pidstrigach"
    ],
    "summary": "Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.",
    "published": "2025-10-02T17:59:39Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02305v1"
  },
  {
    "id": "2510.02302v1",
    "title": "Knowledge Distillation Detection for Open-weights Models",
    "authors": [
      "Qin Shi",
      "Amber Yijia Zheng",
      "Qifan Song",
      "Raymond A. Yeh"
    ],
    "summary": "We propose the task of knowledge distillation detection, which aims to\ndetermine whether a student model has been distilled from a given teacher,\nunder a practical setting where only the student's weights and the teacher's\nAPI are available. This problem is motivated by growing concerns about model\nprovenance and unauthorized replication through distillation. To address this\ntask, we introduce a model-agnostic framework that combines data-free input\nsynthesis and statistical score computation for detecting distillation. Our\napproach is applicable to both classification and generative models.\nExperiments on diverse architectures for image classification and text-to-image\ngeneration show that our method improves detection accuracy over the strongest\nbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image\ngeneration. The code is available at\nhttps://github.com/shqii1j/distillation_detection.",
    "published": "2025-10-02T17:59:14Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02302v1"
  },
  {
    "id": "2510.02300v1",
    "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
    "authors": [
      "Runqian Wang",
      "Yilun Du"
    ],
    "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
    "published": "2025-10-02T17:59:06Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02300v1"
  },
  {
    "id": "2510.02298v1",
    "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control\n  Empower Scalable Real-world Deployment and Adaptation",
    "authors": [
      "Wenye Yu",
      "Jun Lv",
      "Zixi Ying",
      "Yang Jin",
      "Chuan Wen",
      "Cewu Lu"
    ],
    "summary": "Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.",
    "published": "2025-10-02T17:59:02Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02298v1"
  },
  {
    "id": "2510.02297v1",
    "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
    "authors": [
      "Wentao Zhang",
      "Yang Young Lu",
      "Yuntian Deng"
    ],
    "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
    "published": "2025-10-02T17:59:00Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02297v1"
  },
  {
    "id": "2510.02296v1",
    "title": "Continual Personalization for Diffusion Models",
    "authors": [
      "Yu-Chien Liao",
      "Jr-Jen Chen",
      "Chi-Pin Huang",
      "Ci-Siang Lin",
      "Meng-Lin Wu",
      "Yu-Chiang Frank Wang"
    ],
    "summary": "Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.",
    "published": "2025-10-02T17:58:56Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02296v1"
  },
  {
    "id": "2510.02295v1",
    "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
    "authors": [
      "Enxin Song",
      "Wenhao Chai",
      "Shusheng Yang",
      "Ethan Armand",
      "Xiaojun Shan",
      "Haiyang Xu",
      "Jianwen Xie",
      "Zhuowen Tu"
    ],
    "summary": "Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.",
    "published": "2025-10-02T17:58:54Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02295v1"
  },
  {
    "id": "2510.02291v1",
    "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling",
    "authors": [
      "Litu Rout",
      "Andreas Lugmayr",
      "Yasamin Jafarian",
      "Srivatsan Varadharajan",
      "Constantine Caramanis",
      "Sanjay Shakkottai",
      "Ira Kemelmacher-Shlizerman"
    ],
    "summary": "We study the problem of posterior sampling using pretrained discrete\ndiffusion foundation models, aiming to recover images from noisy measurements\nwithout retraining task-specific models. While diffusion models have achieved\nremarkable success in generative modeling, most advances rely on continuous\nGaussian diffusion. In contrast, discrete diffusion offers a unified framework\nfor jointly modeling categorical data such as text and images. Beyond\nunification, discrete diffusion provides faster inference, finer control, and\nprincipled training-free Bayesian inference, making it particularly well-suited\nfor posterior sampling. However, existing approaches to discrete diffusion\nposterior sampling face severe challenges: derivative-free guidance yields\nsparse signals, continuous relaxations limit applicability, and split Gibbs\nsamplers suffer from the curse of dimensionality. To overcome these\nlimitations, we introduce Anchored Posterior Sampling (APS) for masked\ndiffusion foundation models, built on two key innovations -- quantized\nexpectation for gradient-like guidance in discrete embedding space, and\nanchored remasking for adaptive decoding. Our approach achieves\nstate-of-the-art performance among discrete diffusion samplers across linear\nand nonlinear inverse problems on the standard benchmarks. We further\ndemonstrate the benefits of our approach in training-free stylization and\ntext-guided editing.",
    "published": "2025-10-02T17:58:37Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02291v1"
  },
  {
    "id": "2510.02287v1",
    "title": "MultiModal Action Conditioned Video Generation",
    "authors": [
      "Yichen Li",
      "Antonio Torralba"
    ],
    "summary": "Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.",
    "published": "2025-10-02T17:57:06Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02287v1"
  },
  {
    "id": "2510.02286v1",
    "title": "Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming\n  Attacks",
    "authors": [
      "Ruohao Guo",
      "Afshin Oroojlooy",
      "Roshan Sridhar",
      "Miguel Ballesteros",
      "Alan Ritter",
      "Dan Roth"
    ],
    "summary": "Despite recent rapid progress in AI safety, current large language models\nremain vulnerable to adversarial attacks in multi-turn interaction settings,\nwhere attackers strategically adapt their prompts across conversation turns and\npose a more critical yet realistic challenge. Existing approaches that discover\nsafety vulnerabilities either rely on manual red-teaming with human experts or\nemploy automated methods using pre-defined templates and human-curated attack\ndata, with most focusing on single-turn attacks. However, these methods did not\nexplore the vast space of possible multi-turn attacks, failing to consider\nnovel attack trajectories that emerge from complex dialogue dynamics and\nstrategic conversation planning. This gap is particularly critical given recent\nfindings that LLMs exhibit significantly higher vulnerability to multi-turn\nattacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy\nreinforcement learning framework integrated with tree search that autonomously\ndiscovers diverse multi-turn attack strategies by treating the dialogue as a\nsequential decision-making problem, enabling systematic exploration without\nmanually curated data. Through extensive experiments, our approach not only\nachieves more than 25.9% higher ASR across 10 target models compared to\nprevious state-of-the-art approaches, but also effectively uncovers new attack\nstrategies by learning optimal dialogue policies that maximize attack success\nacross multiple turns.",
    "published": "2025-10-02T17:57:05Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02286v1"
  },
  {
    "id": "2510.02284v1",
    "title": "Learning to Generate Object Interactions with Physics-Guided Video\n  Diffusion",
    "authors": [
      "David Romero",
      "Ariana Bermudez",
      "Hao Li",
      "Fabio Pizzati",
      "Ivan Laptev"
    ],
    "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
    "published": "2025-10-02T17:56:46Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02284v1"
  },
  {
    "id": "2510.02282v1",
    "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning\n  MLLMs and RL",
    "authors": [
      "Kyoungjun Park",
      "Yifan Yang",
      "Juheon Yi",
      "Shicheng Zheng",
      "Yifei Shen",
      "Dongqi Han",
      "Caihua Shan",
      "Muhammad Muaz",
      "Lili Qiu"
    ],
    "summary": "With the rapid advancement of AI-generated videos, there is an urgent need\nfor effective detection tools to mitigate societal risks such as misinformation\nand reputational harm. In addition to accurate classification, it is essential\nthat detection models provide interpretable explanations to ensure transparency\nfor regulators and end users. To address these challenges, we introduce\nVidGuard-R1, the first video authenticity detector that fine-tunes a\nmulti-modal large language model (MLLM) using group relative policy\noptimization (GRPO). Our model delivers both highly accurate judgments and\ninsightful reasoning. We curate a challenging dataset of 140k real and\nAI-generated videos produced by state-of-the-art generation models, carefully\ndesigning the generation process to maximize discrimination difficulty. We then\nfine-tune Qwen-VL using GRPO with two specialized reward models that target\ntemporal artifacts and generation complexity. Extensive experiments demonstrate\nthat VidGuard-R1 achieves state-of-the-art zero-shot performance on existing\nbenchmarks, with additional training pushing accuracy above 95%. Case studies\nfurther show that VidGuard-R1 produces precise and interpretable rationales\nbehind its predictions. The code is publicly available at\nhttps://VidGuard-R1.github.io.",
    "published": "2025-10-02T17:55:37Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02282v1"
  },
  {
    "id": "2510.02279v1",
    "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods\n  for Natural Language Generation",
    "authors": [
      "Mykyta Ielanskyi",
      "Kajetan Schweighofer",
      "Lukas Aichberger",
      "Sepp Hochreiter"
    ],
    "summary": "Hallucinations are a common issue that undermine the reliability of large\nlanguage models (LLMs). Recent studies have identified a specific subset of\nhallucinations, known as confabulations, which arise due to predictive\nuncertainty of LLMs. To detect confabulations, various methods for estimating\npredictive uncertainty in natural language generation (NLG) have been\ndeveloped. These methods are typically evaluated by correlating uncertainty\nestimates with the correctness of generated text, with question-answering (QA)\ndatasets serving as the standard benchmark. However, commonly used approximate\ncorrectness functions have substantial disagreement between each other and,\nconsequently, in the ranking of the uncertainty estimation methods. This allows\none to inflate the apparent performance of uncertainty estimation methods. We\npropose using several alternative risk indicators for risk correlation\nexperiments that improve robustness of empirical assessment of UE algorithms\nfor NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge\nvariants leads to reducing the evaluation biases. Furthermore, we explore\nstructured tasks as well as out of distribution and perturbation detection\ntasks which provide robust and controllable risk indicators. Finally, we\npropose to use an Elo rating of uncertainty estimation methods to give an\nobjective summarization over extensive evaluation settings.",
    "published": "2025-10-02T17:54:09Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02279v1"
  },
  {
    "id": "2510.02278v1",
    "title": "Fine-Grained Urban Traffic Forecasting on Metropolis-Scale Road Networks",
    "authors": [
      "Fedor Velikonivtsev",
      "Oleg Platonov",
      "Gleb Bazhenov",
      "Liudmila Prokhorenkova"
    ],
    "summary": "Traffic forecasting on road networks is a complex task of significant\npractical importance that has recently attracted considerable attention from\nthe machine learning community, with spatiotemporal graph neural networks\n(GNNs) becoming the most popular approach. The proper evaluation of traffic\nforecasting methods requires realistic datasets, but current publicly available\nbenchmarks have significant drawbacks, including the absence of information\nabout road connectivity for road graph construction, limited information about\nroad properties, and a relatively small number of road segments that falls\nshort of real-world applications. Further, current datasets mostly contain\ninformation about intercity highways with sparsely located sensors, while city\nroad networks arguably present a more challenging forecasting task due to much\ndenser roads and more complex urban traffic patterns. In this work, we provide\na more complete, realistic, and challenging benchmark for traffic forecasting\nby releasing datasets representing the road networks of two major cities, with\nthe largest containing almost 100,000 road segments (more than a 10-fold\nincrease relative to existing datasets). Our datasets contain rich road\nfeatures and provide fine-grained data about both traffic volume and traffic\nspeed, allowing for building more holistic traffic forecasting systems. We show\nthat most current implementations of neural spatiotemporal models for traffic\nforecasting have problems scaling to datasets of our size. To overcome this\nissue, we propose an alternative approach to neural traffic forecasting that\nuses a GNN without a dedicated module for temporal sequence processing, thus\nachieving much better scalability, while also demonstrating stronger\nforecasting performance. We hope our datasets and modeling insights will serve\nas a valuable resource for research in traffic forecasting.",
    "published": "2025-10-02T17:53:51Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02278v1"
  },
  {
    "id": "2510.02274v1",
    "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps",
    "authors": [
      "Kyoungjun Park",
      "Yifan Yang",
      "Changhan Ge",
      "Lili Qiu",
      "Shiqi Jiang"
    ],
    "summary": "Modeling radio frequency (RF) signal propagation is essential for\nunderstanding the environment, as RF signals offer valuable insights beyond the\ncapabilities of RGB cameras, which are limited by the visible-light spectrum,\nlens coverage, and occlusions. It is also useful for supporting wireless\ndiagnosis, deployment, and optimization. However, accurately predicting RF\nsignals in complex environments remains a challenge due to interactions with\nobstacles such as absorption and reflection. We introduce Diffusion^2, a\ndiffusion-based approach that uses 3D point clouds to model the propagation of\nRF signals across a wide range of frequencies, from Wi-Fi to millimeter waves.\nTo effectively capture RF-related features from 3D data, we present the RF-3D\nEncoder, which encapsulates the complexities of 3D geometry along with\nsignal-specific details. These features undergo multi-scale embedding to\nsimulate the actual RF signal dissemination process. Our evaluation, based on\nsynthetic and real-world measurements, demonstrates that Diffusion^2 accurately\nestimates the behavior of RF signals in various frequency bands and\nenvironmental conditions, with an error margin of just 1.9 dB and 27x faster\nthan existing methods, marking a significant advancement in the field. Refer to\nhttps://rfvision-project.github.io/ for more information.",
    "published": "2025-10-02T17:50:22Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02274v1"
  },
  {
    "id": "2510.02268v1",
    "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with\n  Camera Conditioning",
    "authors": [
      "Tianchong Jiang",
      "Jingtian Ji",
      "Xiangshan Tan",
      "Jiading Fang",
      "Anand Bhattad",
      "Vitor Guizilini",
      "Matthew R. Walter"
    ],
    "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .",
    "published": "2025-10-02T17:47:06Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02268v1"
  },
  {
    "id": "2510.02265v1",
    "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement\n  Learning",
    "authors": [
      "Yalin E. Sagduyu",
      "Tugba Erpek",
      "Kemal Davaslioglu",
      "Sastry Kompella"
    ],
    "summary": "This paper studies the problem of mitigating reactive jamming, where a jammer\nadopts a dynamic policy of selecting channels and sensing thresholds to detect\nand jam ongoing transmissions. The transmitter-receiver pair learns to avoid\njamming and optimize throughput over time (without prior knowledge of channel\nconditions or jamming strategies) by using reinforcement learning (RL) to adapt\ntransmit power, modulation, and channel selection. Q-learning is employed for\ndiscrete jamming-event states, while Deep Q-Networks (DQN) are employed for\ncontinuous states based on received power. Through different reward functions\nand action sets, the results show that RL can adapt rapidly to spectrum\ndynamics and sustain high rates as channels and jamming policies change over\ntime.",
    "published": "2025-10-02T17:44:38Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02265v1"
  },
  {
    "id": "2510.02264v1",
    "title": "Paving the Way Towards Kinematic Assessment Using Monocular Video: A\n  Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose\n  Estimators Against Inertial Sensors in Daily Living Activities",
    "authors": [
      "Mario Medrano-Paredes",
      "Carmen Fernández-González",
      "Francisco-Javier Díaz-Pernas",
      "Hichem Saoudi",
      "Javier González-Alonso",
      "Mario Martínez-Zarzuela"
    ],
    "summary": "Advances in machine learning and wearable sensors offer new opportunities for\ncapturing and analyzing human movement outside specialized laboratories.\nAccurate assessment of human movement under real-world conditions is essential\nfor telemedicine, sports science, and rehabilitation. This preclinical\nbenchmark compares monocular video-based 3D human pose estimation models with\ninertial measurement units (IMUs), leveraging the VIDIMU dataset containing a\ntotal of 13 clinically relevant daily activities which were captured using both\ncommodity video cameras and five IMUs. During this initial study only healthy\nsubjects were recorded, so results cannot be generalized to pathological\ncohorts. Joint angles derived from state-of-the-art deep learning frameworks\n(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA\nBodyTrack) were evaluated against joint angles computed from IMU data using\nOpenSim inverse kinematics following the Human3.6M dataset format with 17\nkeypoints. Among them, MotionAGFormer demonstrated superior performance,\nachieving the lowest overall RMSE ($9.27\\deg \\pm 4.80\\deg$) and MAE ($7.86\\deg\n\\pm 4.18\\deg$), as well as the highest Pearson correlation ($0.86 \\pm 0.15$)\nand the highest coefficient of determination $R^{2}$ ($0.67 \\pm 0.28$). The\nresults reveal that both technologies are viable for out-of-the-lab kinematic\nassessment. However, they also highlight key trade-offs between video- and\nsensor-based approaches including costs, accessibility, and precision. This\nstudy clarifies where off-the-shelf video models already provide clinically\npromising kinematics in healthy adults and where they lag behind IMU-based\nestimates while establishing valuable guidelines for researchers and clinicians\nseeking to develop robust, cost-effective, and user-friendly solutions for\ntelehealth and remote patient monitoring.",
    "published": "2025-10-02T17:44:31Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02264v1"
  },
  {
    "id": "2510.02263v1",
    "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning\n  Problems",
    "authors": [
      "Yuxiao Qu",
      "Anikait Singh",
      "Yoonho Lee",
      "Amrith Setlur",
      "Ruslan Salakhutdinov",
      "Chelsea Finn",
      "Aviral Kumar"
    ],
    "summary": "Reasoning requires going beyond pattern matching or memorization of solutions\nto identify and implement \"algorithmic procedures\" that can be used to deduce\nanswers to hard problems. Doing so requires realizing the most relevant\nprimitives, intermediate results, or shared procedures, and building upon them.\nWhile RL post-training on long chains of thought ultimately aims to uncover\nthis kind of algorithmic behavior, most reasoning traces learned by large\nmodels fail to consistently capture or reuse procedures, instead drifting into\nverbose and degenerate exploration. To address more effective reasoning, we\nintroduce reasoning abstractions: concise natural language descriptions of\nprocedural and factual knowledge that guide the model toward learning\nsuccessful reasoning. We train models to be capable of proposing multiple\nabstractions given a problem, followed by RL that incentivizes building a\nsolution while using the information provided by these abstractions. This\nresults in a two-player RL training paradigm, abbreviated as RLAD, that jointly\ntrains an abstraction generator and a solution generator. This setup\neffectively enables structured exploration, decouples learning signals of\nabstraction proposal and solution generation, and improves generalization to\nharder problems. We also show that allocating more test-time compute to\ngenerating abstractions is more beneficial for performance than generating more\nsolutions at large test budgets, illustrating the role of abstractions in\nguiding meaningful exploration.",
    "published": "2025-10-02T17:44:23Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02263v1"
  },
  {
    "id": "2510.02259v1",
    "title": "Transformers Discover Molecular Structure Without Graph Priors",
    "authors": [
      "Tobias Kreiman",
      "Yutong Bai",
      "Fadi Atieh",
      "Elizabeth Weaver",
      "Eric Qu",
      "Aditi S. Krishnapriyan"
    ],
    "summary": "Graph Neural Networks (GNNs) are the dominant architecture for molecular\nmachine learning, particularly for molecular property prediction and machine\nlearning interatomic potentials (MLIPs). GNNs perform message passing on\npredefined graphs often induced by a fixed radius cutoff or k-nearest neighbor\nscheme. While this design aligns with the locality present in many molecular\ntasks, a hard-coded graph can limit expressivity due to the fixed receptive\nfield and slows down inference with sparse graph operations. In this work, we\ninvestigate whether pure, unmodified Transformers trained directly on Cartesian\ncoordinates$\\unicode{x2013}$without predefined graphs or physical\npriors$\\unicode{x2013}$can approximate molecular energies and forces. As a\nstarting point for our analysis, we demonstrate how to train a Transformer to\ncompetitive energy and force mean absolute errors under a matched training\ncompute budget, relative to a state-of-the-art equivariant GNN on the OMol25\ndataset. We discover that the Transformer learns physically consistent\npatterns$\\unicode{x2013}$such as attention weights that decay inversely with\ninteratomic distance$\\unicode{x2013}$and flexibly adapts them across different\nmolecular environments due to the absence of hard-coded biases. The use of a\nstandard Transformer also unlocks predictable improvements with respect to\nscaling training resources, consistent with empirical scaling laws observed in\nother domains. Our results demonstrate that many favorable properties of GNNs\ncan emerge adaptively in Transformers, challenging the necessity of hard-coded\ngraph inductive biases and pointing toward standardized, scalable architectures\nfor molecular modeling.",
    "published": "2025-10-02T17:42:10Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02259v1"
  },
  {
    "id": "2510.02253v1",
    "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag\n  Editing",
    "authors": [
      "Zihan Zhou",
      "Shilin Lu",
      "Shuli Leng",
      "Shaocong Zhang",
      "Zhuming Lian",
      "Xinlei Yu",
      "Adams Wai-Kin Kong"
    ],
    "summary": "Drag-based image editing has long suffered from distortions in the target\nregion, largely because the priors of earlier base models, Stable Diffusion,\nare insufficient to project optimized latents back onto the natural image\nmanifold. With the shift from UNet-based DDPMs to more scalable DiT with flow\nmatching (e.g., SD3.5, FLUX), generative priors have become significantly\nstronger, enabling advances across diverse editing tasks. However, drag-based\nediting has yet to benefit from these stronger priors. This work proposes the\nfirst framework to effectively harness FLUX's rich prior for drag-based\nediting, dubbed DragFlow, achieving substantial gains over baselines. We first\nshow that directly applying point-based drag editing to DiTs performs poorly:\nunlike the highly compressed features of UNets, DiT features are insufficiently\nstructured to provide reliable guidance for point-wise motion supervision. To\novercome this limitation, DragFlow introduces a region-based editing paradigm,\nwhere affine transformations enable richer and more consistent feature\nsupervision. Additionally, we integrate pretrained open-domain personalization\nadapters (e.g., IP-Adapter) to enhance subject consistency, while preserving\nbackground fidelity through gradient mask-based hard constraints. Multimodal\nlarge language models (MLLMs) are further employed to resolve task ambiguities.\nFor evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)\nfeaturing region-level dragging instructions. Extensive experiments on\nDragBench-DR and ReD Bench show that DragFlow surpasses both point-based and\nregion-based baselines, setting a new state-of-the-art in drag-based image\nediting. Code and datasets will be publicly available upon publication.",
    "published": "2025-10-02T17:39:13Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02253v1"
  },
  {
    "id": "2510.02252v1",
    "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion\n  Tracking",
    "authors": [
      "Joao Pedro Araujo",
      "Yanjie Ze",
      "Pei Xu",
      "Jiajun Wu",
      "C. Karen Liu"
    ],
    "summary": "Humanoid motion tracking policies are central to building teleoperation\npipelines and hierarchical controllers, yet they face a fundamental challenge:\nthe embodiment gap between humans and humanoid robots. Current approaches\naddress this gap by retargeting human motion data to humanoid embodiments and\nthen training reinforcement learning (RL) policies to imitate these reference\ntrajectories. However, artifacts introduced during retargeting, such as foot\nsliding, self-penetration, and physically infeasible motion are often left in\nthe reference trajectories for the RL policy to correct. While prior work has\ndemonstrated motion tracking abilities, they often require extensive reward\nengineering and domain randomization to succeed. In this paper, we\nsystematically evaluate how retargeting quality affects policy performance when\nexcessive reward tuning is suppressed. To address issues that we identify with\nexisting retargeting methods, we propose a new retargeting method, General\nMotion Retargeting (GMR). We evaluate GMR alongside two open-source\nretargeters, PHC and ProtoMotions, as well as with a high-quality closed-source\ndataset from Unitree. Using BeyondMimic for policy training, we isolate\nretargeting effects without reward tuning. Our experiments on a diverse subset\nof the LAFAN1 dataset reveal that while most motions can be tracked, artifacts\nin retargeted data significantly reduce policy robustness, particularly for\ndynamic or long sequences. GMR consistently outperforms existing open-source\nmethods in both tracking performance and faithfulness to the source motion,\nachieving perceptual fidelity and policy success rates close to the\nclosed-source baseline. Website:\nhttps://jaraujo98.github.io/retargeting_matters. Code:\nhttps://github.com/YanjieZe/GMR.",
    "published": "2025-10-02T17:39:04Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02252v1"
  },
  {
    "id": "2510.02250v1",
    "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
    "authors": [
      "Gonzalo Gonzalez-Pumariega",
      "Vincent Tu",
      "Chih-Lun Lee",
      "Jiachen Yang",
      "Ang Li",
      "Xin Eric Wang"
    ],
    "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
    "published": "2025-10-02T17:37:08Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02250v1"
  },
  {
    "id": "2510.02249v1",
    "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative\n  Entropy Regulation",
    "authors": [
      "Tianyi Jiang",
      "Yi Bin",
      "Yujuan Ding",
      "Kainian Zhu",
      "Fei Ma",
      "Jingkuan Song",
      "Heng Tao Shen"
    ],
    "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities\non complex problems using long Chain-of-Thought (CoT) reasoning. However, they\noften suffer from overthinking, meaning generating unnecessarily lengthy\nreasoning steps for simpler problems. This issue may degrade the efficiency of\nthe models and make them difficult to adapt the reasoning depth to the\ncomplexity of problems. To address this, we introduce a novel metric Token\nEntropy Cumulative Average (TECA), which measures the extent of exploration\nthroughout the reasoning process. We further propose a novel reasoning paradigm\n-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy\nRegulation (CER) mechanism. This paradigm leverages TECA to help the model\ndynamically determine the optimal point to conclude its thought process and\nprovide a final answer, thus achieving efficient reasoning. Experimental\nresults across diverse mathematical benchmarks show that our approach\nsubstantially mitigates overthinking without sacrificing problem-solving\nability. With our thinking paradigm, the average response length decreases by\nup to 71% on simpler datasets, demonstrating the effectiveness of our method in\ncreating a more efficient and adaptive reasoning process.",
    "published": "2025-10-02T17:36:50Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02249v1"
  },
  {
    "id": "2510.02245v1",
    "title": "ExGRPO: Learning to Reason from Experience",
    "authors": [
      "Runzhe Zhan",
      "Yafu Li",
      "Zhi Wang",
      "Xiaoye Qu",
      "Dongrui Liu",
      "Jing Shao",
      "Derek F. Wong",
      "Yu Cheng"
    ],
    "summary": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\nfor improving the reasoning ability of large language models. However, standard\non-policy training discards rollout experiences after a single update, leading\nto computational inefficiency and instability. While prior work on RL has\nhighlighted the benefits of reusing past experience, the role of experience\ncharacteristics in shaping learning dynamics of large reasoning models remains\nunderexplored. In this paper, we are the first to investigate what makes a\nreasoning experience valuable and identify rollout correctness and entropy as\neffective indicators of experience value. Based on these insights, we propose\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\norganizes and prioritizes valuable experiences, and employs a mixed-policy\nobjective to balance exploration with experience exploitation. Experiments on\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\nimproves reasoning performance on mathematical/general benchmarks, with an\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\nstabilizes training on both stronger and weaker models where on-policy methods\nfail. These results highlight principled experience management as a key\ningredient for efficient and scalable RLVR.",
    "published": "2025-10-02T17:31:30Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02245v1"
  },
  {
    "id": "2510.02240v1",
    "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via\n  Multi-Stage Reinforcement Learning",
    "authors": [
      "Sicheng Feng",
      "Kaiwen Tuo",
      "Song Wang",
      "Lingdong Kong",
      "Jianke Zhu",
      "Huan Wang"
    ],
    "summary": "Fine-grained visual reasoning remains a core challenge for multimodal large\nlanguage models (MLLMs). The recently introduced ReasonMap highlights this gap\nby showing that even advanced MLLMs struggle with spatial reasoning in\nstructured and information-rich settings such as transit maps, a task of clear\npractical and scientific importance. However, standard reinforcement learning\n(RL) on such tasks is impeded by sparse rewards and unstable optimization. To\naddress this, we first construct ReasonMap-Plus, an extended dataset that\nintroduces dense reward signals through Visual Question Answering (VQA) tasks,\nenabling effective cold-start training of fine-grained visual understanding\nskills. Next, we propose RewardMap, a multi-stage RL framework designed to\nimprove both visual understanding and reasoning capabilities of MLLMs.\nRewardMap incorporates two key designs. First, we introduce a difficulty-aware\nreward design that incorporates detail rewards, directly tackling the sparse\nrewards while providing richer supervision. Second, we propose a multi-stage RL\nscheme that bootstraps training from simple perception to complex reasoning\ntasks, offering a more effective cold-start strategy than conventional\nSupervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus\ndemonstrate that each component of RewardMap contributes to consistent\nperformance gains, while their combination yields the best results. Moreover,\nmodels trained with RewardMap achieve an average improvement of 3.47% across 6\nbenchmarks spanning spatial reasoning, fine-grained visual reasoning, and\ngeneral tasks beyond transit maps, underscoring enhanced visual understanding\nand reasoning capabilities.",
    "published": "2025-10-02T17:29:46Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02240v1"
  },
  {
    "id": "2510.02239v1",
    "title": "Drop-Muon: Update Less, Converge Faster",
    "authors": [
      "Kaja Gruntkowska",
      "Yassine Maziane",
      "Zheng Qu",
      "Peter Richtárik"
    ],
    "summary": "Conventional wisdom in deep learning optimization dictates updating all\nlayers at every step-a principle followed by all recent state-of-the-art\noptimizers such as Muon. In this work, we challenge this assumption, showing\nthat full-network updates can be fundamentally suboptimal, both in theory and\nin practice. We introduce a non-Euclidean Randomized Progressive Training\nmethod-Drop-Muon-a simple yet powerful framework that updates only a subset of\nlayers per step according to a randomized schedule, combining the efficiency of\nprogressive training with layer-specific non-Euclidean updates for top-tier\nperformance. We provide rigorous convergence guarantees under both layer-wise\nsmoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and\nstochastic gradient settings, marking the first such results for progressive\ntraining in the stochastic and non-smooth regime. Our cost analysis further\nreveals that full-network updates are not optimal unless a very specific\nrelationship between layer smoothness constants holds. Through controlled CNN\nexperiments, we empirically demonstrate that Drop-Muon consistently outperforms\nfull-network Muon, achieving the same accuracy up to $1.4\\times$ faster in\nwall-clock time. Together, our results suggest a shift in how large-scale\nmodels can be efficiently trained, challenging the status quo and offering a\nhighly efficient, theoretically grounded alternative to full-network updates.",
    "published": "2025-10-02T17:28:55Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02239v1"
  },
  {
    "id": "2510.02236v1",
    "title": "PUL-Inter-slice Defender: An Anomaly Detection Solution for Distributed\n  Slice Mobility Attacks",
    "authors": [
      "Ricardo Misael Ayala Molina",
      "Hyame Assem Alameddine",
      "Makan Pourzandi",
      "Chadi Assi"
    ],
    "summary": "Network Slices (NSs) are virtual networks operating over a shared physical\ninfrastructure, each designed to meet specific application requirements while\nmaintaining consistent Quality of Service (QoS). In Fifth Generation (5G)\nnetworks, User Equipment (UE) can connect to and seamlessly switch between\nmultiple NSs to access diverse services. However, this flexibility, known as\nInter-Slice Switching (ISS), introduces a potential vulnerability that can be\nexploited to launch Distributed Slice Mobility (DSM) attacks, a form of\nDistributed Denial of Service (DDoS) attack. To secure 5G networks and their\nNSs against DSM attacks, we present in this work, PUL-Inter-Slice Defender; an\nanomaly detection solution that leverages Positive Unlabeled Learning (PUL) and\nincorporates a combination of Long Short-Term Memory Autoencoders and K-Means\nclustering. PUL-Inter-Slice Defender leverages the Third Generation Partnership\nProject (3GPP) key performance indicators and performance measurement counters\nas features for its machine learning models to detect DSM attack variants while\nmaintaining robustness in the presence of contaminated training data. When\nevaluated on data collected from our 5G testbed based on the open-source\nfree5GC and UERANSIM, a UE/ Radio Access Network (RAN) simulator;\nPUL-Inter-Slice Defender achieved F1-scores exceeding 98.50% on training\ndatasets with 10% to 40% attack contamination, consistently outperforming its\ncounterpart Inter-Slice Defender and other PUL based solutions combining\nOne-Class Support Vector Machine (OCSVM) with Random Forest and XGBoost.",
    "published": "2025-10-02T17:24:17Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02236v1"
  },
  {
    "id": "2510.02232v1",
    "title": "Enhanced Arabic-language cyberbullying detection: deep embedding and\n  transformer (BERT) approaches",
    "authors": [
      "Ebtesam Jaber Aljohani",
      "Wael M. S. Yafoo"
    ],
    "summary": "Recent technological advances in smartphones and communications, including\nthe growth of such online platforms as massive social media networks such as X\n(formerly known as Twitter) endangers young people and their emotional\nwell-being by exposing them to cyberbullying, taunting, and bullying content.\nMost proposed approaches for automatically detecting cyberbullying have been\ndeveloped around the English language, and methods for detecting\nArabic-language cyberbullying are scarce. Methods for detecting Arabic-language\ncyberbullying are especially scarce. This paper aims to enhance the\neffectiveness of methods for detecting cyberbullying in Arabic-language\ncontent. We assembled a dataset of 10,662 X posts, pre-processed the data, and\nused the kappa tool to verify and enhance the quality of our annotations. We\nconducted four experiments to test numerous deep learning models for\nautomatically detecting Arabic-language cyberbullying. We first tested a long\nshort-term memory (LSTM) model and a bidirectional long short-term memory\n(Bi-LSTM) model with several experimental word embeddings. We also tested the\nLSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from\nrepresentations (BERT) and then tested them on a different experimental models\nBERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM\nwith FastText embedding word performed even better, achieving 98% accuracy. As\na result, the outcomes are generalize",
    "published": "2025-10-02T17:20:02Z",
    "pdf_url": "http://arxiv.org/abs/2510.02232v1"
  },
  {
    "id": "2510.02230v1",
    "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains\n  Language Models",
    "authors": [
      "Phuc Minh Nguyen",
      "Chinh D. La",
      "Duy M. H. Nguyen",
      "Nitesh V. Chawla",
      "Binh T. Nguyen",
      "Khoa D. Doan"
    ],
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\nmethod for improving Large Language Models' reasoning capabilities, yet recent\nevidence suggests it may paradoxically shrink the reasoning boundary rather\nthan expand it. This paper investigates the shrinkage issue of RLVR by\nanalyzing its learning dynamics and reveals two critical phenomena that explain\nthis failure. First, we expose negative interference in RLVR, where learning to\nsolve certain training problems actively reduces the likelihood of correct\nsolutions for others, leading to the decline of Pass@$k$ performance, or the\nprobability of generating a correct solution within $k$ attempts. Second, we\nuncover the winner-take-all phenomenon: RLVR disproportionately reinforces\nproblems with high likelihood, correct solutions, under the base model, while\nsuppressing other initially low-likelihood ones. Through extensive theoretical\nand empirical analysis on multiple mathematical reasoning benchmarks, we show\nthat this effect arises from the inherent on-policy sampling in standard RL\nobjectives, causing the model to converge toward narrow solution strategies.\nBased on these insights, we propose a simple yet effective data curation\nalgorithm that focuses RLVR learning on low-likelihood problems, achieving\nnotable improvement in Pass@$k$ performance. Our code is available at\nhttps://github.com/mail-research/SELF-llm-interference.",
    "published": "2025-10-02T17:17:27Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02230v1"
  },
  {
    "id": "2510.02228v1",
    "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
    "authors": [
      "Maximilian Beck",
      "Kajetan Schweighofer",
      "Sebastian Böck",
      "Sebastian Lehner",
      "Sepp Hochreiter"
    ],
    "summary": "Scaling laws play a central role in the success of Large Language Models\n(LLMs), enabling the prediction of model performance relative to compute\nbudgets prior to training. While Transformers have been the dominant\narchitecture, recent alternatives such as xLSTM offer linear complexity with\nrespect to context length while remaining competitive in the billion-parameter\nregime. We conduct a comparative investigation on the scaling behavior of\nTransformers and xLSTM along the following lines, providing insights to guide\nfuture model design and deployment. First, we study the scaling behavior for\nxLSTM in compute-optimal and over-training regimes using both IsoFLOP and\nparametric fit approaches on a wide range of model sizes (80M-7B) and number of\ntraining tokens (2B-2T). Second, we examine the dependence of optimal model\nsizes on context length, a pivotal aspect that was largely ignored in previous\nwork. Finally, we analyze inference-time scaling characteristics. Our findings\nreveal that in typical LLM training and inference scenarios, xLSTM scales\nfavorably compared to Transformers. Importantly, xLSTM's advantage widens as\ntraining and inference contexts grow.",
    "published": "2025-10-02T17:14:34Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02228v1"
  },
  {
    "id": "2510.02227v1",
    "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
    "authors": [
      "Xiaoyang Yuan",
      "Yujuan Ding",
      "Yi Bin",
      "Wenqi Shao",
      "Jinyu Cai",
      "Jingkuan Song",
      "Yang Yang",
      "Hengtao Shen"
    ],
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
    "published": "2025-10-02T17:14:00Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02227v1"
  },
  {
    "id": "2510.02226v1",
    "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
    "authors": [
      "Shira Schiber",
      "Ofir Lindenbaum",
      "Idan Schwartz"
    ],
    "summary": "Recent advances in generative video models have enabled the creation of\nhigh-quality videos based on natural language prompts. However, these models\nfrequently lack fine-grained temporal control, meaning they do not allow users\nto specify when particular visual elements should appear within a generated\nsequence. In this work, we introduce TempoControl, a method that allows for\ntemporal alignment of visual concepts during inference, without requiring\nretraining or additional supervision. TempoControl utilizes cross-attention\nmaps, a key component of text-to-video diffusion models, to guide the timing of\nconcepts through a novel optimization approach. Our method steers attention\nusing three complementary principles: aligning its temporal shape with a\ncontrol signal (via correlation), amplifying it where visibility is needed (via\nenergy), and maintaining spatial focus (via entropy). TempoControl allows\nprecise control over timing while ensuring high video quality and diversity. We\ndemonstrate its effectiveness across various video generation applications,\nincluding temporal reordering for single and multiple objects, as well as\naction and audio-aligned generation.",
    "published": "2025-10-02T17:13:35Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02226v1"
  },
  {
    "id": "2510.02224v1",
    "title": "Efficiently Generating Correlated Sample Paths from Multi-step Time\n  Series Foundation Models",
    "authors": [
      "Ethan Baron",
      "Boris Oreshkin",
      "Ruijun Ma",
      "Hanyu Zhang",
      "Kari Torkkola",
      "Michael W. Mahoney",
      "Andrew Gordon Wilson",
      "Tatiana Konstantinova"
    ],
    "summary": "Many time series applications require access to multi-step forecast\ntrajectories in the form of sample paths. Recently, time series foundation\nmodels have leveraged multi-step lookahead predictions to improve the quality\nand efficiency of multi-step forecasts. However, these models only predict\nindependent marginal distributions for each time step, rather than a full joint\npredictive distribution. To generate forecast sample paths with realistic\ncorrelation structures, one typically resorts to autoregressive sampling, which\ncan be extremely expensive. In this paper, we present a copula-based approach\nto efficiently generate accurate, correlated sample paths from existing\nmulti-step time series foundation models in one forward pass. Our copula-based\napproach generates correlated sample paths orders of magnitude faster than\nautoregressive sampling, and it yields improved sample path quality by\nmitigating the snowballing error phenomenon.",
    "published": "2025-10-02T17:08:58Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02224v1"
  },
  {
    "id": "2510.02218v1",
    "title": "Quantum Fisher information matrices from Rényi relative entropies",
    "authors": [
      "Mark M. Wilde"
    ],
    "summary": "Quantum generalizations of the Fisher information are important in quantum\ninformation science, with applications in high energy and condensed matter\nphysics and in quantum estimation theory, machine learning, and optimization.\nOne can derive a quantum generalization of the Fisher information matrix in a\nnatural way as the Hessian matrix arising in a Taylor expansion of a smooth\ndivergence. Such an approach is appealing for quantum information theorists,\ngiven the ubiquity of divergences in quantum information theory. In contrast to\nthe classical case, there is not a unique quantum generalization of the Fisher\ninformation matrix, similar to how there is not a unique quantum generalization\nof the relative entropy or the R\\'enyi relative entropy. In this paper, I\nderive information matrices arising from the log-Euclidean, $\\alpha$-$z$, and\ngeometric R\\'enyi relative entropies, with the main technical tool for doing so\nbeing the method of divided differences for calculating matrix derivatives.\nInterestingly, for all non-negative values of the R\\'enyi parameter $\\alpha$,\nthe log-Euclidean R\\'enyi relative entropy leads to the Kubo-Mori information\nmatrix, and the geometric R\\'enyi relative entropy leads to the\nright-logarithmic derivative Fisher information matrix. Thus, the resulting\ninformation matrices obey the data-processing inequality for all non-negative\nvalues of the R\\'enyi parameter $\\alpha$ even though the original quantities do\nnot. Additionally, I derive and establish basic properties of $\\alpha$-$z$\ninformation matrices resulting from the $\\alpha$-$z$ R\\'enyi relative\nentropies. For parameterized thermal states, I establish formulas for their\n$\\alpha$-$z$ information matrices and hybrid quantum-classical algorithms for\nestimating them, with applications in quantum Boltzmann machine learning.",
    "published": "2025-10-02T17:02:48Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02218v1"
  },
  {
    "id": "2510.02216v1",
    "title": "Diffusion Transformers for Imputation: Statistical Efficiency and\n  Uncertainty Quantification",
    "authors": [
      "Zeqi Ye",
      "Minshuo Chen"
    ],
    "summary": "Imputation methods play a critical role in enhancing the quality of practical\ntime-series data, which often suffer from pervasive missing values. Recently,\ndiffusion-based generative imputation methods have demonstrated remarkable\nsuccess compared to autoregressive and conventional statistical approaches.\nDespite their empirical success, the theoretical understanding of how well\ndiffusion-based models capture complex spatial and temporal dependencies\nbetween the missing values and observed ones remains limited. Our work\naddresses this gap by investigating the statistical efficiency of conditional\ndiffusion transformers for imputation and quantifying the uncertainty in\nmissing values. Specifically, we derive statistical sample complexity bounds\nbased on a novel approximation theory for conditional score functions using\ntransformers, and, through this, construct tight confidence regions for missing\nvalues. Our findings also reveal that the efficiency and accuracy of imputation\nare significantly influenced by the missing patterns. Furthermore, we validate\nthese theoretical insights through simulation and propose a mixed-masking\ntraining strategy to enhance the imputation performance.",
    "published": "2025-10-02T17:00:18Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02216v1"
  },
  {
    "id": "2510.02215v1",
    "title": "C2AL: Cohort-Contrastive Auxiliary Learning for Large-scale\n  Recommendation Systems",
    "authors": [
      "Mertcan Cokbas",
      "Ziteng Liu",
      "Zeyi Tao",
      "Chengkai Zhang",
      "Elder Veliz",
      "Qin Huang",
      "Ellie Wen",
      "Huayu Li",
      "Qiang Jin",
      "Murat Duman",
      "Benjamin Au",
      "Guy Lebanon",
      "Sagar Chordia"
    ],
    "summary": "Training large-scale recommendation models under a single global objective\nimplicitly assumes homogeneity across user populations. However, real-world\ndata are composites of heterogeneous cohorts with distinct conditional\ndistributions. As models increase in scale and complexity and as more data is\nused for training, they become dominated by central distribution patterns,\nneglecting head and tail regions. This imbalance limits the model's learning\nability and can result in inactive attention weights or dead neurons. In this\npaper, we reveal how the attention mechanism can play a key role in\nfactorization machines for shared embedding selection, and propose to address\nthis challenge by analyzing the substructures in the dataset and exposing those\nwith strong distributional contrast through auxiliary learning. Unlike previous\nresearch, which heuristically applies weighted labels or multi-task heads to\nmitigate such biases, we leverage partially conflicting auxiliary labels to\nregularize the shared representation. This approach customizes the learning\nprocess of attention layers to preserve mutual information with minority\ncohorts while improving global performance. We evaluated C2AL on massive\nproduction datasets with billions of data points each for six SOTA models.\nExperiments show that the factorization machine is able to capture fine-grained\nuser-ad interactions using the proposed method, achieving up to a 0.16%\nreduction in normalized entropy overall and delivering gains exceeding 0.30% on\ntargeted minority cohorts.",
    "published": "2025-10-02T17:00:17Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02215v1"
  },
  {
    "id": "2510.02212v1",
    "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via\n  Reinforcement Learning",
    "authors": [
      "Hanyang Zhao",
      "Dawen Liang",
      "Wenpin Tang",
      "David Yao",
      "Nathan Kallus"
    ],
    "summary": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified\nframework for training masked diffusion large language models (dLLMs) to reason\nnot only better (furious), but also faster via reinforcement learning (RL). We\nfirst unify the existing baseline approach such as d1 by proposing to train\nsurrogate policies via off-policy RL, whose likelihood is much more tractable\nas an approximation to the true dLLM policy. This naturally motivates a more\naccurate and informative two-stage likelihood approximation combined with\nimportance sampling correction, which leads to generalized RL algorithms with\nbetter sample efficiency and superior task performance. Second, we propose a\nnew direction of joint training efficient samplers/controllers of dLLMs policy.\nVia RL, we incentivize dLLMs' natural multi-token prediction capabilities by\nletting the model learn to adaptively allocate an inference threshold for each\nprompt. By jointly training the sampler, we yield better accuracies with lower\nnumber of function evaluations (NFEs) compared to training the model only,\nobtaining the best performance in improving the Pareto frontier of the\ninference-time compute of dLLMs. We showcase the effectiveness of our pipeline\nby training open source large diffusion language models over benchmark math and\nplanning tasks.",
    "published": "2025-10-02T16:57:24Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02212v1"
  },
  {
    "id": "2510.02209v1",
    "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
    "authors": [
      "Yanxu Chen",
      "Zijun Yao",
      "Yantao Liu",
      "Jin Ye",
      "Jianing Yu",
      "Lei Hou",
      "Juanzi Li"
    ],
    "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
    "published": "2025-10-02T16:54:57Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02209v1"
  },
  {
    "id": "2510.02208v1",
    "title": "Measurement-Guided Consistency Model Sampling for Inverse Problems",
    "authors": [
      "Amirreza Tanevardi",
      "Pooria Abbas Rad Moghadam",
      "Sajjad Amini"
    ],
    "summary": "Diffusion models have become powerful generative priors for solving inverse\nimaging problems, but their reliance on slow multi-step sampling limits\npractical deployment. Consistency models address this bottleneck by enabling\nhigh-quality generation in a single or only a few steps, yet their direct\nadaptation to inverse problems is underexplored. In this paper, we present a\nmodified consistency sampling approach tailored for inverse problem\nreconstruction: the sampler's stochasticity is guided by a\nmeasurement-consistency mechanism tied to the measurement operator, which\nenforces fidelity to the acquired measurements while retaining the efficiency\nof consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom\ndatasets demonstrate consistent improvements in perceptual and pixel-level\nmetrics, including Fr\\'echet Inception Distance, Kernel Inception Distance,\npeak signal-to-noise ratio, and structural similarity index measure, compared\nto baseline consistency sampling, yielding competitive or superior\nreconstructions with only a handful of steps.",
    "published": "2025-10-02T16:53:07Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02208v1"
  },
  {
    "id": "2510.02206v1",
    "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling",
    "authors": [
      "Daniel Gallo Fernández"
    ],
    "summary": "Sequence-to-sequence models have become central in Artificial Intelligence,\nparticularly following the introduction of the transformer architecture. While\ninitially developed for Natural Language Processing, these models have\ndemonstrated utility across domains, including Computer Vision. Such models\nrequire mechanisms to exchange information along the time dimension, typically\nusing recurrent or self-attention layers. However, self-attention scales\nquadratically with sequence length, limiting its practicality for very long\nsequences.\n  We introduce Poolformer, a sequence-to-sequence model that replaces\nself-attention with recurrent layers and incorporates pooling operations to\nreduce sequence length. Poolformer is defined recursively using SkipBlocks,\nwhich contain residual blocks, a down-pooling layer, a nested SkipBlock, an\nup-pooling layer, and additional residual blocks. We conduct extensive\nexperiments to support our architectural choices.\n  Our results show that pooling greatly accelerates training, improves\nperceptual metrics (FID and IS), and prevents overfitting. Our experiments also\nsuggest that long-range dependencies are handled by deep layers, while shallow\nlayers take care of short-term features.\n  Evaluated on raw audio, which naturally features long sequence lengths,\nPoolformer outperforms state-of-the-art models such as SaShiMi and Mamba.\nFuture directions include applications to text and vision, as well as\nmulti-modal scenarios, where a Poolformer-based LLM could effectively process\ndense representations of images and videos.",
    "published": "2025-10-02T16:52:45Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02206v1"
  },
  {
    "id": "2510.02202v1",
    "title": "Detection of Chagas Disease from the ECG: The George B. Moody PhysioNet\n  Challenge 2025",
    "authors": [
      "Matthew A. Reyna",
      "Zuzana Koscova",
      "Jan Pavlus",
      "Soheil Saghafi",
      "James Weigle",
      "Andoni Elola",
      "Salman Seyedi",
      "Kiersten Campbell",
      "Qiao Li",
      "Ali Bahrami Rad",
      "Antônio H. Ribeiro",
      "Antonio Luiz P. Ribeiro",
      "Reza Sameni",
      "Gari D. Clifford"
    ],
    "summary": "Objective: Chagas disease is a parasitic infection that is endemic to South\nAmerica, Central America, and, more recently, the U.S., primarily transmitted\nby insects. Chronic Chagas disease can cause cardiovascular diseases and\ndigestive problems. Serological testing capacities for Chagas disease are\nlimited, but Chagas cardiomyopathy often manifests in ECGs, providing an\nopportunity to prioritize patients for testing and treatment. Approach: The\nGeorge B. Moody PhysioNet Challenge 2025 invites teams to develop algorithmic\napproaches for identifying Chagas disease from electrocardiograms (ECGs). Main\nresults: This Challenge provides multiple innovations. First, we leveraged\nseveral datasets with labels from patient reports and serological testing,\nprovided a large dataset with weak labels and smaller datasets with strong\nlabels. Second, we augmented the data to support model robustness and\ngeneralizability to unseen data sources. Third, we applied an evaluation metric\nthat captured the local serological testing capacity for Chagas disease to\nframe the machine learning problem as a triage task. Significance: Over 630\nparticipants from 111 teams submitted over 1300 entries during the Challenge,\nrepresenting diverse approaches from academia and industry worldwide.",
    "published": "2025-10-02T16:50:36Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02202v1"
  },
  {
    "id": "2510.02197v1",
    "title": "Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition:\n  A Machine Learning Approach for Small-Scale Farming Applications",
    "authors": [
      "Emmanuel Nsengiyumvaa",
      "Leonard Niyitegekaa",
      "Eric Umuhoza"
    ],
    "summary": "Accurate livestock identification is a cornerstone of modern farming: it\nsupports health monitoring, breeding programs, and productivity tracking.\nHowever, common pig identification methods, such as ear tags and microchips,\nare often unreliable, costly, target pure breeds, and thus impractical for\nsmall-scale farmers. To address this gap, we propose a noninvasive biometric\nidentification approach that leverages uniqueness of the auricular vein\npatterns. To this end, we have collected 800 ear images from 20 mixed-breed\npigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a\nstandard smartphone and simple back lighting. A multistage computer vision\npipeline was developed to enhance vein visibility, extract structural and\nspatial features, and generate biometric signatures. These features were then\nclassified using machine learning models. Support Vector Machines (SVM)\nachieved the highest accuracy: correctly identifying pigs with 98.12% precision\nacross mixed-breed populations. The entire process from image processing to\nclassification was completed in an average of 8.3 seconds, demonstrating\nfeasibility for real-time farm deployment. We believe that by replacing fragile\nphysical identifiers with permanent biological markers, this system provides\nfarmers with a cost-effective and stress-free method of animal identification.\nMore broadly, the findings confirm the practicality of auricular vein\nbiometrics for digitizing livestock management, reinforcing its potential to\nextend the benefits of precision farming to resource-constrained agricultural\ncommunities.",
    "published": "2025-10-02T16:45:43Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02197v1"
  },
  {
    "id": "2510.02194v1",
    "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language\n  Models",
    "authors": [
      "Yuhao Sun",
      "Zhuoer Xu",
      "Shiwen Cui",
      "Kun Yang",
      "Lingyun Yu",
      "Yongdong Zhang",
      "Hongtao Xie"
    ],
    "summary": "Large Language Models (LLMs) have achieved remarkable progress across a wide\nrange of tasks, but remain vulnerable to safety risks such as harmful content\ngeneration and jailbreak attacks. Existing safety techniques -- including\nexternal guardrails, inference-time guidance, and post-training alignment --\neach face limitations in balancing safety, utility, and controllability. In\nthis work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM\nsafety through safety-aware upcycling. Our approach first identifies\nsafety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)\nstructure, where the router acts as a soft guardrail that selectively activates\noriginal MLPs and added safety experts. We further introduce a two-stage SFT\nstrategy to strengthen safety discrimination while preserving general\ncapabilities. To enable flexible control at inference time, we introduce a\nsafety temperature mechanism, allowing dynamic adjustment of the trade-off\nbetween safety and utility. Experiments across multiple benchmarks, base model,\nand model scales demonstrate that UpSafe$^\\circ$C achieves robust safety\nimprovements against harmful and jailbreak inputs, while maintaining\ncompetitive performance on general tasks. Moreover, analysis shows that safety\ntemperature provides fine-grained inference-time control that achieves the\nPareto-optimal frontier between utility and safety. Our results highlight a new\ndirection for LLM safety: moving from static alignment toward dynamic, modular,\nand inference-aware control.",
    "published": "2025-10-02T16:43:33Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02194v1"
  },
  {
    "id": "2510.02189v1",
    "title": "Hybrid Physics-ML Framework for Pan-Arctic Permafrost Infrastructure\n  Risk at Record 2.9-Million Observation Scale",
    "authors": [
      "Boris Kriuk"
    ],
    "summary": "Arctic warming threatens over 100 billion in permafrost-dependent\ninfrastructure across Northern territories, yet existing risk assessment\nframeworks lack spatiotemporal validation, uncertainty quantification, and\noperational decision-support capabilities. We present a hybrid physics-machine\nlearning framework integrating 2.9 million observations from 171,605 locations\n(2005-2021) combining permafrost fraction data with climate reanalysis. Our\nstacked ensemble model (Random Forest + Histogram Gradient Boosting + Elastic\nNet) achieves R2=0.980 (RMSE=5.01 pp) with rigorous spatiotemporal\ncross-validation preventing data leakage. To address machine learning\nlimitations in extrapolative climate scenarios, we develop a hybrid approach\ncombining learned climate-permafrost relationships (60%) with physical\npermafrost sensitivity models (40%, -10 pp/C). Under RCP8.5 forcing (+5C over\n10 years), we project mean permafrost fraction decline of -20.3 pp (median:\n-20.0 pp), with 51.5% of Arctic Russia experiencing over 20 percentage point\nloss. Infrastructure risk classification identifies 15% high-risk zones (25%\nmedium-risk) with spatially explicit uncertainty maps. Our framework represents\nthe largest validated permafrost ML dataset globally, provides the first\noperational hybrid physics-ML forecasting system for Arctic infrastructure, and\ndelivers open-source tools enabling probabilistic permafrost projections for\nengineering design codes and climate adaptation planning. The methodology is\ngeneralizable to other permafrost regions and demonstrates how hybrid\napproaches can overcome pure data-driven limitations in climate change\napplications.",
    "published": "2025-10-02T16:38:36Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02189v1"
  },
  {
    "id": "2510.02187v1",
    "title": "High-Fidelity Speech Enhancement via Discrete Audio Tokens",
    "authors": [
      "Luca A. Lanzendörfer",
      "Frédéric Berdoz",
      "Antonis Asonitis",
      "Roger Wattenhofer"
    ],
    "summary": "Recent autoregressive transformer-based speech enhancement (SE) methods have\nshown promising results by leveraging advanced semantic understanding and\ncontextual modeling of speech. However, these approaches often rely on complex\nmulti-stage pipelines and low sampling rate codecs, limiting them to narrow and\ntask-specific speech enhancement. In this work, we introduce DAC-SE1, a\nsimplified language model-based SE framework leveraging discrete\nhigh-resolution audio representations; DAC-SE1 preserves fine-grained acoustic\ndetails while maintaining semantic coherence. Our experiments show that DAC-SE1\nsurpasses state-of-the-art autoregressive SE methods on both objective\nperceptual metrics and in a MUSHRA human evaluation. We release our codebase\nand model checkpoints to support further research in scalable, unified, and\nhigh-quality speech enhancement.",
    "published": "2025-10-02T16:38:05Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02187v1"
  },
  {
    "id": "2510.02186v1",
    "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for\n  Open-Vocabulary 3D Segmentation",
    "authors": [
      "Weijia Dou",
      "Xu Zhang",
      "Yi Bin",
      "Jian Liu",
      "Bo Peng",
      "Guoqing Wang",
      "Yang Yang",
      "Heng Tao Shen"
    ],
    "summary": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to\n3D semantic segmentation expose a persistent trade-off. Directly projecting 2D\nfeatures into 3D yields noisy and fragmented predictions, whereas enforcing\ngeometric coherence necessitates costly training pipelines and large-scale\nannotated 3D data. We argue that this limitation stems from the dominant\nsegmentation-and-matching paradigm, which fails to reconcile 2D semantics with\n3D geometric structure. The geometric cues are not eliminated during the\n2D-to-3D transfer but remain latent within the noisy and view-aggregated\nfeatures. To exploit this property, we propose GeoPurify that applies a small\nStudent Affinity Network to purify 2D VLM-generated 3D point features using\ngeometric priors distilled from a 3D self-supervised teacher model. During\ninference, we devise a Geometry-Guided Pooling module to further denoise the\npoint cloud and ensure the semantic and structural consistency. Benefiting from\nlatent geometric information and the learned affinity network, GeoPurify\neffectively mitigates the trade-off and achieves superior data efficiency.\nExtensive experiments on major 3D benchmarks demonstrate that GeoPurify\nachieves or surpasses state-of-the-art performance while utilizing only about\n1.5% of the training data. Our codes and checkpoints are available at\n[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).",
    "published": "2025-10-02T16:37:56Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02186v1"
  },
  {
    "id": "2510.02182v1",
    "title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex\n  with Mutual Information-Guided Diffusion",
    "authors": [
      "Yule Wang",
      "Joseph Yu",
      "Chengrui Li",
      "Weihan Li",
      "Anqi Wu"
    ],
    "summary": "Understanding how neural populations in higher visual areas encode\nobject-centered visual information remains a central challenge in computational\nneuroscience. Prior works have investigated representational alignment between\nartificial neural networks and the visual cortex. Nevertheless, these findings\nare indirect and offer limited insights to the structure of neural populations\nthemselves. Similarly, decoding-based methods have quantified semantic features\nfrom neural populations but have not uncovered their underlying organizations.\nThis leaves open a scientific question: \"how feature-specific visual\ninformation is distributed across neural populations in higher visual areas,\nand whether it is organized into structured, semantically meaningful\nsubspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages\nthe generative power of diffusion models to visualize and validate the\nvisual-semantic attributes encoded in neural latent subspaces. Our method first\nuses a variational autoencoder to infer a group-wise disentangled neural latent\nsubspace from neural populations. Subsequently, we propose a mutual information\n(MI)-guided diffusion synthesis procedure to visualize the specific\nvisual-semantic features encoded by each latent group. We validate MIG-Vis on\nmulti-session neural spiking datasets from the inferior temporal (IT) cortex of\ntwo macaques. The synthesized results demonstrate that our method identifies\nneural latent groups with clear semantic selectivity to diverse visual\nfeatures, including object pose, inter-category transformations, and\nintra-class content. These findings provide direct, interpretable evidence of\nstructured semantic representation in the higher visual cortex and advance our\nunderstanding of its encoding principles.",
    "published": "2025-10-02T16:33:40Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02182v1"
  },
  {
    "id": "2510.02180v1",
    "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement\n  Learning",
    "authors": [
      "Silvia Sapora",
      "Devon Hjelm",
      "Alexander Toshev",
      "Omar Attia",
      "Bogdan Mazoure"
    ],
    "summary": "Inverse Reinforcement Learning aims to recover reward models from expert\ndemonstrations, but traditional methods yield \"black-box\" models that are\ndifficult to interpret and debug. In this work, we introduce GRACE (Generating\nRewards As CodE), a method for using Large Language Models within an\nevolutionary search to reverse-engineer an interpretable, code-based reward\nfunction directly from expert trajectories. The resulting reward function is\nexecutable code that can be inspected and verified. We empirically validate\nGRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns\nhighly accurate rewards, even in complex, multi-task settings. Further, we\ndemonstrate that the resulting reward leads to strong policies, compared to\nboth competitive Imitation Learning and online RL approaches with ground-truth\nrewards. Finally, we show that GRACE is able to build complex reward APIs in\nmulti-task setups.",
    "published": "2025-10-02T16:31:39Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02180v1"
  },
  {
    "id": "2510.02174v1",
    "title": "Flatness-Aware Stochastic Gradient Langevin Dynamics",
    "authors": [
      "Stefano Bruno",
      "Youngsik Hwang",
      "Jaehyeon An",
      "Sotirios Sabanis",
      "Dong-Young Lim"
    ],
    "summary": "Generalization in deep learning is closely tied to the pursuit of flat minima\nin the loss landscape, yet classical Stochastic Gradient Langevin Dynamics\n(SGLD) offers no mechanism to bias its dynamics toward such low-curvature\nsolutions. This work introduces Flatness-Aware Stochastic Gradient Langevin\nDynamics (fSGLD), designed to efficiently and provably seek flat minima in\nhigh-dimensional nonconvex optimization problems. At each iteration, fSGLD uses\nthe stochastic gradient evaluated at parameters perturbed by isotropic Gaussian\nnoise, commonly referred to as Random Weight Perturbation (RWP), thereby\noptimizing a randomized-smoothing objective that implicitly captures curvature\ninformation. Leveraging these properties, we prove that the invariant measure\nof fSGLD stays close to a stationary measure concentrated on the global\nminimizers of a loss function regularized by the Hessian trace whenever the\ninverse temperature and the scale of random weight perturbation are properly\ncoupled. This result provides a rigorous theoretical explanation for the\nbenefits of random weight perturbation. In particular, we establish\nnon-asymptotic convergence guarantees in Wasserstein distance with the best\nknown rate and derive an excess-risk bound for the Hessian-trace regularized\nobjective. Extensive experiments on noisy-label and large-scale vision tasks,\nin both training-from-scratch and fine-tuning settings, demonstrate that fSGLD\nachieves superior or comparable generalization and robustness to baseline\nalgorithms while maintaining the computational cost of SGD, about half that of\nSAM. Hessian-spectrum analysis further confirms that fSGLD converges to\nsignificantly flatter minima.",
    "published": "2025-10-02T16:24:46Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02174v1"
  },
  {
    "id": "2510.02173v1",
    "title": "Learning to Reason for Hallucination Span Detection",
    "authors": [
      "Hsuan Su",
      "Ting-Yao Hu",
      "Hema Swetha Koppula",
      "Kundan Krishna",
      "Hadi Pouransari",
      "Cheng-Yu Hsieh",
      "Cem Koc",
      "Joseph Yitan Cheng",
      "Oncel Tuzel",
      "Raviteja Vemulapalli"
    ],
    "summary": "Large language models (LLMs) often generate hallucinations -- unsupported\ncontent that undermines reliability. While most prior works frame hallucination\ndetection as a binary task, many real-world applications require identifying\nhallucinated spans, which is a multi-step decision making process. This\nnaturally raises the question of whether explicit reasoning can help the\ncomplex task of detecting hallucination spans. To answer this question, we\nfirst evaluate pretrained models with and without Chain-of-Thought (CoT)\nreasoning, and show that CoT reasoning has the potential to generate at least\none correct answer when sampled multiple times. Motivated by this, we propose\nRL4HS, a reinforcement learning framework that incentivizes reasoning with a\nspan-level reward function. RL4HS builds on Group Relative Policy Optimization\nand introduces Class-Aware Policy Optimization to mitigate reward imbalance\nissue. Experiments on the RAGTruth benchmark (summarization, question\nanswering, data-to-text) show that RL4HS surpasses pretrained reasoning models\nand supervised fine-tuning, demonstrating the necessity of reinforcement\nlearning with span-level rewards for detecting hallucination spans.",
    "published": "2025-10-02T16:24:28Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02173v1"
  },
  {
    "id": "2510.02172v1",
    "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with\n  Self-Penalization",
    "authors": [
      "Zhaoning Yu",
      "Will Su",
      "Leitian Tao",
      "Haozhu Wang",
      "Aashu Singh",
      "Hanchao Yu",
      "Jianyu Wang",
      "Hongyang Gao",
      "Weizhe Yuan",
      "Jason Weston",
      "Ping Yu",
      "Jing Xu"
    ],
    "summary": "Reinforcement learning with human-annotated data has boosted chain-of-thought\nreasoning in large reasoning models, but these gains come at high costs in\nlabeled data while faltering on harder tasks. A natural next step is\nexperience-driven learning, where models improve without curated labels by\nadapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with\nSelf-restraint), a self-penalizing RL framework that converts the absence of\ngold labels into a useful learning signal. Instead of overcommitting to\nspurious majority votes, RESTRAIN exploits signals from the model's entire\nanswer distribution: penalizing overconfident rollouts and low-consistency\nexamples while preserving promising reasoning chains. The self-penalization\nmechanism integrates seamlessly into policy optimization methods such as GRPO,\nenabling continual self-improvement without supervision. On challenging\nreasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.\nWith Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to\n+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on\nGPQA-Diamond, nearly matching gold-label training while using no gold labels.\nThese results demonstrate that RESTRAIN establishes a scalable path toward\nstronger reasoning without gold labels.",
    "published": "2025-10-02T16:24:01Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02172v1"
  },
  {
    "id": "2510.02171v1",
    "title": "Go witheFlow: Real-time Emotion Driven Audio Effects Modulation",
    "authors": [
      "Edmund Dervakos",
      "Spyridon Kantarelis",
      "Vassilis Lyberatos",
      "Jason Liartis",
      "Giorgos Stamou"
    ],
    "summary": "Music performance is a distinctly human activity, intrinsically linked to the\nperformer's ability to convey, evoke, or express emotion. Machines cannot\nperform music in the human sense; they can produce, reproduce, execute, or\nsynthesize music, but they lack the capacity for affective or emotional\nexperience. As such, music performance is an ideal candidate through which to\nexplore aspects of collaboration between humans and machines. In this paper, we\nintroduce the witheFlow system, designed to enhance real-time music performance\nby automatically modulating audio effects based on features extracted from both\nbiosignals and the audio itself. The system, currently in a proof-of-concept\nphase, is designed to be lightweight, able to run locally on a laptop, and is\nopen-source given the availability of a compatible Digital Audio Workstation\nand sensors.",
    "published": "2025-10-02T16:23:47Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02171v1"
  },
  {
    "id": "2510.02168v1",
    "title": "Wasserstein normalized autoencoder for anomaly detection",
    "authors": [
      "CMS Collaboration"
    ],
    "summary": "A novel anomaly detection algorithm is presented. The Wasserstein normalized\nautoencoder (WNAE) is a normalized probabilistic model that minimizes the\nWasserstein distance between the learned probability distribution -- a\nBoltzmann distribution where the energy is the reconstruction error of the\nautoencoder -- and the distribution of the training data. This algorithm has\nbeen developed and applied to the identification of semivisible jets -- conical\nsprays of visible standard model particles and invisible dark matter states --\nwith the CMS experiment at the CERN LHC. Trained on jets of particles from\nsimulated standard model processes, the WNAE is shown to learn the probability\ndistribution of the input data in a fully unsupervised fashion, such that it\neffectively identifies new physics jets as anomalies. The model consistently\ndemonstrates stable, convergent training and achieves strong classification\nperformance across a wide range of signals, improving upon standard normalized\nautoencoders, while remaining agnostic to the signal. The WNAE directly tackles\nthe problem of outlier reconstruction, a common failure mode of autoencoders in\nanomaly detection tasks.",
    "published": "2025-10-02T16:15:48Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02168v1"
  },
  {
    "id": "2510.02166v1",
    "title": "SIEVE: Towards Verifiable Certification for Code-datasets",
    "authors": [
      "Fatou Ndiaye Mbodji",
      "El-hacen Diallo",
      "Jordan Samhi",
      "Kui Liu",
      "Jacques Klein",
      "Tegawendé F. Bissyande"
    ],
    "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.",
    "published": "2025-10-02T16:14:23Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02166v1"
  },
  {
    "id": "2510.02162v1",
    "title": "NoMod: A Non-modular Attack on Module Learning With Errors",
    "authors": [
      "Cristian Bassotto",
      "Ermes Franch",
      "Marina Krček",
      "Stjepan Picek"
    ],
    "summary": "The advent of quantum computing threatens classical public-key cryptography,\nmotivating NIST's adoption of post-quantum schemes such as those based on the\nModule Learning With Errors (Module-LWE) problem. We present NoMod ML-Attack, a\nhybrid white-box cryptanalytic method that circumvents the challenge of\nmodeling modular reduction by treating wrap-arounds as statistical corruption\nand casting secret recovery as robust linear estimation. Our approach combines\noptimized lattice preprocessing--including reduced-vector saving and algebraic\namplification--with robust estimators trained via Tukey's Biweight loss.\nExperiments show NoMod achieves full recovery of binary secrets for dimension\n$n = 350$, recovery of sparse binomial secrets for $n = 256$, and successful\nrecovery of sparse secrets in CRYSTALS-Kyber settings with parameters $(n, k) =\n(128, 3)$ and $(256, 2)$. We release our implementation in an anonymous\nrepository https://anonymous.4open.science/r/NoMod-3BD4.",
    "published": "2025-10-02T16:12:13Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02162v1"
  },
  {
    "id": "2510.02161v1",
    "title": "Comparing Contrastive and Triplet Loss in Audio-Visual Embedding:\n  Intra-Class Variance and Greediness Analysis",
    "authors": [
      "Donghuo Zeng"
    ],
    "summary": "Contrastive loss and triplet loss are widely used objectives in deep metric\nlearning, yet their effects on representation quality remain insufficiently\nunderstood. We present a theoretical and empirical comparison of these losses,\nfocusing on intra- and inter-class variance and optimization behavior (e.g.,\ngreedy updates). Through task-specific experiments with consistent settings on\nsynthetic data and real datasets-MNIST, CIFAR-10-it is shown that triplet loss\npreserves greater variance within and across classes, supporting finer-grained\ndistinctions in the learned representations. In contrast, contrastive loss\ntends to compact intra-class embeddings, which may obscure subtle semantic\ndifferences. To better understand their optimization dynamics, By examining\nloss-decay rate, active ratio, and gradient norm, we find that contrastive loss\ndrives many small updates early on, while triplet loss produces fewer but\nstronger updates that sustain learning on hard examples. Finally, across both\nclassification and retrieval tasks on MNIST, CIFAR-10, CUB-200, and CARS196\ndatasets, our results consistently show that triplet loss yields superior\nperformance, which suggests using triplet loss for detail retention and\nhard-sample focus, and contrastive loss for smoother, broad-based embedding\nrefinement.",
    "published": "2025-10-02T16:11:46Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02161v1"
  },
  {
    "id": "2510.02159v1",
    "title": "Machine learning in lattice quantum gravity",
    "authors": [
      "Jan Ambjorn",
      "Zbigniew Drogosz",
      "Jakub Gizbert-Studnicki",
      "Andrzej Görlich",
      "Dániel Németh",
      "Marcus Reitz"
    ],
    "summary": "Using numerical data coming from Monte Carlo simulations of four-dimensional\nCausal Dynamical Triangulations, we study how automated machine learning\nalgorithms can be used to recognize transitions between different phases of\nquantum geometries observed in lattice quantum gravity. We tested seven\nsupervised and seven unsupervised machine learning models and found that most\nof them were very successful in that task, even outperforming standard methods\nbased on order parameters.",
    "published": "2025-10-02T16:10:05Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02159v1"
  },
  {
    "id": "2510.02149v1",
    "title": "Reinforcement Learning with Action-Triggered Observations",
    "authors": [
      "Alexander Ryabchenko",
      "Wenlong Mou"
    ],
    "summary": "We study reinforcement learning problems where state observations are\nstochastically triggered by actions, a constraint common in many real-world\napplications. This framework is formulated as Action-Triggered Sporadically\nTraceable Markov Decision Processes (ATST-MDPs), where each action has a\nspecified probability of triggering a state observation. We derive tailored\nBellman optimality equations for this framework and introduce the\naction-sequence learning paradigm in which agents commit to executing a\nsequence of actions until the next observation arrives. Under the linear MDP\nassumption, value-functions are shown to admit linear representations in an\ninduced action-sequence feature map. Leveraging this structure, we propose\noff-policy estimators with statistical error guarantees for such feature maps\nand introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered\nsettings. ST-LSVI-UCB achieves regret $\\widetilde\nO(\\sqrt{Kd^3(1-\\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the\nfeature dimension, and $\\gamma$ the discount factor (per-step episode\nnon-termination probability). Crucially, this work establishes the theoretical\nfoundation for learning with sporadic, action-triggered observations while\ndemonstrating that efficient learning remains feasible under such observation\nconstraints.",
    "published": "2025-10-02T16:00:50Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02149v1"
  },
  {
    "id": "2510.02148v1",
    "title": "Policy Gradient Guidance Enables Test Time Control",
    "authors": [
      "Jianing Qi",
      "Hao Tang",
      "Zhigang Zhu"
    ],
    "summary": "We introduce Policy Gradient Guidance (PGG), a simple extension of\nclassifier-free guidance from diffusion models to classical policy gradient\nmethods. PGG augments the policy gradient with an unconditional branch and\ninterpolates conditional and unconditional branches, yielding a test-time\ncontrol knob that modulates behavior without retraining. We provide a\ntheoretical derivation showing that the additional normalization term vanishes\nunder advantage estimation, leading to a clean guided policy gradient update.\nEmpirically, we evaluate PGG on discrete and continuous control benchmarks. We\nfind that conditioning dropout-central to diffusion guidance-offers gains in\nsimple discrete tasks and low sample regimes, but dropout destabilizes\ncontinuous control. Training with modestly larger guidance ($\\gamma>1$)\nconsistently improves stability, sample efficiency, and controllability. Our\nresults show that guidance, previously confined to diffusion policies, can be\nadapted to standard on-policy methods, opening new directions for controllable\nonline reinforcement learning.",
    "published": "2025-10-02T16:00:35Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02148v1"
  },
  {
    "id": "2510.02143v1",
    "title": "How to Find Fantastic Papers: Self-Rankings as a Powerful Predictor of\n  Scientific Impact Beyond Peer Review",
    "authors": [
      "Buxin Su",
      "Natalie Collina",
      "Garrett Wen",
      "Didong Li",
      "Kyunghyun Cho",
      "Jianqing Fan",
      "Bingxin Zhao",
      "Weijie Su"
    ],
    "summary": "Peer review in academic research aims not only to ensure factual correctness\nbut also to identify work of high scientific potential that can shape future\nresearch directions. This task is especially critical in fast-moving fields\nsuch as artificial intelligence (AI), yet it has become increasingly difficult\ngiven the rapid growth of submissions. In this paper, we investigate an\nunderexplored measure for identifying high-impact research: authors' own\nrankings of their multiple submissions to the same AI conference. Grounded in\ngame-theoretic reasoning, we hypothesize that self-rankings are informative\nbecause authors possess unique understanding of their work's conceptual depth\nand long-term promise. To test this hypothesis, we conducted a large-scale\nexperiment at a leading AI conference, where 1,342 researchers self-ranked\ntheir 2,592 submissions by perceived quality. Tracking outcomes over more than\na year, we found that papers ranked highest by their authors received twice as\nmany citations as their lowest-ranked counterparts; self-rankings were\nespecially effective at identifying highly cited papers (those with over 150\ncitations). Moreover, we showed that self-rankings outperformed peer review\nscores in predicting future citation counts. Our results remained robust after\naccounting for confounders such as preprint posting time and self-citations.\nTogether, these findings demonstrate that authors' self-rankings provide a\nreliable and valuable complement to peer review for identifying and elevating\nhigh-impact research in AI.",
    "published": "2025-10-02T15:50:21Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02143v1"
  },
  {
    "id": "2510.02142v1",
    "title": "Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution\n  reaction case study",
    "authors": [
      "Lena Podina",
      "Christina Humer",
      "Alexandre Duval",
      "Victor Schmidt",
      "Ali Ramlaoui",
      "Shahana Chatterjee",
      "Yoshua Bengio",
      "Alex Hernandez-Garcia",
      "David Rolnick",
      "Félix Therrien"
    ],
    "summary": "Efficient and inexpensive energy storage is essential for accelerating the\nadoption of renewable energy and ensuring a stable supply, despite fluctuations\nin sources such as wind and solar. Electrocatalysts play a key role in hydrogen\nenergy storage (HES), allowing the energy to be stored as hydrogen. However,\nthe development of affordable and high-performance catalysts for this process\nremains a significant challenge. We introduce Catalyst GFlowNet, a generative\nmodel that leverages machine learning-based predictors of formation and\nadsorption energy to design crystal surfaces that act as efficient catalysts.\nWe demonstrate the performance of the model through a proof-of-concept\napplication to the hydrogen evolution reaction, a key reaction in HES, for\nwhich we successfully identified platinum as the most efficient known catalyst.\nIn future work, we aim to extend this approach to the oxygen evolution\nreaction, where current optimal catalysts are expensive metal oxides, and open\nthe search space to discover new materials. This generative modeling framework\noffers a promising pathway for accelerating the search for novel and efficient\ncatalysts.",
    "published": "2025-10-02T15:49:39Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02142v1"
  },
  {
    "id": "2510.02139v1",
    "title": "BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic\n  Bioinformatics",
    "authors": [
      "Florensia Widjaja",
      "Zhangtianyi Chen",
      "Juexiao Zhou"
    ],
    "summary": "Bioinformatics tools are essential for complex computational biology tasks,\nyet their integration with emerging AI-agent frameworks is hindered by\nincompatible interfaces, heterogeneous input-output formats, and inconsistent\nparameter conventions. The Model Context Protocol (MCP) provides a standardized\nframework for tool-AI communication, but manually converting hundreds of\nexisting and rapidly growing specialized bioinformatics tools into\nMCP-compliant servers is labor-intensive and unsustainable. Here, we present\nBioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter,\nwhich automatically generates robust MCP servers from tool documentation using\nlarge language models, and BioinfoMCP Benchmark, which systematically validates\nthe reliability and versatility of converted tools across diverse computational\ntasks. We present a platform of 38 MCP-converted bioinformatics tools,\nextensively validated to show that 94.7% successfully executed complex\nworkflows across three widely used AI-agent platforms. By removing technical\nbarriers to AI automation, BioinfoMCP enables natural-language interaction with\nsophisticated bioinformatics analyses without requiring extensive programming\nexpertise, offering a scalable path to intelligent, interoperable computational\nbiology.",
    "published": "2025-10-02T15:47:59Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02139v1"
  },
  {
    "id": "2510.02137v1",
    "title": "Improving Survival Models in Healthcare by Balancing Imbalanced Cohorts:\n  A Novel Approach",
    "authors": [
      "Catherine Ning",
      "Dimitris Bertsimas",
      "Johan Gagnière",
      "Stefan Buettner",
      "Per Eystein Loenning",
      "Hideo Baba",
      "Itaru Endo",
      "Georgios Stasinos",
      "Richard Burkhart",
      "Federico N. Auecio",
      "Felix Balzer",
      "Cornelis Verhoef",
      "Martin E. Kreis",
      "Georgios Antonios Margonis"
    ],
    "summary": "We explore whether survival model performance in underrepresented high- and\nlow-risk subgroups - regions of the prognostic spectrum where clinical\ndecisions are most consequential - can be improved through targeted\nrestructuring of the training dataset. Rather than modifying model\narchitecture, we propose a novel risk-stratified sampling method that addresses\nimbalances in prognostic subgroup density to support more reliable learning in\nunderrepresented tail strata. We introduce a novel methodology that partitions\npatients by baseline prognostic risk and applies matching within each stratum\nto equalize representation across the risk distribution. We implement this\nframework on a cohort of 1,799 patients with resected colorectal liver\nmetastases (CRLM), including 1,197 who received adjuvant chemotherapy and 602\nwho did not. All models used in this study are Cox proportional hazards models\ntrained on the same set of selected variables. Model performance is assessed\nvia Harrell's C index, time-dependent AUC, and Integrated Calibration Index\n(ICI), with internal validation using Efron's bias-corrected bootstrapping.\nExternal validation is conducted on two independent CRLM datasets. Cox models\ntrained on risk-balanced cohorts showed consistent improvements in internal\nvalidation compared to models trained on the full dataset while noticeably\nenhancing stratified C-index values in underrepresented high- and low-risk\nstrata of the external cohorts. Our findings suggest that survival model\nperformance in observational oncology cohorts can be meaningfully improved\nthrough targeted rebalancing of the training data across prognostic risk\nstrata. This approach offers a practical and model-agnostic complement to\nexisting methods, especially in applications where predictive reliability\nacross the full risk continuum is critical to downstream clinical decisions.",
    "published": "2025-10-02T15:45:05Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02137v1"
  },
  {
    "id": "2510.02134v1",
    "title": "Interference Resilient Quantum Receivers with Rydberg Atoms",
    "authors": [
      "Javane Rostampoor",
      "Raviraj Adve"
    ],
    "summary": "Quantum sensing has attracted significant attention due to its ability to\nmeasure physical quantities with extremely high accuracy. Rydberg atoms -\ntypically alkali atoms with a highly excited valence electron that is far from\nthe nucleus - exhibit strong sensitivity to external electromagnetic fields.\nThis sensitivity leads to coupling between different atomic energy levels,\nwhich can be observed by monitoring changes in a control laser beam before and\nafter it passes through a vapor cell containing the Rydberg atoms. By analyzing\nthe transmitted laser signal with a photodetector, variations in transmission\ncan be attributed to the presence and characteristics of the external\nelectromagnetic field. Because Rydberg atoms operate in a highly excited\nquantum state without relying on traditional electronic circuitry, they\ninherently avoid thermal noise, thereby enabling more sensitive detection. In\nthis paper, we investigate the performance of a Rydberg atomic receiver based\non Rb-85 and compare it with that of a conventional receiver in detecting an\n8-level pulse amplitude modulation (8-PAM) signal in the presence of\noff-resonant interference. We demonstrate that the Rydberg receiver can\nsuppress interference without the need for an additional filter. Effectively,\nour results show that the Rydberg receiver serves as an integrated filter and\ndemodulator, outperforming conventional circuit-based receivers in terms of\nachievable symbol error rate",
    "published": "2025-10-02T15:44:11Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02134v1"
  },
  {
    "id": "2510.02133v1",
    "title": "FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic\n  Documents for Training Document Understanding Models",
    "authors": [
      "Karan Dua",
      "Hitesh Laxmichand Patel",
      "Puneet Mittal",
      "Ranjeet Gupta",
      "Amit Agarwal",
      "Praneet Pabolu",
      "Srikant Panda",
      "Hansa Meghwani",
      "Graham Horwood",
      "Fahad Shah"
    ],
    "summary": "Developing document understanding models at enterprise scale requires large,\ndiverse, and well-annotated datasets spanning a wide range of document types.\nHowever, collecting such data is prohibitively expensive due to privacy\nconstraints, legal restrictions, and the sheer volume of manual annotation\nneeded - costs that can scale into millions of dollars. We introduce FlexDoc, a\nscalable synthetic data generation framework that combines Stochastic Schemas\nand Parameterized Sampling to produce realistic, multilingual semi-structured\ndocuments with rich annotations. By probabilistically modeling layout patterns,\nvisual structure, and content variability, FlexDoc enables the controlled\ngeneration of diverse document variants at scale. Experiments on Key\nInformation Extraction (KIE) tasks demonstrate that FlexDoc-generated data\nimproves the absolute F1 Score by up to 11% when used to augment real datasets,\nwhile reducing annotation effort by over 90% compared to traditional\nhard-template methods. The solution is in active deployment, where it has\naccelerated the development of enterprise-grade document understanding models\nwhile significantly reducing data acquisition and annotation costs.",
    "published": "2025-10-02T15:42:35Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02133v1"
  },
  {
    "id": "2510.02123v1",
    "title": "Identifying Subgroup and Context Effects in Conjoint Experiments",
    "authors": [
      "Steven Wang",
      "Isys Johnson",
      "Jessica Grogan",
      "Lalit Jain",
      "Atri Rudra",
      "Kyle Hunt",
      "Kenneth Joseph"
    ],
    "summary": "Conjoint experiments have become central to survey research in political\nscience and related fields because they allow researchers to study preferences\nacross multiple attributes simultaneously. Beyond estimating main effects,\nscholars increasingly analyze heterogeneity through subgroup analysis and\ncontextual variables, raising methodological challenges in detecting and\ninterpreting interaction effects. Statistical power constraints, common in\nsurvey experiments, further complicate this task. This paper addresses the\nquestion: how can both main and interaction effects be reliably inferred in\nconjoint studies? We contribute in two ways. First, we conduct a systematic\nevaluation of leading approaches, including post-hoc corrections, sparse\nregression methods, and Bayesian models, across simulation regimes that vary\nsparsity, noise, and data availability. Second, we propose a novel black-box\ninference framework that leverages machine learning to recover main and\ninteraction effects in conjoint experiments. Our approach balances\ncomputational efficiency with accuracy, providing a practical tool for\nresearchers studying heterogeneous effects.",
    "published": "2025-10-02T15:34:41Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02123v1"
  },
  {
    "id": "2510.02120v1",
    "title": "VarCoNet: A variability-aware self-supervised framework for functional\n  connectome extraction from resting-state fMRI",
    "authors": [
      "Charalampos Lamprou",
      "Aamna Alshehhi",
      "Leontios J. Hadjileontiadis",
      "Mohamed L. Seghier"
    ],
    "summary": "Accounting for inter-individual variability in brain function is key to\nprecision medicine. Here, by considering functional inter-individual\nvariability as meaningful data rather than noise, we introduce VarCoNet, an\nenhanced self-supervised framework for robust functional connectome (FC)\nextraction from resting-state fMRI (rs-fMRI) data. VarCoNet employs\nself-supervised contrastive learning to exploit inherent functional\ninter-individual variability, serving as a brain function encoder that\ngenerates FC embeddings readily applicable to downstream tasks even in the\nabsence of labeled data. Contrastive learning is facilitated by a novel\naugmentation strategy based on segmenting rs-fMRI signals. At its core,\nVarCoNet integrates a 1D-CNN-Transformer encoder for advanced time-series\nprocessing, enhanced with a robust Bayesian hyperparameter optimization. Our\nVarCoNet framework is evaluated on two downstream tasks: (i) subject\nfingerprinting, using rs-fMRI data from the Human Connectome Project, and (ii)\nautism spectrum disorder (ASD) classification, using rs-fMRI data from the\nABIDE I and ABIDE II datasets. Using different brain parcellations, our\nextensive testing against state-of-the-art methods, including 13 deep learning\nmethods, demonstrates VarCoNet's superiority, robustness, interpretability, and\ngeneralizability. Overall, VarCoNet provides a versatile and robust framework\nfor FC analysis in rs-fMRI.",
    "published": "2025-10-02T15:29:17Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02120v1"
  },
  {
    "id": "2510.02119v1",
    "title": "Non-Asymptotic Analysis of Data Augmentation for Precision Matrix\n  Estimation",
    "authors": [
      "Lucas Morisset",
      "Adrien Hardy",
      "Alain Durmus"
    ],
    "summary": "This paper addresses the problem of inverse covariance (also known as\nprecision matrix) estimation in high-dimensional settings. Specifically, we\nfocus on two classes of estimators: linear shrinkage estimators with a target\nproportional to the identity matrix, and estimators derived from data\naugmentation (DA). Here, DA refers to the common practice of enriching a\ndataset with artificial samples--typically generated via a generative model or\nthrough random transformations of the original data--prior to model fitting.\nFor both classes of estimators, we derive estimators and provide concentration\nbounds for their quadratic error. This allows for both method comparison and\nhyperparameter tuning, such as selecting the optimal proportion of artificial\nsamples. On the technical side, our analysis relies on tools from random matrix\ntheory. We introduce a novel deterministic equivalent for generalized resolvent\nmatrices, accommodating dependent samples with specific structure. We support\nour theoretical results with numerical experiments.",
    "published": "2025-10-02T15:28:14Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02119v1"
  },
  {
    "id": "2510.02117v1",
    "title": "DAG DECORation: Continuous Optimization for Structure Learning under\n  Hidden Confounding",
    "authors": [
      "Samhita Pal",
      "James O'quinn",
      "Kaveh Aryan",
      "Heather Pua",
      "James P. Long",
      "Amir Asiaee"
    ],
    "summary": "We study structure learning for linear Gaussian SEMs in the presence of\nlatent confounding. Existing continuous methods excel when errors are\nindependent, while deconfounding-first pipelines rely on pervasive factor\nstructure or nonlinearity. We propose \\textsc{DECOR}, a single likelihood-based\nand fully differentiable estimator that jointly learns a DAG and a correlated\nnoise model. Our theory gives simple sufficient conditions for global parameter\nidentifiability: if the mixed graph is bow free and the noise covariance has a\nuniform eigenvalue margin, then the map from $(\\B,\\OmegaMat)$ to the\nobservational covariance is injective, so both the directed structure and the\nnoise are uniquely determined. The estimator alternates a smooth-acyclic graph\nupdate with a convex noise update and can include a light bow complementarity\npenalty or a post hoc reconciliation step. On synthetic benchmarks that vary\nconfounding density, graph density, latent rank, and dimension with $n<p$,\n\\textsc{DECOR} matches or outperforms strong baselines and is especially robust\nwhen confounding is non-pervasive, while remaining competitive under\npervasiveness.",
    "published": "2025-10-02T15:23:30Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02117v1"
  },
  {
    "id": "2510.02116v1",
    "title": "Ensemble Threshold Calibration for Stable Sensitivity Control",
    "authors": [
      "John N. Daras"
    ],
    "summary": "Precise recall control is critical in large-scale spatial conflation and\nentity-matching tasks, where missing even a few true matches can break\ndownstream analytics, while excessive manual review inflates cost. Classical\nconfidence-interval cuts such as Clopper-Pearson or Wilson provide lower bounds\non recall, but they routinely overshoot the target by several percentage points\nand exhibit high run-to-run variance under skewed score distributions. We\npresent an end-to-end framework that achieves exact recall with sub-percent\nvariance over tens of millions of geometry pairs, while remaining TPU-friendly.\nOur pipeline starts with an equigrid bounding-box filter and compressed sparse\nrow (CSR) candidate representation, reducing pair enumeration by two orders of\nmagnitude. A deterministic xxHash bootstrap sample trains a lightweight neural\nranker; its scores are propagated to all remaining pairs via a single forward\npass and used to construct a reproducible, score-decile-stratified calibration\nset. Four complementary threshold estimators - Clopper-Pearson, Jeffreys,\nWilson, and an exact quantile - are aggregated via inverse-variance weighting,\nthen fused across nine independent subsamples. This ensemble reduces threshold\nvariance compared to any single method. Evaluated on two real cadastral\ndatasets (approximately 6.31M and 67.34M pairs), our approach consistently hits\na recall target within a small error, decreases redundant verifications\nrelative to other calibrations, and runs end-to-end on a single TPU v3 core.",
    "published": "2025-10-02T15:22:28Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02116v1"
  },
  {
    "id": "2510.02115v1",
    "title": "Hybrid Deep Learning Modeling Approach to Predict Natural Gas\n  Consumption of Home Subscribers on Limited Data",
    "authors": [
      "Milad Firoozeh",
      "Nader Dashti",
      "Mohammad Ali Hatefi"
    ],
    "summary": "Today, natural gas, as a clean fuel and the best alternative to crude oil,\ncovers a significant part of global demand. Iran is one of the largest\ncountries with energy resources and in terms of gas is the second-largest\ncountry in the world. But, due to the increase in population and energy\nconsumption, it faces problems such as pressure drops and gas outages yearly in\ncold seasons and therefore it is necessary to control gas consumption,\nespecially in the residential sector, which has the largest share in Iran. This\nstudy aims to analyze and predict gas consumption for residential customers in\nZanjan province, Iran, using machine learning models, including LSTM, GRU, and\na hybrid BiLSTM-XGBoost model. The dataset consists of gas consumption and\nmeteorology data collected over six years, from 2017 to 2022. The models were\ntrained and evaluated based on their ability to accurately predict consumption\npatterns. The results indicate that the hybrid BiLSTM-XGBoost model\noutperformed the other models in terms of accuracy, with lower Root Mean\nSquared Error (RMSE), Mean Absolute Percentage Error (MAPE) values, and Mean\nPercentage Error (MPE). Additionally, the Hybrid model demonstrated robust\nperformance, particularly in scenarios with limited data. The findings suggest\nthat machine learning approaches, particularly hybrid models, can be\neffectively utilized to manage and predict gas consumption, contributing to\nmore efficient resource management and reducing seasonal shortages. This study\nhighlights the importance of incorporating geographical and climatic factors in\npredictive modeling, as these significantly influence gas usage across\ndifferent regions.",
    "published": "2025-10-02T15:22:19Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02115v1"
  },
  {
    "id": "2510.02114v1",
    "title": "FRIEREN: Federated Learning with Vision-Language Regularization for\n  Segmentation",
    "authors": [
      "Ding-Ruei Shen"
    ],
    "summary": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic\nSegmentation (SS) tasks to adapt to new domains, but faces significant\nchallenges from these domain shifts, particularly when client data is\nunlabeled. However, most existing FL methods unrealistically assume access to\nlabeled data on remote clients or fail to leverage the power of modern Vision\nFoundation Models (VFMs). Here, we propose a novel and challenging task,\nFFREEDG, in which a model is pretrained on a server's labeled source dataset\nand subsequently trained across clients using only their unlabeled data,\nwithout ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a\nframework that leverages the knowledge of a VFM by integrating vision and\nlanguage modalities. Our approach employs a Vision-Language decoder guided by\nCLIP-based text embeddings to improve semantic disambiguation and uses a\nweak-to-strong consistency learning strategy for robust local training on\npseudo-labels. Our experiments on synthetic-to-real and\nclear-to-adverse-weather benchmarks demonstrate that our framework effectively\ntackles this new task, achieving competitive performance against established\ndomain generalization and adaptation methods and setting a strong baseline for\nfuture research.",
    "published": "2025-10-02T15:21:49Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02114v1"
  },
  {
    "id": "2510.02110v1",
    "title": "SoundReactor: Frame-level Online Video-to-Audio Generation",
    "authors": [
      "Koichi Saito",
      "Julian Tanke",
      "Christian Simon",
      "Masato Ishii",
      "Kazuki Shimada",
      "Zachary Novack",
      "Zhi Zhong",
      "Akio Hayakawa",
      "Takashi Shibuya",
      "Yuki Mitsufuji"
    ],
    "summary": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming\nan entire video sequence or chunks of frames are available beforehand. This\ncritically limits their use in interactive applications such as live content\ncreation and emerging generative world models. To address this gap, we\nintroduce the novel task of frame-level online V2A generation, where a model\nautoregressively generates audio from video without access to future video\nframes. Furthermore, we propose SoundReactor, which, to the best of our\nknowledge, is the first simple yet effective framework explicitly tailored for\nthis task. Our design enforces end-to-end causality and targets low per-frame\nlatency with audio-visual synchronization. Our model's backbone is a\ndecoder-only causal transformer over continuous audio latents. For vision\nconditioning, it leverages grid (patch) features extracted from the smallest\nvariant of the DINOv2 vision encoder, which are aggregated into a single token\nper frame to maintain end-to-end causality and efficiency. The model is trained\nthrough a diffusion pre-training followed by consistency fine-tuning to\naccelerate the diffusion head decoding. On a benchmark of diverse gameplay\nvideos from AAA titles, our model successfully generates semantically and\ntemporally aligned, high-quality full-band stereo audio, validated by both\nobjective and human evaluations. Furthermore, our model achieves low per-frame\nwaveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on\n30FPS, 480p videos using a single H100. Demo samples are available at\nhttps://koichi-saito-sony.github.io/soundreactor/.",
    "published": "2025-10-02T15:18:00Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02110v1"
  },
  {
    "id": "2510.02109v1",
    "title": "SpurBreast: A Curated Dataset for Investigating Spurious Correlations in\n  Real-world Breast MRI Classification",
    "authors": [
      "Jong Bum Won",
      "Wesley De Neve",
      "Joris Vankerschaver",
      "Utku Ozbulak"
    ],
    "summary": "Deep neural networks (DNNs) have demonstrated remarkable success in medical\nimaging, yet their real-world deployment remains challenging due to spurious\ncorrelations, where models can learn non-clinical features instead of\nmeaningful medical patterns. Existing medical imaging datasets are not designed\nto systematically study this issue, largely due to restrictive licensing and\nlimited supplementary patient data. To address this gap, we introduce\nSpurBreast, a curated breast MRI dataset that intentionally incorporates\nspurious correlations to evaluate their impact on model performance. Analyzing\nover 100 features involving patient, device, and imaging protocol, we identify\ntwo dominant spurious signals: magnetic field strength (a global feature\ninfluencing the entire image) and image orientation (a local feature affecting\nspatial alignment). Through controlled dataset splits, we demonstrate that DNNs\ncan exploit these non-clinical signals, achieving high validation accuracy\nwhile failing to generalize to unbiased test data. Alongside these two datasets\ncontaining spurious correlations, we also provide benchmark datasets without\nspurious correlations, allowing researchers to systematically investigate\nclinically relevant and irrelevant features, uncertainty estimation,\nadversarial robustness, and generalization strategies. Models and datasets are\navailable at https://github.com/utkuozbulak/spurbreast.",
    "published": "2025-10-02T15:16:20Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02109v1"
  },
  {
    "id": "2510.02108v1",
    "title": "Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant\n  Neural Network",
    "authors": [
      "Jinshuo Zhang",
      "Yafei Wang",
      "Xinping Yi",
      "Wenjin Wang",
      "Shi Jin",
      "Symeon Chatzinotas",
      "Björn Ottersten"
    ],
    "summary": "Although symbol-level precoding (SLP) based on constructive interference (CI)\nexploitation offers performance gains, its high complexity remains a\nbottleneck. This paper addresses this challenge with an end-to-end deep\nlearning (DL) framework with low inference complexity that leverages the\nstructure of the optimal SLP solution in the closed-form and its inherent\ntensor equivariance (TE), where TE denotes that a permutation of the input\ninduces the corresponding permutation of the output. Building upon the\ncomputationally efficient model-based formulations, as well as their known\nclosed-form solutions, we analyze their relationship with linear precoding (LP)\nand investigate the corresponding optimality condition. We then construct a\nmapping from the problem formulation to the solution and prove its TE, based on\nwhich the designed networks reveal a specific parameter-sharing pattern that\ndelivers low computational complexity and strong generalization. Leveraging\nthese, we propose the backbone of the framework with an attention-based TE\nmodule, achieving linear computational complexity. Furthermore, we demonstrate\nthat such a framework is also applicable to imperfect CSI scenarios, where we\ndesign a TE-based network to map the CSI, statistics, and symbols to auxiliary\nvariables. Simulation results show that the proposed framework captures\nsubstantial performance gains of optimal SLP, while achieving an approximately\n80-times speedup over conventional methods and maintaining strong\ngeneralization across user numbers and symbol block lengths.",
    "published": "2025-10-02T15:15:50Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02108v1"
  },
  {
    "id": "2510.02107v1",
    "title": "PENEX: AdaBoost-Inspired Neural Network Regularization",
    "authors": [
      "Klaus-Rudolf Kladny",
      "Bernhard Schölkopf",
      "Michael Muehlebach"
    ],
    "summary": "AdaBoost sequentially fits so-called weak learners to minimize an exponential\nloss, which penalizes mislabeled data points more severely than other loss\nfunctions like cross-entropy. Paradoxically, AdaBoost generalizes well in\npractice as the number of weak learners grows. In the present work, we\nintroduce Penalized Exponential Loss (PENEX), a new formulation of the\nmulti-class exponential loss that is theoretically grounded and, in contrast to\nthe existing formulation, amenable to optimization via first-order methods. We\ndemonstrate both empirically and theoretically that PENEX implicitly maximizes\nmargins of data points. Also, we show that gradient increments on PENEX\nimplicitly parameterize weak learners in the boosting framework. Across\ncomputer vision and language tasks, we show that PENEX exhibits a regularizing\neffect often better than established methods with similar computational cost.\nOur results highlight PENEX's potential as an AdaBoost-inspired alternative for\neffective training and fine-tuning of deep neural networks.",
    "published": "2025-10-02T15:13:02Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02107v1"
  },
  {
    "id": "2510.02097v1",
    "title": "Mapping Historic Urban Footprints in France: Balancing Quality,\n  Scalability and AI Techniques",
    "authors": [
      "Walid Rabehi",
      "Marion Le Texier",
      "Rémi Lemoy"
    ],
    "summary": "Quantitative analysis of historical urban sprawl in France before the 1970s\nis hindered by the lack of nationwide digital urban footprint data. This study\nbridges this gap by developing a scalable deep learning pipeline to extract\nurban areas from the Scan Histo historical map series (1925-1950), which\nproduces the first open-access, national-scale urban footprint dataset for this\npivotal period. Our key innovation is a dual-pass U-Net approach designed to\nhandle the high radiometric and stylistic complexity of historical maps. The\nfirst pass, trained on an initial dataset, generates a preliminary map that\nidentifies areas of confusion, such as text and roads, to guide targeted data\naugmentation. The second pass uses a refined dataset and the binarized output\nof the first model to minimize radiometric noise, which significantly reduces\nfalse positives. Deployed on a high-performance computing cluster, our method\nprocesses 941 high-resolution tiles covering the entirety of metropolitan\nFrance. The final mosaic achieves an overall accuracy of 73%, effectively\ncapturing diverse urban patterns while overcoming common artifacts like labels\nand contour lines. We openly release the code, training datasets, and the\nresulting nationwide urban raster to support future research in long-term\nurbanization dynamics.",
    "published": "2025-10-02T15:04:53Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02097v1"
  },
  {
    "id": "2510.02096v1",
    "title": "Learning Model Representations Using Publicly Available Model Hubs",
    "authors": [
      "Damian Falk",
      "Konstantin Schürholt",
      "Konstantinos Tzevelekakis",
      "Léo Meynent",
      "Damian Borth"
    ],
    "summary": "The weights of neural networks have emerged as a novel data modality, giving\nrise to the field of weight space learning. A central challenge in this area is\nthat learning meaningful representations of weights typically requires large,\ncarefully constructed collections of trained models, typically referred to as\nmodel zoos. These model zoos are often trained ad-hoc, requiring large\ncomputational resources, constraining the learned weight space representations\nin scale and flexibility. In this work, we drop this requirement by training a\nweight space learning backbone on arbitrary models downloaded from large,\nunstructured model repositories such as Hugging Face. Unlike curated model\nzoos, these repositories contain highly heterogeneous models: they vary in\narchitecture and dataset, and are largely undocumented. To address the\nmethodological challenges posed by such heterogeneity, we propose a new weight\nspace backbone designed to handle unstructured model populations. We\ndemonstrate that weight space representations trained on models from Hugging\nFace achieve strong performance, often outperforming backbones trained on\nlaboratory-generated model zoos. Finally, we show that the diversity of the\nmodel weights in our training set allows our weight space model to generalize\nto unseen data modalities. By demonstrating that high-quality weight space\nrepresentations can be learned in the wild, we show that curated model zoos are\nnot indispensable, thereby overcoming a strong limitation currently faced by\nthe weight space learning community.",
    "published": "2025-10-02T15:04:31Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02096v1"
  },
  {
    "id": "2510.02091v1",
    "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and\n  Reasoning",
    "authors": [
      "Xinyuan Song",
      "Keyu Wang",
      "PengXiang Li",
      "Lu Yin",
      "Shiwei Liu"
    ],
    "summary": "Recent studies suggest that the deeper layers of Large Language Models (LLMs)\ncontribute little to representation learning and can often be removed without\nsignificant performance loss. However, such claims are typically drawn from\nnarrow evaluations and may overlook important aspects of model behavior. In\nthis work, we present a systematic study of depth utilization across diverse\ndimensions, including evaluation protocols, task categories, and model\narchitectures. Our analysis confirms that very deep layers are generally less\neffective than earlier ones, but their contributions vary substantially with\nthe evaluation setting. Under likelihood-based metrics without generation,\npruning most layers preserves performance, with only the initial few being\ncritical. By contrast, generation-based evaluation uncovers indispensable roles\nfor middle and deeper layers in enabling reasoning and maintaining long-range\ncoherence. We further find that knowledge and retrieval are concentrated in\nshallow components, whereas reasoning accuracy relies heavily on deeper layers\n-- yet can be reshaped through distillation. These results highlight that depth\nusage in LLMs is highly heterogeneous and context-dependent, underscoring the\nneed for task-, metric-, and model-aware perspectives in both interpreting and\ncompressing large models.",
    "published": "2025-10-02T14:57:13Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02091v1"
  },
  {
    "id": "2510.02084v1",
    "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
    "authors": [
      "Kuiye Ding",
      "Fanda Fan",
      "Zheya Wang",
      "Hongxiao Li",
      "Yifan Wang",
      "Lei Wang",
      "Chunjie Luo",
      "Jianfeng Zhan"
    ],
    "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
    "published": "2025-10-02T14:50:50Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02084v1"
  },
  {
    "id": "2510.02081v1",
    "title": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of\n  Reconstructions",
    "authors": [
      "Zhaoyi Li",
      "Jingtao Ding",
      "Yong Li",
      "Shihua Li"
    ],
    "summary": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks\nespecially in robotic manipulation. Building upon the foundations of diffusion\nmodels, the simulation-free paradigm of FM enables simple and efficient\ntraining, but inherently introduces a train-inference gap. Specifically, we\ncannot assess the model's output during the training phase. In contrast, other\ngenerative models including Variational Autoencoder (VAE), Normalizing Flow and\nGenerative Adversarial Networks (GANs) directly optimize on the reconstruction\nloss. Such a gap is particularly evident in scenarios that demand high\nprecision, such as robotic manipulation. Moreover, we show that FM's\nover-pursuit of straight predefined paths may introduce some serious problems\nsuch as stiffness into the system. These motivate us to fine-tune FM via\nMaximum Likelihood Estimation of reconstructions - an approach made feasible by\nFM's underlying smooth ODE formulation, in contrast to the stochastic\ndifferential equations (SDEs) used in diffusion models. This paper first\ntheoretically analyzes the relation between training loss and inference error\nin FM. Then we propose a method of fine-tuning FM via Maximum Likelihood\nEstimation of reconstructions, which includes both straightforward fine-tuning\nand residual-based fine-tuning approaches. Furthermore, through specifically\ndesigned architectures, the residual-based fine-tuning can incorporate the\ncontraction property into the model, which is crucial for the model's\nrobustness and interpretability. Experimental results in image generation and\nrobotic manipulation verify that our method reliably improves the inference\nperformance of FM.",
    "published": "2025-10-02T14:49:47Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02081v1"
  },
  {
    "id": "2510.02073v1",
    "title": "Inferring Optical Tissue Properties from Photoplethysmography using\n  Hybrid Amortized Inference",
    "authors": [
      "Jens Behrmann",
      "Maria R. Cervera",
      "Antoine Wehenkel",
      "Andrew C. Miller",
      "Albert Cerussi",
      "Pranay Jain",
      "Vivek Venugopal",
      "Shijie Yan",
      "Guillermo Sapiro",
      "Luca Pegolotti",
      "Jörn-Henrik Jacobsen"
    ],
    "summary": "Smart wearables enable continuous tracking of established biomarkers such as\nheart rate, heart rate variability, and blood oxygen saturation via\nphotoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer\nphysiological information, as recent deep learning (DL) studies demonstrate.\nHowever, DL models often rely on features with unclear physiological meaning,\ncreating a tension between predictive power, clinical interpretability, and\nsensor design. We address this gap by introducing PPGen, a biophysical model\nthat relates PPG signals to interpretable physiological and optical parameters.\nBuilding on PPGen, we propose hybrid amortized inference (HAI), enabling fast,\nrobust, and scalable estimation of relevant physiological parameters from PPG\nsignals while correcting for model misspecification. In extensive in-silico\nexperiments, we show that HAI can accurately infer physiological parameters\nunder diverse noise and sensor conditions. Our results illustrate a path toward\nPPG models that retain the fidelity needed for DL-based features while\nsupporting clinical interpretation and informed hardware design.",
    "published": "2025-10-02T14:36:02Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02073v1"
  },
  {
    "id": "2510.02067v1",
    "title": "Adaptive Kernel Selection for Stein Variational Gradient Descent",
    "authors": [
      "Moritz Melcher",
      "Simon Weissmann",
      "Ashia C. Wilson",
      "Jakob Zech"
    ],
    "summary": "A central challenge in Bayesian inference is efficiently approximating\nposterior distributions. Stein Variational Gradient Descent (SVGD) is a popular\nvariational inference method which transports a set of particles to approximate\na target distribution. The SVGD dynamics are governed by a reproducing kernel\nHilbert space (RKHS) and are highly sensitive to the choice of the kernel\nfunction, which directly influences both convergence and approximation quality.\nThe commonly used median heuristic offers a simple approach for setting kernel\nbandwidths but lacks flexibility and often performs poorly, particularly in\nhigh-dimensional settings. In this work, we propose an alternative strategy for\nadaptively choosing kernel parameters over an abstract family of kernels.\nRecent convergence analyses based on the kernelized Stein discrepancy (KSD)\nsuggest that optimizing the kernel parameters by maximizing the KSD can improve\nperformance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a\nmethod that alternates between updating the particles via SVGD and adaptively\ntuning kernel bandwidths through gradient ascent on the KSD. We provide a\nsimplified theoretical analysis that extends existing results on minimizing the\nKSD for fixed kernels to our adaptive setting, showing convergence properties\nfor the maximal KSD over our kernel class. Our empirical results further\nsupport this intuition: Ad-SVGD consistently outperforms standard heuristics in\na variety of tasks.",
    "published": "2025-10-02T14:33:57Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02067v1"
  },
  {
    "id": "2510.02060v1",
    "title": "ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly\n  Detection",
    "authors": [
      "Sanghyu Yoon",
      "Dongmin Kim",
      "Suhee Yoon",
      "Ye Seul Sim",
      "Seungdong Yoa",
      "Hye-Seung Cho",
      "Soonyoung Lee",
      "Hankook Lee",
      "Woohyung Lim"
    ],
    "summary": "In tabular anomaly detection (AD), textual semantics often carry critical\nsignals, as the definition of an anomaly is closely tied to domain-specific\ncontext. However, existing benchmarks provide only raw data points without\nsemantic context, overlooking rich textual metadata such as feature\ndescriptions and domain knowledge that experts rely on in practice. This\nlimitation restricts research flexibility and prevents models from fully\nleveraging domain knowledge for detection. ReTabAD addresses this gap by\nrestoring textual semantics to enable context-aware tabular AD research. We\nprovide (1) 20 carefully curated tabular datasets enriched with structured\ntextual metadata, together with implementations of state-of-the-art AD\nalgorithms including classical, deep learning, and LLM-based approaches, and\n(2) a zero-shot LLM framework that leverages semantic context without\ntask-specific training, establishing a strong baseline for future research.\nFurthermore, this work provides insights into the role and utility of textual\nmetadata in AD through experiments and analysis. Results show that semantic\ncontext improves detection performance and enhances interpretability by\nsupporting domain-aware reasoning. These findings establish ReTabAD as a\nbenchmark for systematic exploration of context-aware AD.",
    "published": "2025-10-02T14:28:45Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02060v1"
  },
  {
    "id": "2510.02058v1",
    "title": "Accurate Modelling of Intrabeam Scattering and its Impact on\n  Photoinjectors for Free-Electron Lasers",
    "authors": [
      "Thomas Geoffrey Lucas",
      "Paolo Craievich",
      "Eduard Prat",
      "Sven Reiche",
      "Erion Gjonaj"
    ],
    "summary": "Intrabeam scattering (IBS) is a fundamental effect that can limit the\nperformance of high-brightness electron machines, yet it has so far been\nneglected in standard modelling of RF photoinjectors. Recent measurements at\nSwissFEL reveal that the slice energy spread (SES) in the injector is\nsignificantly underestimated by conventional tracking codes. In this work, we\npresent a dedicated Monte Carlo simulation model that accurately predicts the\nIBS-induced SES growth in the photoinjector of an X-ray free-electron laser.\nThe simulations are benchmarked against SES measurements at the SwissFEL as\nwell as theoretically supported by a new analytical model. The results\ndemonstrate that IBS-induced SES growth occurs throughout the injector, most\nprominently in the electron source, and must be taken into account when\nassessing photoinjector performance. We further show that while the 5D\nbrightness is largely conserved, the 6D brightness undergoes notable\ndegradation with propagation, underscoring the need to include IBS in the\naccurate design and optimization of photoinjectors.",
    "published": "2025-10-02T14:26:39Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02058v1"
  },
  {
    "id": "2510.02056v1",
    "title": "Adaptive Heterogeneous Mixtures of Normalising Flows for Robust\n  Variational Inference",
    "authors": [
      "Benjamin Wiriyapong",
      "Oktay Karakuş",
      "Kirill Sidorov"
    ],
    "summary": "Normalising-flow variational inference (VI) can approximate complex\nposteriors, yet single-flow models often behave inconsistently across\nqualitatively different distributions. We propose Adaptive Mixture Flow\nVariational Inference (AMF-VI), a heterogeneous mixture of complementary flows\n(MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of\nindividual flows, and (ii) adaptive global weight estimation via\nlikelihood-driven updates, without per-sample gating or architectural changes.\nEvaluated on six canonical posterior families of banana, X-shape, two-moons,\nrings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower\nnegative log-likelihood than each single-flow baseline and delivers stable\ngains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD),\nindicating improved robustness across shapes and modalities. The procedure is\nefficient and architecture-agnostic, incurring minimal overhead relative to\nstandard flow training, and demonstrates that adaptive mixtures of diverse\nflows provide a reliable route to robust VI across diverse posterior families\nwhilst preserving each expert's inductive bias.",
    "published": "2025-10-02T14:25:29Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02056v1"
  },
  {
    "id": "2510.02050v1",
    "title": "Multidata Causal Discovery for Statistical Hurricane Intensity\n  Forecasting",
    "authors": [
      "Saranya Ganesh S.",
      "Frederick Iat-Hin Tam",
      "Milton S. Gomez",
      "Marie McGraw",
      "Mark DeMaria",
      "Kate Musgrave",
      "Jakob Runge",
      "Tom Beucler"
    ],
    "summary": "Improving statistical forecasts of Atlantic hurricane intensity is limited by\ncomplex nonlinear interactions and difficulty in identifying relevant\npredictors. Conventional methods prioritize correlation or fit, often\noverlooking confounding variables and limiting generalizability to unseen\ntropical storms. To address this, we leverage a multidata causal discovery\nframework with a replicated dataset based on Statistical Hurricane Intensity\nPrediction Scheme (SHIPS) using ERA5 meteorological reanalysis. We conduct\nmultiple experiments to identify and select predictors causally linked to\nhurricane intensity changes. We train multiple linear regression models to\ncompare causal feature selection with no selection, correlation, and random\nforest feature importance across five forecast lead times from 1 to 5 days (24\nto 120 hours). Causal feature selection consistently outperforms on unseen test\ncases, especially for lead times shorter than 3 days. The causal features\nprimarily include vertical shear, mid-tropospheric potential vorticity and\nsurface moisture conditions, which are physically significant yet often\nunderutilized in hurricane intensity predictions. Further, we build an extended\npredictor set (SHIPS+) by adding selected features to the standard SHIPS\npredictors. SHIPS+ yields increased short-term predictive skill at lead times\nof 24, 48, and 72 hours. Adding nonlinearity using multilayer perceptron\nfurther extends skill to longer lead times, despite our framework being purely\nregional and not requiring global forecast data. Operational SHIPS tests\nconfirm that three of the six added causally discovered predictors improve\nforecasts, with the largest gains at longer lead times. Our results demonstrate\nthat causal discovery improves hurricane intensity prediction and pave the way\ntoward more empirical forecasts.",
    "published": "2025-10-02T14:23:51Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02050v1"
  },
  {
    "id": "2510.02049v1",
    "title": "Mathematical Modeling and Convergence Analysis of Deep Neural Networks\n  with Dense Layer Connectivities in Deep Learning",
    "authors": [
      "Jinshu Huang",
      "Haibin Su",
      "Xue-Cheng Tai",
      "Chunlin Wu"
    ],
    "summary": "In deep learning, dense layer connectivity has become a key design principle\nin deep neural networks (DNNs), enabling efficient information flow and strong\nperformance across a range of applications. In this work, we model densely\nconnected DNNs mathematically and analyze their learning problems in the\ndeep-layer limit. For a broad applicability, we present our analysis in a\nframework setting of DNNs with densely connected layers and general non-local\nfeature transformations (with local feature transformations as special cases)\nwithin layers, which is called dense non-local (DNL) framework and includes\nstandard DenseNets and variants as special examples. In this formulation, the\ndensely connected networks are modeled as nonlinear integral equations, in\ncontrast to the ordinary differential equation viewpoint commonly adopted in\nprior works. We study the associated training problems from an optimal control\nperspective and prove convergence results from the network learning problem to\nits continuous-time counterpart. In particular, we show the convergence of\noptimal values and the subsequence convergence of minimizers, using a piecewise\nlinear extension and $\\Gamma$-convergence analysis. Our results provide a\nmathematical foundation for understanding densely connected DNNs and further\nsuggest that such architectures can offer stability of training deep models.",
    "published": "2025-10-02T14:22:51Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02049v1"
  },
  {
    "id": "2510.02048v1",
    "title": "Variational Secret Common Randomness Extraction",
    "authors": [
      "Xinyang Li",
      "Vlad C. Andrei",
      "Peter J. Gu",
      "Yiqi Chen",
      "Ullrich J. Mönich",
      "Holger Boche"
    ],
    "summary": "This paper studies the problem of extracting common randomness (CR) or secret\nkeys from correlated random sources observed by two legitimate parties, Alice\nand Bob, through public discussion in the presence of an eavesdropper, Eve. We\npropose a practical two-stage CR extraction framework. In the first stage, the\nvariational probabilistic quantization (VPQ) step is introduced, where Alice\nand Bob employ probabilistic neural network (NN) encoders to map their\nobservations into discrete, nearly uniform random variables (RVs) with high\nagreement probability while minimizing information leakage to Eve. This is\nrealized through a variational learning objective combined with adversarial\ntraining. In the second stage, a secure sketch using code-offset construction\nreconciles the encoder outputs into identical secret keys, whose secrecy is\nguaranteed by the VPQ objective. As a representative application, we study\nphysical layer key (PLK) generation. Beyond the traditional methods, which rely\non the channel reciprocity principle and require two-way channel probing, thus\nsuffering from large protocol overhead and being unsuitable in high mobility\nscenarios, we propose a sensing-based PLK generation method for integrated\nsensing and communications (ISAC) systems, where paired range-angle (RA) maps\nmeasured at Alice and Bob serve as correlated sources. The idea is verified\nthrough both end-to-end simulations and real-world software-defined radio (SDR)\nmeasurements, including scenarios where Eve has partial knowledge about Bob's\nposition. The results demonstrate the feasibility and convincing performance of\nboth the proposed CR extraction framework and sensing-based PLK generation\nmethod.",
    "published": "2025-10-02T14:22:21Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02048v1"
  },
  {
    "id": "2510.02043v1",
    "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers",
    "authors": [
      "Sahil Bhandary Karnoor",
      "Romit Roy Choudhury"
    ],
    "summary": "Pose estimation refers to tracking a human's full body posture, including\ntheir head, torso, arms, and legs. The problem is challenging in practical\nsettings where the number of body sensors are limited. Past work has shown\npromising results using conditional diffusion models, where the pose prediction\nis conditioned on both <location, rotation> measurements from the sensors.\nUnfortunately, nearly all these approaches generalize poorly across users,\nprimarly because location measurements are highly influenced by the body size\nof the user. In this paper, we formulate pose estimation as an inverse problem\nand design an algorithm capable of zero-shot generalization. Our idea utilizes\na pre-trained diffusion model and conditions it on rotational measurements\nalone; the priors from this model are then guided by a likelihood term, derived\nfrom the measured locations. Thus, given any user, our proposed InPose method\ngeneratively estimates the highly likely sequence of poses that best explains\nthe sparse on-body measurements.",
    "published": "2025-10-02T14:16:43Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02043v1"
  },
  {
    "id": "2510.02040v1",
    "title": "Komitee Equal Shares: Choosing Together as Voters and as Groups with a\n  Co-designed Virtual Budget Algorithm",
    "authors": [
      "Joshua C. Yang",
      "Noemi Scheurer"
    ],
    "summary": "Public funding processes demand fairness, learning, and outcomes that\nparticipants can understand. We introduce Komitee Equal Shares, a priceable\nvirtual-budget allocation framework that integrates two signals: in voter mode,\nparticipants cast point votes; in evaluator mode, small groups assess proposals\nagainst collectively defined impact fields. The framework extends the Method of\nEqual Shares by translating both signals into virtual spending power and\nproducing voting receipts. We deployed the framework in the 2025 Kultur Komitee\nin Winterthur, Switzerland. Our contributions are: (1) a clear separation of\ndecision modes, addressing a gap in social choice that typically treats\nparticipatory budgeting as preference aggregation while citizens also see\nthemselves as evaluators; and (2) the design of voting receipts that\noperationalise priceability into participant-facing explanations, making\nproportional allocations legible and traceable. The framework generalises to\nparticipatory grant-making and budgeting, offering a model where citizens act\nas voters and evaluators within one proportional, explainable allocation.",
    "published": "2025-10-02T14:11:59Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02040v1"
  },
  {
    "id": "2510.02030v1",
    "title": "kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring",
    "authors": [
      "Jenna Kline",
      "Maksim Kholiavchenko",
      "Samuel Stevens",
      "Nina van Tiel",
      "Alison Zhong",
      "Namrata Banerji",
      "Alec Sheets",
      "Sowbaranika Balasubramaniam",
      "Isla Duporge",
      "Matthew Thompson",
      "Elizabeth Campolongo",
      "Jackson Miliko",
      "Neil Rosser",
      "Tanya Berger-Wolf",
      "Charles V. Stewart",
      "Daniel I. Rubenstein"
    ],
    "summary": "A comprehensive understanding of animal behavior ecology depends on scalable\napproaches to quantify and interpret complex, multidimensional behavioral\npatterns. Traditional field observations are often limited in scope,\ntime-consuming, and labor-intensive, hindering the assessment of behavioral\nresponses across landscapes. To address this, we present kabr-tools (Kenyan\nAnimal Behavior Recognition Tools), an open-source package for automated\nmulti-species behavioral monitoring. This framework integrates drone-based\nvideo with machine learning systems to extract behavioral, social, and spatial\nmetrics from wildlife footage. Our pipeline leverages object detection,\ntracking, and behavioral classification systems to generate key metrics,\nincluding time budgets, behavioral transitions, social interactions, habitat\nassociations, and group composition dynamics. Compared to ground-based methods,\ndrone-based observations significantly improved behavioral granularity,\nreducing visibility loss by 15% and capturing more transitions with higher\naccuracy and continuity. We validate kabr-tools through three case studies,\nanalyzing 969 behavioral sequences, surpassing the capacity of traditional\nmethods for data capture and annotation. We found that, like Plains zebras,\nvigilance in Grevy's zebras decreases with herd size, but, unlike Plains\nzebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit\nstrong behavioral inertia, with rare transitions to alert behaviors and\nobserved spatial segregation between Grevy's zebras, Plains zebras, and\ngiraffes in mixed-species herds. By enabling automated behavioral monitoring at\nscale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing\nconservation, biodiversity research, and ecological monitoring.",
    "published": "2025-10-02T14:03:55Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02030v1"
  },
  {
    "id": "2510.02017v1",
    "title": "FairContrast: Enhancing Fairness through Contrastive learning and\n  Customized Augmenting Methods on Tabular Data",
    "authors": [
      "Aida Tayebi",
      "Ali Khodabandeh Yalabadi",
      "Mehdi Yazdani-Jahromi",
      "Ozlem Ozmen Garibay"
    ],
    "summary": "As AI systems become more embedded in everyday life, the development of fair\nand unbiased models becomes more critical. Considering the social impact of AI\nsystems is not merely a technical challenge but a moral imperative. As\nevidenced in numerous research studies, learning fair and robust\nrepresentations has proven to be a powerful approach to effectively debiasing\nalgorithms and improving fairness while maintaining essential information for\nprediction tasks. Representation learning frameworks, particularly those that\nutilize self-supervised and contrastive learning, have demonstrated superior\nrobustness and generalizability across various domains. Despite the growing\ninterest in applying these approaches to tabular data, the issue of fairness in\nthese learned representations remains underexplored. In this study, we\nintroduce a contrastive learning framework specifically designed to address\nbias and learn fair representations in tabular datasets. By strategically\nselecting positive pair samples and employing supervised and self-supervised\ncontrastive learning, we significantly reduce bias compared to existing\nstate-of-the-art contrastive learning models for tabular data. Our results\ndemonstrate the efficacy of our approach in mitigating bias with minimum\ntrade-off in accuracy and leveraging the learned fair representations in\nvarious downstream tasks.",
    "published": "2025-10-02T13:43:53Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02017v1"
  },
  {
    "id": "2510.02016v1",
    "title": "Design and Scientific Prospects of the POLAR-2 Mission",
    "authors": [
      "Merlin Kole",
      "Nicolas de Angelis",
      "Jiang He",
      "Hongbang Liu",
      "Jianchao Sun",
      "Fei Xie",
      "Jimmy Zaid"
    ],
    "summary": "The POLAR-2 mission consists of 3 instruments designed with the combined aim\nof producing a deeper understanding of Gamma-Ray Bursts. To achieve this,\nPOLAR-2 relies on polarisation measurements and, for the first time will\nprovide these using 2 separate polarimeter detectors. The first of these is a\npayload optimised to perform Compton polarimetry measurements in the 40-1000\nkeV energy range using a combination of plastic scintillators and SiPMs. The\ndevelopment of this payload, the design of which is based on lessons learned\nfrom the POLAR mission, included optimization of plastic scintillator design.\nIn addition, its development included detailed characterization, space\nqualification and radiation damage and mitigation strategies for the large\nnumber of silicon photo-multipliers included in the design. We will present\nthese along with an overview of the readout electronics. These electronics were\ndeveloped with flexibility in mind, as well as low cost and low power\nconsumption. As such, its design is of interest beyond this polarimeter and is\nalso used on the spectrometer instrument of POLAR-2 where it is used to read\nout an array of GAGG:Ce scintillators. This readout, in combination with a\ncoded mask, allows this secondary instrument to provide detailed spectral and\nlocalization measurements. The final instrument used in the mission aims to use\ngas-based detectors to perform polarization measurements in the keV energy\nregion. The novelty of this design is that it will be optimized to use these\nfor wide field of view observations. The combination of the three instruments\nwill allow to perform detailed spectral, localization and polarization\nmeasurements of these transient phenomena together for the first time. Here we\nprovide an overview of the technologies employed in the mission along with\ndetailed predictions on its capabilities after its launch currently foreseen in\n2027.",
    "published": "2025-10-02T13:43:01Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02016v1"
  },
  {
    "id": "2510.02014v1",
    "title": "Normality Calibration in Semi-supervised Graph Anomaly Detection",
    "authors": [
      "Guolei Zeng",
      "Hezhe Qiao",
      "Guoguo Ai",
      "Jinsong Guo",
      "Guansong Pang"
    ],
    "summary": "Graph anomaly detection (GAD) has attracted growing interest for its crucial\nability to uncover irregular patterns in broad applications. Semi-supervised\nGAD, which assumes a subset of annotated normal nodes available during\ntraining, is among the most widely explored application settings. However, the\nnormality learned by existing semi-supervised GAD methods is limited to the\nlabeled normal nodes, often inclining to overfitting the given patterns. These\ncan lead to high detection errors, such as high false positives. To overcome\nthis limitation, we propose GraphNC , a graph normality calibration framework\nthat leverages both labeled and unlabeled data to calibrate the normality from\na teacher model (a pre-trained semi-supervised GAD model) jointly in anomaly\nscore and node representation spaces. GraphNC includes two main components,\nanomaly score distribution alignment (ScoreDA) and perturbation-based normality\nregularization (NormReg). ScoreDA optimizes the anomaly scores of our model by\naligning them with the score distribution yielded by the teacher model. Due to\naccurate scores in most of the normal nodes and part of the anomaly nodes in\nthe teacher model, the score alignment effectively pulls the anomaly scores of\nthe normal and abnormal classes toward the two ends, resulting in more\nseparable anomaly scores. Nevertheless, there are inaccurate scores from the\nteacher model. To mitigate the misleading by these scores, NormReg is designed\nto regularize the graph normality in the representation space, making the\nrepresentations of normal nodes more compact by minimizing a\nperturbation-guided consistency loss solely on the labeled nodes.",
    "published": "2025-10-02T13:36:04Z",
    "pdf_url": "http://arxiv.org/pdf/2510.02014v1"
  }
]