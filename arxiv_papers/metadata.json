[
  {
    "id": "2510.05102v1",
    "title": "TopInG: Topologically Interpretable Graph Learning via Persistent\n  Rationale Filtration",
    "authors": [
      "Cheng Xin",
      "Fan Xu",
      "Xin Ding",
      "Jie Gao",
      "Jiaxin Ding"
    ],
    "summary": "Graph Neural Networks (GNNs) have shown remarkable success across various\nscientific fields, yet their adoption in critical decision-making is often\nhindered by a lack of interpretability. Recently, intrinsically interpretable\nGNNs have been studied to provide insights into model predictions by\nidentifying rationale substructures in graphs. However, existing methods face\nchallenges when the underlying rationale subgraphs are complex and varied. In\nthis work, we propose TopInG: Topologically Interpretable Graph Learning, a\nnovel topological framework that leverages persistent homology to identify\npersistent rationale subgraphs. TopInG employs a rationale filtration learning\napproach to model an autoregressive generation process of rationale subgraphs,\nand introduces a self-adjusted topological constraint, termed topological\ndiscrepancy, to enforce a persistent topological distinction between rationale\nsubgraphs and irrelevant counterparts. We provide theoretical guarantees that\nour loss function is uniquely optimized by the ground truth under specific\nconditions. Extensive experiments demonstrate TopInG's effectiveness in\ntackling key challenges, such as handling variform rationale subgraphs,\nbalancing predictive performance with interpretability, and mitigating spurious\ncorrelations. Results show that our approach improves upon state-of-the-art\nmethods on both predictive accuracy and interpretation quality.",
    "published": "2025-10-06T17:59:44Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05102v1"
  },
  {
    "id": "2510.05097v1",
    "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation",
    "authors": [
      "Robin Courant",
      "Xi Wang",
      "David Loiseaux",
      "Marc Christie",
      "Vicky Kalogeiton"
    ],
    "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.",
    "published": "2025-10-06T17:58:34Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05097v1"
  },
  {
    "id": "2510.05095v1",
    "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized\n  Preference Optimization for Aligning Large Reasoning Models",
    "authors": [
      "Mingkang Zhu",
      "Xi Chen",
      "Bei Yu",
      "Hengshuang Zhao",
      "Jiaya Jia"
    ],
    "summary": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.",
    "published": "2025-10-06T17:58:01Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05095v1"
  },
  {
    "id": "2510.05093v1",
    "title": "Character Mixing for Video Generation",
    "authors": [
      "Tingting Liao",
      "Chongjian Ge",
      "Guangyi Liu",
      "Hao Li",
      "Yi Zhou"
    ],
    "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.",
    "published": "2025-10-06T17:57:39Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05093v1"
  },
  {
    "id": "2510.05092v1",
    "title": "Learning to Interpret Weight Differences in Language Models",
    "authors": [
      "Avichal Goel",
      "Yoon Kim",
      "Nir Shavit",
      "Tony T. Wang"
    ],
    "summary": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions.",
    "published": "2025-10-06T17:57:23Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05092v1"
  },
  {
    "id": "2510.05089v1",
    "title": "QuantumBoost: A lazy, yet fast, quantum algorithm for learning with weak\n  hypotheses",
    "authors": [
      "Amira Abbas",
      "Yanlin Chen",
      "Tuyen Nguyen",
      "Ronald de Wolf"
    ],
    "summary": "The technique of combining multiple votes to enhance the quality of a\ndecision is the core of boosting algorithms in machine learning. In particular,\nboosting provably increases decision quality by combining multiple weak\nlearners-hypotheses that are only slightly better than random guessing-into a\nsingle strong learner that classifies data well. There exist various versions\nof boosting algorithms, which we improve upon through the introduction of\nQuantumBoost. Inspired by classical work by Barak, Hardt and Kale, our\nQuantumBoost algorithm achieves the best known runtime over other boosting\nmethods through two innovations. First, it uses a quantum algorithm to compute\napproximate Bregman projections faster. Second, it combines this with a lazy\nprojection strategy, a technique from convex optimization where projections are\nperformed infrequently rather than every iteration. To our knowledge,\nQuantumBoost is the first algorithm, classical or quantum, to successfully\nadopt a lazy projection strategy in the context of boosting.",
    "published": "2025-10-06T17:56:05Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05089v1"
  },
  {
    "id": "2510.05087v1",
    "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
    "authors": [
      "Janos Perczel",
      "Jin Chow",
      "Dorottya Demszky"
    ],
    "summary": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.",
    "published": "2025-10-06T17:55:04Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05087v1"
  },
  {
    "id": "2510.05083v1",
    "title": "Robust multicellular programs dissect the complex tumor microenvironment\n  and track disease progression in colorectal adenocarcinomas",
    "authors": [
      "Loan Vulliard",
      "Teresa Glauner",
      "Sven Truxa",
      "Miray Cetin",
      "Yu-Le Wu",
      "Ronald Simon",
      "Laura Behm",
      "Jovan Tanevski",
      "Julio Saez-Rodriguez",
      "Guido Sauter",
      "Felix J. Hartmann"
    ],
    "summary": "Colorectal cancer (CRC) is highly heterogeneous, with five-year survival\nrates dropping from $\\sim$90% in localized disease to $\\sim$15% with distant\nmetastases. Disease progression is shaped not only by tumor-intrinsic\nalterations but also by the reorganization of the tumor microenvironment (TME).\nMetabolic, compositional, and spatial changes contribute to this progression,\nbut considered individually they lack context and often fail as therapeutic\ntargets. Understanding their coordination could reveal processes to alter the\ndisease course. Here, we combined multiplexed ion beam imaging (MIBI) with\nmachine learning to profile metabolic, functional and spatial states of 522\ncolorectal lesions with single-cell resolution. We observed recurrent\nstage-specific remodeling marked by a lymphoid-to-myeloid shift, stromal-cancer\ncooperation, and malignant metabolic shifts. Spatial organization of\nepithelial, stromal, and immune compartments provided stronger stratification\nof disease stage than tumor-intrinsic changes or bulk immune infiltration\nalone. To systematically model these coordinated changes, we condensed\nmultimodal features into 10 latent factors of TME organization. These factors\ntracked disease progression, were conserved across cohorts, and revealed\nfrequent multicellular metabolic niches and distinct, non-exclusive TME\ntrajectories. Our framework MuVIcell exposes the elements that together drive\nCRC progression by grouping co-occurring changes across cell types and feature\nclasses into coordinated multicellular programs. This creates a rational basis\nto therapeutically target TME reorganization. Importantly, the framework is\nscalable and flexible, offering a resource for studying multicellular\norganization in other solid tumors.",
    "published": "2025-10-06T17:51:26Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05083v1"
  },
  {
    "id": "2510.05080v1",
    "title": "MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis",
    "authors": [
      "Yangyang Wang",
      "Tayo Fabusuyi"
    ],
    "summary": "This study presents a novel small-area estimation framework to enhance urban\ntransportation planning through detailed characterization of travel behavior.\nOur approach improves on the four-step travel model by employing publicly\navailable microdata files and machine learning methods to predict travel\nbehavior for a representative, synthetic population at small geographic areas.\nThis approach enables high-resolution estimation of trip generation, trip\ndistribution, mode choice, and route assignment. Validation using ACS/PUMS\nwork-commute datasets demonstrates that our framework achieves higher accuracy\ncompared to conventional approaches. The resulting granular insights enable the\ntailoring of interventions to address localized situations and support a range\nof policy applications and targeted interventions, including the optimal\nplacement of micro-fulfillment centers, effective curb-space management, and\nthe design of more inclusive transportation solutions particularly for\nvulnerable communities.",
    "published": "2025-10-06T17:50:56Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05080v1"
  },
  {
    "id": "2510.05071v1",
    "title": "Neuroplastic Modular Framework: Cross-Domain Image Classification of\n  Garbage and Industrial Surfaces",
    "authors": [
      "Debojyoti Ghosh",
      "Soumya K Ghosh",
      "Adrijit Goswami"
    ],
    "summary": "Efficient and accurate classification of waste and industrial surface defects\nis essential for ensuring sustainable waste management and maintaining high\nstandards in quality control. This paper introduces the Neuroplastic Modular\nClassifier, a novel hybrid architecture designed for robust and adaptive image\nclassification in dynamic environments. The model combines a ResNet-50 backbone\nfor localized feature extraction with a Vision Transformer (ViT) to capture\nglobal semantic context. Additionally, FAISS-based similarity retrieval is\nincorporated to provide a memory-like reference to previously encountered data,\nenriching the model's feature space. A key innovation of our architecture is\nthe neuroplastic modular design composed of expandable, learnable blocks that\ndynamically grow during training when performance plateaus. Inspired by\nbiological learning systems, this mechanism allows the model to adapt to data\ncomplexity over time, improving generalization. Beyond garbage classification,\nwe validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),\nwhich involves industrial defect detection on metal surfaces. Experimental\nresults across domains show that the proposed architecture outperforms\ntraditional static models in both accuracy and adaptability. The Neuroplastic\nModular Classifier offers a scalable, high-performance solution for real-world\nimage classification, with strong applicability in both environmental and\nindustrial domains.",
    "published": "2025-10-06T17:47:45Z",
    "pdf_url": "http://arxiv.org/pdf/2510.05071v1"
  }
]