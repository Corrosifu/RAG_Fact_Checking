[
  {
    "id": "2510.03231v1",
    "title": "Reward Models are Metrics in a Trench Coat",
    "authors": [
      "Sebastian Gehrmann"
    ],
    "summary": "The emergence of reinforcement learning in post-training of large language\nmodels has sparked significant interest in reward models. Reward models assess\nthe quality of sampled model outputs to generate training signals. This task is\nalso performed by evaluation metrics that monitor the performance of an AI\nmodel. We find that the two research areas are mostly separate, leading to\nredundant terminology and repeated pitfalls. Common challenges include\nsusceptibility to spurious correlations, impact on downstream reward hacking,\nmethods to improve data quality, and approaches to meta-evaluation. Our\nposition paper argues that a closer collaboration between the fields can help\novercome these issues. To that end, we show how metrics outperform reward\nmodels on specific tasks and provide an extensive survey of the two areas.\nGrounded in this survey, we point to multiple research topics in which closer\nalignment can improve reward models and metrics in areas such as preference\nelicitation methods, avoidance of spurious correlations and reward hacking, and\ncalibration-aware meta-evaluation.",
    "published": "2025-10-03T17:59:44Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03231v1"
  },
  {
    "id": "2510.03230v1",
    "title": "Improving GUI Grounding with Explicit Position-to-Coordinate Mapping",
    "authors": [
      "Suyuchen Wang",
      "Tianyu Zhang",
      "Ahmed Masry",
      "Christopher Pal",
      "Spandana Gella",
      "Bang Liu",
      "Perouz Taslakian"
    ],
    "summary": "GUI grounding, the task of mapping natural-language instructions to pixel\ncoordinates, is crucial for autonomous agents, yet remains difficult for\ncurrent VLMs. The core bottleneck is reliable patch-to-pixel mapping, which\nbreaks when extrapolating to high-resolution displays unseen during training.\nCurrent approaches generate coordinates as text tokens directly from visual\nfeatures, forcing the model to infer complex position-to-pixel mappings\nimplicitly; as a result, accuracy degrades and failures proliferate on new\nresolutions. We address this with two complementary innovations. First, RULER\ntokens serve as explicit coordinate markers, letting the model reference\npositions similar to gridlines on a map and adjust rather than generate\ncoordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial\nencoding by ensuring that width and height dimensions are represented equally,\naddressing the asymmetry of standard positional schemes. Experiments on\nScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in\ngrounding accuracy, with the largest improvements on high-resolution\ninterfaces. By providing explicit spatial guidance rather than relying on\nimplicit learning, our approach enables more reliable GUI automation across\ndiverse resolutions and platforms.",
    "published": "2025-10-03T17:59:34Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03230v1"
  },
  {
    "id": "2510.03228v1",
    "title": "MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture\n  Recognition",
    "authors": [
      "Ricardo T. Fares",
      "Lucas C. Ribas"
    ],
    "summary": "Randomized neural networks for representation learning have consistently\nachieved prominent results in texture recognition tasks, effectively combining\nthe advantages of both traditional techniques and learning-based approaches.\nHowever, existing approaches have so far focused mainly on improving\ncross-information prediction, without introducing significant advancements to\nthe overall randomized network architecture. In this paper, we propose Mixer, a\nnovel randomized neural network for texture representation learning. At its\ncore, the method leverages hyperspherical random embeddings coupled with a\ndual-branch learning module to capture both intra- and inter-channel\nrelationships, further enhanced by a newly formulated optimization problem for\nbuilding rich texture representations. Experimental results have shown the\ninteresting results of the proposed approach across several pure texture\nbenchmarks, each with distinct characteristics and challenges. The source code\nwill be available upon publication.",
    "published": "2025-10-03T17:58:04Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03228v1"
  },
  {
    "id": "2510.03226v1",
    "title": "A fast non-reversible sampler for Bayesian finite mixture models",
    "authors": [
      "Filippo Ascolani",
      "Giacomo Zanella"
    ],
    "summary": "Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known\nthat sampling from the resulting posterior distribution can be a hard task. In\nparticular, popular reversible Markov chain Monte Carlo schemes are often slow\nto converge when the number of observations $n$ is large. In this paper we\nintroduce a novel and simple non-reversible sampling scheme for Bayesian finite\nmixture models, which is shown to drastically outperform classical samplers in\nmany scenarios of interest, especially during convergence phase and when\ncomponents in the mixture have non-negligible overlap. At the theoretical\nlevel, we show that the performance of the proposed non-reversible scheme\ncannot be worse than the standard one, in terms of asymptotic variance, by more\nthan a factor of four; and we provide a scaling limit analysis suggesting that\nthe non-reversible sampler can reduce the convergence time from O$(n^2)$ to\nO$(n)$. We also discuss why the statistical features of mixture models make\nthem an ideal case for the use of non-reversible discrete samplers.",
    "published": "2025-10-03T17:57:44Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03226v1"
  },
  {
    "id": "2510.03224v1",
    "title": "Test-Time Defense Against Adversarial Attacks via Stochastic Resonance\n  of Latent Ensembles",
    "authors": [
      "Dong Lao",
      "Yuxiang Zhang",
      "Haniyeh Ehsani Oskouie",
      "Yangchao Wu",
      "Alex Wong",
      "Stefano Soatto"
    ],
    "summary": "We propose a test-time defense mechanism against adversarial attacks:\nimperceptible image perturbations that significantly alter the predictions of a\nmodel. Unlike existing methods that rely on feature filtering or smoothing,\nwhich can lead to information loss, we propose to \"combat noise with noise\" by\nleveraging stochastic resonance to enhance robustness while minimizing\ninformation loss. Our approach introduces small translational perturbations to\nthe input image, aligns the transformed feature embeddings, and aggregates them\nbefore mapping back to the original reference image. This can be expressed in a\nclosed-form formula, which can be deployed on diverse existing network\narchitectures without introducing additional network modules or fine-tuning for\nspecific attack types. The resulting method is entirely training-free,\narchitecture-agnostic, and attack-agnostic. Empirical results show\nstate-of-the-art robustness on image classification and, for the first time,\nestablish a generic test-time defense for dense prediction tasks, including\nstereo matching and optical flow, highlighting the method's versatility and\npracticality. Specifically, relative to clean (unperturbed) performance, our\nmethod recovers up to 68.1% of the accuracy loss on image classification, 71.9%\non stereo matching, and 29.2% on optical flow under various types of\nadversarial attacks.",
    "published": "2025-10-03T17:57:25Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03224v1"
  },
  {
    "id": "2510.03223v1",
    "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment",
    "authors": [
      "Hongxiang Zhang",
      "Yuan Tian",
      "Tianyi Zhang"
    ],
    "summary": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
    "published": "2025-10-03T17:56:33Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03223v1"
  },
  {
    "id": "2510.03222v1",
    "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning\n  with Verifiable Reward",
    "authors": [
      "Guanhua Huang",
      "Tingqiang Xu",
      "Mingze Wang",
      "Qi Yi",
      "Xue Gong",
      "Siheng Li",
      "Ruibin Xiong",
      "Kejiao Li",
      "Yuhao Jiang",
      "Bo Zhou"
    ],
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textbf{\\textit{reasoning\nsparks}}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\n\\textit{reasoning sparks} is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n$60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
    "published": "2025-10-03T17:56:13Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03222v1"
  },
  {
    "id": "2510.03217v1",
    "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair",
    "authors": [
      "Jos√© Cambronero",
      "Michele Tufano",
      "Sherry Shi",
      "Renyao Wei",
      "Grant Uy",
      "Runxiang Cheng",
      "Chin-Jung Liu",
      "Shiying Pan",
      "Satish Chandra",
      "Pat Rondon"
    ],
    "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.",
    "published": "2025-10-03T17:53:28Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03217v1"
  },
  {
    "id": "2510.03215v1",
    "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
    "authors": [
      "Tianyu Fu",
      "Zihan Min",
      "Hanling Zhang",
      "Jichao Yan",
      "Guohao Dai",
      "Wanli Ouyang",
      "Yu Wang"
    ],
    "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
    "published": "2025-10-03T17:52:32Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03215v1"
  },
  {
    "id": "2510.03209v1",
    "title": "Joint Bidding on Intraday and Frequency Containment Reserve Markets",
    "authors": [
      "Yiming Zhang",
      "Wolfgang Ridinger",
      "David Wozabal"
    ],
    "summary": "As renewable energy integration increases supply variability, battery energy\nstorage systems (BESS) present a viable solution for balancing supply and\ndemand. This paper proposes a novel approach for optimizing battery BESS\nparticipation in multiple electricity markets. We develop a joint bidding\nstrategy that combines participation in the primary frequency reserve market\nwith continuous trading in the intraday market, addressing a gap in the extant\nliterature which typically considers these markets in isolation or simplifies\nthe continuous nature of intraday trading. Our approach utilizes a mixed\ninteger linear programming implementation of the rolling intrinsic algorithm\nfor intraday decisions and state of charge recovery, alongside a learned\nclassifier strategy (LCS) that determines optimal capacity allocation between\nmarkets. A comprehensive out-of-sample backtest over more than one year of\nhistorical German market data validates our approach: The LCS increases overall\nprofits by over 4% compared to the best-performing static strategy and by more\nthan 3% over a naive dynamic benchmark. Crucially, our method closes the gap to\na theoretical perfect foresight strategy to just 4%, demonstrating the\neffectiveness of dynamic, learning-based allocation in a complex, multi-market\nenvironment.",
    "published": "2025-10-03T17:48:21Z",
    "pdf_url": "http://arxiv.org/pdf/2510.03209v1"
  }
]